<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[redis-持久化]]></title>
    <url>%2F2020%2F06%2F01%2Fredis-%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Redis 持久化[toc] Redis 是基于内存的k-v服务，内存在断电时无法保存数据，因此需要做数据的备份操作。 Redis 有两种备份方式： 内存快照的持久化保存 RDB 适合全量复制，容灾备份，每日周期性定时执行（间隔6小时） 压缩后的二进制文件，恢复速度快 耗时长，无法做到实时备份 记录修改操作日志的持久化保存 AOF 实时备份（1s 一次） RDB 内存快照的二进制压缩格式 RDB 备份流程 执行bgsave 命令，判断RDF 或 AOF子进程是否在运行，如果存在则直接退出； 主进程fork 产生子进程，子进程用来执行备份操作（会阻塞主进程）； 子进程根据父进程生成的临时快照文件来创建 RDB 文件，替换旧 RDB 文件； 子进程发送完成信号给父进程，父进程会统计相关信息； linux 进程中的 fork :fork函数得到的子进程从父进程的继承了整个进程的地址空间，包括：进程上下文、进程堆栈、内存信息、打开的文件描述符、信号控制设置、进程优先级、进程组号、当前工作目录、根目录、资源限制、控制终端等。 fork 不会拷贝父进程的物理空间，会共用 RDB 什么情况下会被触发？触发 RDB 备份 主要是两个命令： save 和 bgsave save: 主线程去执行 RDB 备份，所以无法响应请求； bgsave: 子进程去执行备份，只在fork 阶段产生阻塞； 所以，我们只需要知道 bgsave 什么时候触发调用就行。 配置save m n ：即 m 秒内数据集被修改了n次，则触发bgsave 从节点的全量复制：主节点会将RDB文件传递到从节点 debug reload 时 执行 shutdown时 从节点需要全量复制时？？？ AOF 默认是关闭的，但优先级高于 RDB 相关的指令运行期指定 RDB 文件保存地址config set dir {newDir} AOFappend only file 日志记录修改操作，可以实时记录备份 AOF 备份流程 写入命令会追加到 aof_buf 缓冲区中; 根据对应的策略 写入到硬盘； 当文件过于大时，会重写 AOF 文件; 重启优先加载 AOF 文件; AOF 备份-1.命令写入 先写入缓存区，然后再备份到硬盘中； 直接写入文本协议格式 AOF 备份-2.文件同步策略同步策略： 配置 appendfsync 参数 always everysec （默认配置， 1s 执行一次写入） no AOF 备份-3.重写机制重写的主要点： 被删除的无效数据不需要在写入 过期数据不需要在写入 多条命令可以合并 手动触发重写命令： bgrewriteaof AOF 备份-4. 重启加载比 RDB 优先级别高 如何配置相关参数？ 开启 AOF功能 配置 appendonly yes AOF 什么情况下会被触发？根据备份策略，一般是定时触发]]></content>
      <categories>
        <category>持久化</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis学习-Jedis 使用]]></title>
    <url>%2F2020%2F05%2F16%2Fredis%E5%AD%A6%E4%B9%A0-Jedis-%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[[toc] [toc] 本系列文章整理摘抄自 客户端怎么和 Redis 服务器连接？客户端和 Redis 服务器的通信是 建立在 TCP 连接的基础上的。 并且 Redis 制定了 RESP 序列化协议，是一个简单地通信约定。 Resp序列化协议*&lt;参数数量&gt;\r\n$&lt;参数1的字节数量&gt;\r\n&lt;参数1&gt;\r\n$&lt;参数2的字节数量&gt;\r\n&lt;参数2&gt;\r\n 来给可视化一下：12345*&lt;参数数量&gt;\r\n$&lt;参数1的字节数量&gt;\r\n&lt;参数1&gt;\r\n$&lt;参数2的字节数量&gt;\r\n&lt;参数2&gt;\r\n 其他可以参考该书章节。 Jedis 连接池的使用简单的API 介绍 获取 jedis连接12345678910111213Jedis jedis = new Jedis(&quot;127.0.0.1&quot;, 6379);Jedis jedis = null;try &#123; jedis = new Jedis(&quot;127.0.0.1&quot;, 6379); &#125; catch (Exception e) &#123; logger.error(e.getMessage(),e);&#125; finally &#123; if (jedis != null) &#123; jedis.close(); &#125;&#125; 123456789101112131415161718// Stringjedis.set(&quot;key&quot;, &quot;value&quot;);jedis.get(&quot;key&quot;)// hset -字典jedis.set(&quot;hash&quot;, &quot;key1&quot;, &quot;value1&quot;);jedis.set(&quot;hash&quot;, &quot;key2&quot;, &quot;value2&quot;);jedis.get(&quot;key1&quot;)// list -列表jedis.rpush(&quot;mylist&quot;, &quot;1&quot;);// set -集合jedis.sadd(&quot;set&quot;, &quot;aaa&quot;);.... springboot 环境下的使用1. maven 依赖123456789101112131415161718&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.4.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.14&lt;/version&gt;&lt;/dependency&gt; 2. 代码实现配置类 -&gt; 从 application.preperties 读取配置项，并且配置12345678@Component@ConfigurationProperties(prefix = &quot;redis&quot;)public class RedisConfig &#123; // 私有属性 // 配置项 get... set...&#125; poolFactory 工厂类创建 pool12345678910111213141516171819202122@Componentpublic class RedisPoolFactory &#123; // 注入配置项 @Autowired RedisConfig redisConfig; // Bean注解 ： 根据方法创建对象，类型是JedisPool @Bean public JedisPool JedisPoolFactory()&#123; JedisPoolConfig poolConfig = new JedisPoolConfig(); // 各种配置 poolConfig.setMaxIdle(redisConfig.getPoolMaxIdle()); poolConfig.setMaxTotal(redisConfig.getPoolMaxTotal()); poolConfig.setMaxWaitMillis(redisConfig.getPoolMaxTotal()*1000); JedisPool jedisPool = new JedisPool(poolConfig, redisConfig.getHost(), redisConfig.getPort(), redisConfig.getTimeout()*1000, redisConfig.getPassword(), 0); return jedisPool;&#125; Redis 服务类 开始封装各种服务 当然，也要为服务模块化，比如 RedisUserService, RedisMiaoshaService 123456789101112131415161718192021222324252627282930313233@Service public class RedisService &#123; // 注入，@Bean 产生的jedisPool @Autowired JedisPool jedisPool; public &lt;T&gt; boolean set(IProfixForKey prefix, String key, T value)&#123; Jedis resource = null; try&#123; // 拿到连接 resource = jedisPool.getResource(); // 封装一下key, 加上特点的头信息，例如： dbName:tableName:id String strValue = beanToString(value); // 生成key String realKey = prefix.getPrefix() + key; if (strValue == null || strValue.length() &lt;= 0)&#123; return false; &#125; // 设置 resource.set(realKey, strValue); return true; &#125;finally &#123; returnToPool(resource); &#125; &#125; &#125; 非 spring 环境下的使用举个例子，不知道合不合适。 单例模式 来创建 JedisPool1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class JedisFactory &#123; private volatile static JedisPool jedisPool; private volatile static JedisPoolConfig poolConfig; private volatile static String ip; private volatile static int port; private volatile static int timeout; private volatile static String password; private volatile static int database; private JedisFactory() &#123; //JedisPoolConfig poolConfig, String ip, int port, int timeout, String password, int database /* apache common-pool 工具 * * JedisPoolConfig * */ this.jedisPool = new JedisPool(poolConfig, ip, port, timeout , password, 0); &#125; public static JedisPool getJedisPool()&#123; if (jedisPool == null)&#123; synchronized (JedisFactory.class)&#123; if (jedisPool == null)&#123; jedisPool = new JedisPool(JedisFactory.poolConfig, JedisFactory.ip, JedisFactory.port, JedisFactory.timeout, JedisFactory.password, JedisFactory.database); &#125; &#125; &#125; return jedisPool; &#125; public static void setJedisPoolConfig(JedisPoolConfig poolConfig, String ip, int port, int timeout, String password, int database)&#123; System.out.println(&quot; 配置jedis 参数...&quot;); JedisFactory.poolConfig = poolConfig; JedisFactory.ip = ip; JedisFactory.port = port; JedisFactory.timeout = timeout * 1000; JedisFactory.password = password; JedisFactory.database = database; &#125;&#125; JedisService 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package top.elgong.jedis;import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPool;/*** 解决redis 用到的* */public class JedisService &#123; /** * * @param key * @param value */ public void set(String key, String value)&#123; Jedis resource = null; try&#123; JedisPool jedisPool = JedisFactory.getJedisPool(); resource = jedisPool.getResource(); resource.set(key, value); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125;finally &#123; // 送回连接池中 if (resource != null)&#123; resource.close(); // close 就是送回池子 &#125; &#125; &#125; /* * * */ public String get(String key)&#123; Jedis resource = null; String ret = null; try&#123; JedisPool jedisPool = JedisFactory.getJedisPool(); resource = jedisPool.getResource(); ret = resource.get(key); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125;finally &#123; // 送回连接池中 if (resource != null)&#123; resource.close(); // close 就是送回池子 &#125; &#125; return ret; &#125;&#125; test12345678910111213141516171819202122232425262728package top.elgong.jedis;import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPoolConfig;import java.time.LocalDateTime;public class Test &#123; public static JedisService jedisService = new JedisService(); public static void main(String[] args) &#123; JedisPoolConfig poolConfig = new JedisPoolConfig(); String ip = &quot;121.41.111.45&quot;; int port = 6379; int timeout = 300; String password = &quot;Gelqq666%&quot;; int database = 0; JedisFactory.setJedisPoolConfig(poolConfig, ip, port, timeout, password, database); jedisService.set(&quot;leetcode-java:jedis:test:key1&quot;, &quot;haha-&quot; + LocalDateTime.now().toString()); String s = jedisService.get(&quot;leetcode-java:jedis:test:key1&quot;); System.out.println(s); &#125;&#125; Jedis Pipline 的使用123456789101112Jedis jedis = new Jedis(&quot;127.0.0.1&quot;);// 1)生成pipeline对象Pipeline pipeline = jedis.pipelined();// 2)pipeline执行命令， 注意此时命令并未真正执行for (String key : keys) &#123; pipeline.del(key);&#125;// 3)执行命令pipeline.sync(); Jedis Lua 脚本待补充。。]]></content>
      <categories>
        <category>缓存中间件</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>jedis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis学习-缓存设计中要提前考虑的事情]]></title>
    <url>%2F2020%2F05%2F16%2Fredis%E5%AD%A6%E4%B9%A0-%E7%BC%93%E5%AD%98%E8%AE%BE%E8%AE%A1%E4%B8%AD%E8%A6%81%E6%8F%90%E5%89%8D%E8%80%83%E8%99%91%E7%9A%84%E4%BA%8B%E6%83%85%2F</url>
    <content type="text"><![CDATA[[toc] [toc] 本系列文章整理摘抄自 缓存设计前需要权衡成本和收益 收益 加速读写 降低后端负载 成本 数据不一致性 缓存层和存储层数据势必会有不一致的时间，需要考虑如何避免对业务造成的影响。 代码维护成本 redis 和 mysql 两方代码都要考虑。 缓存如何更新？基于内存的缓存不可能将所有的数据都做缓存，一般针对的都是热点数据。热点数据具有时效性，过了一定时间将成为非热点数据。 所以，缓存数据通常具有生命时长，到达指定的时间后，将被更新或者删除。 熟悉缓存的常用更新策略，才能针对业务场景做出合理的选择。 更新策略1： LRU/LFU/FIFO 算法通常缓存量超过了预设的最大值时，将会采取以上策略，具体由 maxmemory-policy 参数指定。 一致性最差。 更新策略2: 超时剔除通过给缓存数据设置过期时间，让其在过期时间后自动删除，例如Redis提供的expire命令。 一致性取决于时间窗口。 更新策略3: 主动更新当堆数据一致性要求高时，在更新存储层数据后，需要立即更新缓存中的数据。 一致性最好。 最佳实践方案低一致性业务配置最大内存 + 淘汰策略 高一致性业务超时剔除 + 主动更新 缓存的粒度如何选？对于缓存数据库的内容时，要考虑缓存全部字段，或者部分字段。 全部字段： 全部缓存占用空间过大 并不是所有字段都用的到 部分字段： 未来可能用到新字段 缓存预热在系统刚上线，直接将数据加载进缓存系统，提前准备。 缓存穿透问题（针对无效查询）概念描述缓存穿透是指查询一个根本不存在的数据，缓存层和存储层都不命中。 产生的原因 问题1：业务代码或者数据出现问题 问题2：恶意攻击，爬虫造成的大量空命中 造成的影响如果出现大量的缓存穿透，会对后端的数据库产生大流量的冲击，严重可使得数据库服务宕机。 通用解决方案问题如何发现？统计总调用数、 缓存层命中数、 存储层命中数，如果发现大量存储层空命中， 可能就是出现了缓存穿透问题。 问题如何预防？问题1的解决. 缓存空对象 + 设置短过期时间实时性高。 存储层不命中后，仍然将空对象保留到缓存层中，之后再访问这个数据将会从缓存中获取，这样就保护了后端数据源。 问题2的解决. 布隆过滤器拦截（缓存层之前对存在的key做保存）实时性差。 访问缓存层和存储层之前，将存在的key用布隆过滤器提前保存起来， 做第一层拦截 缓存雪崩概念描述发生大规模的缓存失效的情况，或者缓存层宕机，大量流量冲入存储层。 产生的原因缓存服务由于各种原因失效了。 造成的影响数据库服务宕机。 通用解决方案提前的规划： 保证缓存服务的高可用。主从 + 哨兵， 集群。Redis Sentinel和RedisCluster都实现了高可用 出现雪崩时的处理：隔离组件做限流和降级处理 ehcache本地缓存 + Hystrix限流并降级,避免MySQL被搞死。 加锁排队（并发量不高的情况下） Hystrix限流并降级的流程 海量的用户请求出现： 首先通过限流组件Hystrix限流（只有预定的请求进入存储层） 对于未通过的流量，直接导到 预定的降级处理方案，比如友好的提示等待。 3. 提前排演测试缓存击(针对失效数据)-热点数据高并发访问时，失效来不及重建概念描述当热点数据并发访问量非常大，由于之前设置了过期时间，失效后难以短时间重建。 比如： 热点的娱乐新闻。 产生的原因缓存服务由于各种原因失效了。 造成的影响数据库服务宕机。 通用解决方案1. 限制重建的线程数 - 互斥锁并不是所有的线程都需要去重建，第一个遇到的线程重建，其他线程等待即可。 互斥锁的实现1：通过setnx 和 expire命令实现12345678910111213141516171819202122String get(String key) &#123; // 从Redis中获取数据 String value = redis.get(key); // 如果value为空， 则开始重构缓存 if (value == null) &#123; // 只允许一个线程重构缓存， 使用nx， 并设置过期时间ex String mutexKey = &quot;mutext:key:&quot; + key; if (redis.set(mutexKey, &quot;1&quot;, &quot;ex 180&quot;, &quot;nx&quot;)) &#123; // 从数据源获取数据 value = db.get(key); // 回写Redis， 并设置过期时间 redis.setex(key, timeout, value); // 删除key_mutex redis.delete(mutexKey); &#125;/ / 其他线程休息50毫秒后重试 else &#123; Thread.sleep(50); get(key); &#125; &#125;return value;&#125; 互斥锁的实现2：通过watch和Redis的事务命令实现2. 永不过期但是会出现数据不一致的情况。 3. 错峰失效不要让热点数据集中失效，而是一批一批，分时间段的失效 4. 失效后sleep(rand()) 这样不会所有请求都去立刻查db]]></content>
      <categories>
        <category>缓存中间件</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>缓存雪崩</tag>
        <tag>缓存穿透</tag>
        <tag>缓存击穿</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis学习-常用API]]></title>
    <url>%2F2020%2F05%2F16%2Fredis%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8API%2F</url>
    <content type="text"><![CDATA[[toc] 本系列文章整理摘抄自 全局命令 1. 查看所有键 keys遍历所有的键，时间复杂度O(n), 线上禁止使用。1keys * 2. 键总数 dbsize该指令直接获取Redis 内置的键总数变量， 时间复杂度 O(1)1dbsize 3. 键是否存在 exist存在返回1， 不存在返回01exist key 4. 删除键 del1del key1 key2 key3 5. 设置键过期 expire成功返回1123// expire key secondsset key hellowordexpire key 10 6. 查询键过期 ttl 返回 &gt;0 : 剩余的过期时间 返回 -1 : 键没设置过期时间 返回 -2 ：键不存在1ttl key 7. 键的类型 type若键不存在，返回 none1type key 5种常用的数据类型Redis 是字典服务器，其中键都是字符串类型，并且数据结构也都是在字符串类型上构建的。 而字符串类型的底层实现值可以为字符串，整数，浮点数，二进制。 字符串类型最大不能超过 512MB. 字符串常用命令1. 设置值 set、setex、setnx、set .. xx==set 命令的参数：== ex seconds: 秒过期 px milliseconds ： 毫秒过期 nx ：键必须不存在 （失败返回 0） xx ：键必须存在123// set key value [ex seconds] [px milliseconds] [nx | xx]set hello world ex 60 ==设置时间:== 1setex key seconds value ==设置不存在的key== setnx 可以作为分布式锁的一种实现方案 失败： 返回 0 成功： 返回 11setnx key value ==设置存在的key== 失败： 返回 nil 成功： 返回 ok1set key value xx 2. 获取值 get==获取键值== 不存在： 返回 nil 存在： 返回值1get key 3. 批量设置值 mset批量操作可以减少网络资源的浪费 1mset key1 value1 key2 value2 key3 value3 #### 4. 批量获取值 mget 如果有空的， 该空值返回 nil 1mget key1 key2 key3 #### 5. 计数操作 incr、decr、incrby(自增指定值)、decrby(自减指定值) redis 的计数不是cas, 因为是单线程，不会出现冲突 - 不存在： 自动创建并返回1 - 存在： 返回增后的值 12// 自增//incr key ### 不常用命令 #### 1. 追加值 append 可以对incr 的整数或者其他类型追加，因为它们都是string类型 1// append key value 2. 字符串的长度 strlen 不存在： 返回 0 存在： 返回字符串长度1// strlen key 3. 设置并且返回原值 getset1// getset key value 4. 设置指定位置的字符1// setrange key offset value 5. 获取部分字符串1// getrange key start end 内部编码字符串类型内部有3 种编码， int : 8字节 embstr ：&lt;= 39 字节的字符串 raw ： &gt; 39 字节的字符串 查看类型的方法 1object encoding key1 应用场景1. 缓存加速读写并减轻后端数据库的压力 推荐的key 定义规则： 123// 业务名：对象名：id：[属性]miaosha:item:itemId:price 2. 计数实现业务上的快速技术、查询缓存，能够异步的写入数据库，减少数据库的访问压力。 3. session 共享web 服务通常由多台服务器协同提供用户访问的服务，而用户登陆后的登陆信息如何保存？ 借助 Redis 缓存将 用户session 集中管理，当用户登陆和查询时，在 Redis 服务器更新或者查询即可。 4. 限速比如限制单位时间内验证码的次数。 哈希常用命令1. 设置值 hset &amp; hsetnx (不存在才设置)hset: 成功： 返回 1 失败： 返回 0 hsetnx 成功： 失败： 待验证12// hashkey : valuehset key hashKey value 2. 获取值 hget不存在： 返回nil1hget key 3. 删除值 hdel key field 删除成功：返回 1 删除失败：返回 0 12345// hset mykey key1 value1// hset mykey key2 value2hdel mykey key1// 删除后再查询会返回 nil 4. 计算field 个数 hlen1hlen mykey 5. 批量设置或者获取 hmset &amp; hmgethmset 成功返回 OK 12345hmset mykey key3 value3 key4 value 4// 批量获取返回1) &quot;value1&quot;2) &quot;value2&quot; 6. 判断field 是否存在 hexists 存在： 返回 1 不存在： 返回 01hexists mykey key3 7. 获取所有的field, hkeys123456789hkeys mykey// 返回1) &quot;key2&quot;2) &quot;key1&quot;3) &quot;key3&quot;4) &quot;key4&quot;5) &quot;key5&quot; 8. 获取所有的value， hvals mykey9. 获取 k - v ： hgetall mykey值太多时，会引起阻塞，线上可以 hscan 和 hmget 10. values 的自增 hincrby hincrbyfloat1hincrby 11. 计算value的字符串长度（需要Redis3.2以上） hstrlen1hstrlen mykey key1 内部编码哈希类型内部编码主要有两种： ziplist （更省内存）（数量小于512且value 小于64字节时，默认使用） hashtable 读写时间复杂度 O(1) 应用场景利用 hash 存储 数据库中的整行字段列表 列表用来储存多个有序的字符串。 可以当队列和堆栈使用 主要的操作： 添加 rpush &amp; lpush, linsert 删除 lpop &amp; rpop, lrem, ltrim 查找 lrange, lindex, llen 修改 lset 阻塞 blpop &amp; brpop 1. 添加 lpush &amp; rpush, linsert1234567// 左，右插入lpush elgong.list 1rpush elgong.lsit 2// 指定位置插入linsert elgong.list after| before location value 2. 删除 rpop &amp; lpop, lremlrem 删除指定元素lrem elgong.list count value value 为被删除的值 count &gt; 0: 从左往右删除 count 个 count = 0: 全部删除 count &lt; 0： 从右往左删除 count 个 1234rpop elgong.listlpop elgong.list// 删除指定的元素 3. 查找 lrange ， lindex， llen12345678// 获取从左到右 所有元素lrange elgong.list 0 -1// 指定范围查找lrange elgong.list start end// 获取指定下标的元素, 索引为2lindex elgong.list 2 4. 修改 lset1lset key index newValue 5. 阻塞 blpop &amp; brpop有参数 timeout， 如果timeout = 0， 一直阻塞12345// 等到list有值时再返回blpop elgong.list 0// 阻塞最长3sblpop elgong.list 3 内部编码 ziplist （长度小于512， 单值 小于64 字节时默认采用） linkedlist 应用场景1. 消息队列通过阻塞可以实现生产者消费者模式。 集合内部编码应用场景有序集合常用命令不常用命令内部编码应用场景键的管理常用的指令入 del, exists, expire, type, object. 6. 键的重命名 rename1rename elgong.list newlist 7. 随机返回一个键 randomKey1randomKey 8. 键过期时间9. 键的遍历123// 匹配keys [j,r]edis// 输出： jedis 和 redis 中存在的]]></content>
      <categories>
        <category>缓存中间件</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bloom Filter 布隆过滤器]]></title>
    <url>%2F2020%2F05%2F16%2FBloom-Filter-%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%2F</url>
    <content type="text"><![CDATA[[toc] [toc] 谷歌开源的Guava的布隆过滤器 布隆过滤器是什么？布隆过滤器由很长的二进制向量和一系列随机映射函数组成。 布隆过滤器可以用于检索一个元素是否在一个集合中。 它能给出的答案是： ==一定不存在 /可能存在== 布隆过滤器的应用场景？ 提升磁盘查询未命中的效率 通过bloom filter 过滤掉一定不存在的数据查询，减少访问磁盘和网络的次数。 redis限流-缓存击穿问题 redis 缓存前加一层布隆过滤器，应对缓存击穿问题 海量网页黑名单 爬虫网址判重系统 布隆过滤器的优缺点? 优点： 布隆过滤器存储空间和插入/查询时间都是常数 O（k） 随机映射函数间独立，可以并行计算 缺点： 随着存入的元素数量增加，误算率随之增加。 无法删除，因为不能确定元素是否真的在bitmap中。 实现原理布隆过滤器是一个很长的二进制向量，配合K 个随机映射函数，主要的操作是两个：插入和查找 插入时： 元素通过k 个随机映射函数，得到 k 个索引， 将二进制向量中的对应位置 置1 查找时： 元素通过k 个随机映射函数，得到 k 个索引，查看对应位置是否全为1， 如果有0则一定不存在。 怎么选择 参数？假设输入元素的个数为 n，二进制向量的长度为m（也就是布隆过滤器大小），所容忍的误判率p和随机映射函数的个数k。计算公式如下：（小数向上取整） 实现代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import java.util.BitSet;public class MyBloomFilter &#123; //2&lt;&lt;25表示32亿个比特位 private static final int DEFAULT_SIZE= 2&lt;&lt; 25; private static final int[] seeds=new int[]&#123;3,5,7,11,13,19,23,37&#125;; //这么大存储在BitSet private BitSet bits=new BitSet(DEFAULT_SIZE); private SimpleHash[] func=new SimpleHash[seeds.length]; public static void main(String[] args) &#123; //可疑网站 String value=&quot;www.baidu.com&quot;; MyBloomFilter filter=new MyBloomFilter(); //加入之前判断一下 System.out.println(filter.contains(value)); filter.add(value); //加入之后判断一下 System.out.println(filter.contains(value)); &#125; //构造函数 public MyBloomFilter()&#123; for(int i=0;i&lt;seeds.length;i++)&#123; func[i]=new SimpleHash(DEFAULT_SIZE,seeds[i]); &#125; &#125; //添加网站 public void add(String value)&#123; for (SimpleHash f : func) &#123; bits.set(f.hash(value),true); &#125; &#125; //判断可疑网站是否存在 public boolean contains(String value)&#123; if(value==null)&#123; return false; &#125; boolean ret=true; for (SimpleHash f : func) &#123; ret=ret&amp;&amp;bits.get(f.hash(value)); &#125; return ret; &#125; public static class SimpleHash &#123; private int cap; private int seed; public SimpleHash(int cap,int seed)&#123; this.cap=cap; this.seed=seed; &#125; public int hash(String value)&#123; int result=0; int len=value.length(); for(int i=0;i&lt;len;i++)&#123; result=seed*result+value.charAt(i); &#125; return (cap-1)&amp;result; &#125; &#125;&#125;]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
      <tags>
        <tag>布隆过滤器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis学习-redis的价值]]></title>
    <url>%2F2020%2F05%2F12%2Fredis%E5%AD%A6%E4%B9%A0-redis%E7%9A%84%E4%BB%B7%E5%80%BC%2F</url>
    <content type="text"><![CDATA[[toc] 本系列文章整理摘抄自 Redis 是什么？ Redis 是一种 基于键值对的 NOSQL 内存数据库, 具有丰富的数据类型和基于内存的快速读写能力,单线程命令处理机制。 Redis 的值类型有多种： string hash list set zset (有序集合) Bitmaps (位图) HyperLogLog GEO (地理信息定位) Redis 提供了一下的附加功能： 键的有效期 发布订阅 事务 Pipeline 流水线 Lua脚本 可持久化（RDB 和 AOF 两种策略） Redis 有哪些特点？Redis 有5大重要的特征： 读写速度快（读写速度可达10万/秒） 基于内存 C 语言实现 单线程架构（IO多路模型），省却了线程间调度 基于键值对的字典服务器 可持久化 主从复制 高可用 &amp; 分布式 Redis 应用场景有哪些？ 缓存 排行榜系统(提供列表和有序集合) 计数器 （网站播放数，电商浏览数） 社交网络（点赞，粉丝，共同爱好，推送，下拉刷新） 消息队列 session 共享 限制访问的次数 为什么选择单线程？官方的解释： 我们知道，运算和处理速度上，CPU &gt;&gt;&gt; 内存 &gt;&gt;&gt; 硬盘和网络。 原因1：单线程内存读写速度优于多线程的并发切换消耗 CPU并不是Redis的瓶颈，因为通常Redis要么受内存限制，要么受网络限制，而多线程的线程调度消耗的资源要比在内存读取数据大得多，所以选择了单线程 + IO 多路复用模型。 原因2：数据的一致性 单线程的操作，一定是线程安全的，并且避免了使用锁同步等。 为了最大程度地利用CPU，可以在同一服务器上启动多个Redis实例，并为每个实例绑定一个CPU. (Linux 通过 `taskset -c 1,2,3 /etc/init.d/mysql start`) Redis 的不足之处？数据有冷热之分，热点数据可以放在Redis 中，加速读写，减少对后端数据库的访问压力。 但是过于冷的数据，会浪费内存空间。 与同类型的技术的横向对比？]]></content>
      <categories>
        <category>缓存中间件</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[将博客搬至CSDN]]></title>
    <url>%2F2020%2F04%2F22%2F%E5%B0%86%E5%8D%9A%E5%AE%A2%E6%90%AC%E8%87%B3CSDN%2F</url>
    <content type="text"><![CDATA[本文为满足将博客内容搬迁至csdn 的审核要求。]]></content>
      <categories>
        <category>闲聊</category>
      </categories>
      <tags>
        <tag>闲聊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 向上转型&向下转型]]></title>
    <url>%2F2020%2F04%2F22%2FJava-%E5%90%91%E4%B8%8A%E8%BD%AC%E5%9E%8B-%E5%90%91%E4%B8%8B%E8%BD%AC%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[向上转型和向下转型 向上转型： 父类引用 指向子类对象 调用效果： 父类的属性 + 父类的方法（未被子类重写）+ 子类的方法 （重写了父类） 向下转型： 调用效果：子类的属性 + 子类的方法 例子如下： 12345678910111213141516171819202122232425262728293031package top.elgong.cast;/* Father.java */public class Father &#123; /* 静态类变量 */ public static int staticInt = 1; public static String staticStr = "father static str"; /* 实例变量 */ public int Int = 2; public String Str = "father str"; /* 会被子类覆盖的方法 */ public void say()&#123; System.out.println("被子类覆盖的方法 :say "); &#125; /* 不被子类覆盖的方法 */ public void sayOnlyFather()&#123; System.out.println("未被子类覆盖的方法 : sayOnlyFather"); &#125; /* 私有方法默认为 fianl， 不可被继承， 也不参与转型 */ private void sleep()&#123; System.out.println(" father sleep"); &#125;&#125; 12345678910111213141516171819202122232425262728package top.elgong.cast;/* Son.java */public class Son extends Father &#123; /* 子类的 变量区 */ /* 静态变量 */ public static int staticInt = 111; public static String staticStr = "son static str"; /* 实例变量 */ public int Int = 222; public String Str = "son str"; /* 子类独有的变量 */ public String strOnlySon = "str Only Son"; @Override public void say() &#123; System.out.println("子类重写的方法：say"); &#125; public void sleep()&#123; System.out.println("子类独有的方法： son sleep : "); &#125;&#125; 12345678910111213141516171819202122232425262728package top.elgong.cast;public class Test &#123; public static void main(String[] args) &#123; System.out.println(" 向上转型： "); /* 向上转型 */ Father f = new Son(); System.out.println(f.Int); // 打印 2 System.out.println(f.Str); // 打印 father str f.say(); // 打印 son say : f.sayOnlyFather(); // 打印 father say 2 /* 向下转型 */ System.out.println(" 向下转型： "); Son s = (Son)f; System.out.println(s.Int); // 打印 222 System.out.println(s.Str); // 打印 son str System.out.println(s.strOnlySon); // 打印 strOnlySon s.say(); // 打印 son say : &#125;&#125;]]></content>
      <categories>
        <category>Java多态</category>
      </categories>
      <tags>
        <tag>向上转型&amp;向下转型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[蓄水池抽样算法]]></title>
    <url>%2F2020%2F04%2F19%2F%E8%93%84%E6%B0%B4%E6%B1%A0%E6%8A%BD%E6%A0%B7%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一、预备知识Java 随机数生成的方法： java.util.Random 123456789101112131415161718192021/* Main.java */public class Main &#123; public static void main(String[] args) &#123; // 指定随机种子，默认以时间为种子 Random random = new Random(2048); // 生成 0 ~ 99 之间的整数 System.out.println( random.nextInt(100)); // 生成 0 ~ 1.0 之间的小数 System.out.println( random.nextDouble()); // 生成 布尔 System.out.println( random.nextBoolean()); &#125;&#125; 二、海量数据随机采样K个的需求从固定区间内随机采样数据十分简单，直接调用 random.nextInt() 就可以。 但如果是长度未知的海量数据流呢？该如何实现等概率采样？ ​ 答：蓄水池采样算法就是一种解决方案。 三、实现原理（从未知长度的海量数据随机采样K个元素）3.1 举例说明：从未知流中随机选择一个元素(K = 1)的实现方法 当数据流中只有一个数据： 直接返回该数据 当数据流中有两个数据： D0，D1 中随机选择一个。 概率均为 1/2 当数据流中有三个数据 Step1 : 处理 D0, D1 时 先保留一个，概率分别为 1/2 Step2 :处理 D3 时, 1/3 的概率保留D3, 1 - 1/3 的概率保留 Step1 中的结果 递推下去 3.2 解析：假设流的长度只有3数据 D1 被采样概率：（1/2） (2/3) = 1/3数据 D2 被采样概率：（1/2）(2/3) = 1/3数据 D3 被采样概率： 1/3 3.3 算法描述 先选取数据流中的前k个元素，保存在池子pool中； 从第j（k + 1 &lt;= j &lt;= n）个元素开始 每次先以概率 p = k/j选择是否让第j个元素留下； 若j被选中，则从A中随机选择一个元素并用该元素j替换它 否则直接淘汰该元素； 重复步骤2直到结束，最后集合A中剩下的就是保证随机抽取的k个元素。 四、代码123456789101112131415161718private int[] sampling(int K) &#123; int[] pool = new int[K]; // 前 K 个元素直接放入数组中 for (int i = 0; i &lt; K; i++) &#123; pool[i] = stream[i]; &#125; for (int i = K; i &lt; N; i++) &#123; // K + 1 个元素开始进行概率采样 int r = random.nextInt(i + 1); // 这里其实就是k/j的体现 if (r &lt; K) &#123; pool[r] = stream[i]; &#125; &#125; return pool;&#125; 五、leetcode 题目：T1. 随机数索引 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/* 蓄水池算法： 只对重复值采用蓄水池算法 出现次数 概率 1 1 2 1/2 3 1/3 4 1/4*/class Solution &#123; private int[] nums; public Solution(int[] nums) &#123; this.nums = nums; &#125; public int pick(int target) &#123; int index = getIndex(nums, target); return index; &#125; private int getIndex(int[] nums, int target)&#123; // 统计出现的次数 int count = 0; int index = -1; Random random = new Random(); for (int i = 0; i &lt; nums.length; i++)&#123; if (target == nums[i])&#123; if (index == -1)&#123; index = i; continue; &#125; count += 1; int r = random.nextInt(count + 1); if (r == 0)&#123; index = i; &#125; &#125; &#125; return index; &#125;&#125; T2. 链表中随机选择节点 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123; /** @param head The linked list's head. Note that the head is guaranteed to be not null, so it contains at least one node. */ private ListNode head; public Solution(ListNode head) &#123; this.head = head; &#125; /** Returns a random node's value. */ public int getRandom() &#123; int count = 0; ListNode now = head; int ret = -1; Random random = new Random(); while (now != null)&#123; count += 1; if (ret == -1)&#123; ret = now.val; now = now.next; continue; &#125; int rm = random.nextInt(count ); if (rm == 0)&#123; ret = now.val; &#125; now = now.next; &#125; return ret; &#125;&#125;]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
      <tags>
        <tag>蓄水池抽样算法 海量数据随机采样</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql-必知必会7-综合内容]]></title>
    <url>%2F2020%2F04%2F11%2Fmysql-%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A7-%E7%BB%BC%E5%90%88%E5%86%85%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[一、关系型数据库Mysql数据库（Database）是按照数据结构来组织、存储和管理数据的仓库。 数据库: 数据库是一些关联表的集合。. 数据表: 表是数据的矩阵。在一个数据库中的表看起来像一个简单的电子表格。 列: 一列(数据元素) 包含了相同的数据, 例如邮政编码的数据。 行：一行（=元组，或记录）是一组相关的数据，例如一条用户订阅的数据。 冗余：存储两倍数据，冗余可以使系统速度更快。 主键：主键是唯一的。一个数据表中只能包含一个主键。你可以使用主键来查询数据。 外键：外键用于关联两个表。 复合键：复合键（组合键）将多个列作为一个索引键，一般用于复合索引。 索引：使用索引可快速访问数据库表中的特定信息。索引是对数据库表中一列或多列的值进行排序的一种结构。类似于书籍的目录。 参照完整性: 参照的完整性要求关系中不允许引用不存在的实体。与实体完整性是关系模型必须满足的完整性约束条件，目的是保证数据的一致性 主键: 表示特定行. 主键不能重复 每行必有主键,且不能为 NULL 外键: product 只存产品信息, 和供应商ID, vendors 存供应商的信息(主键是ID),则vendors的主键就是product的外键. MySQL支持大型数据库，支持5000万条记录的数据仓库，32位系统表文件最大可支持4GB，64位系统支持最大的表文件为8TB。 二、 安装与删 删除mysql sudo apt purge mysql-* sudo apt autoremove 安装mysql 123sudo apt-get install mysql-server sudo apt install mysql-client sudo apt install libmysqlclient-dev ` 数据库规范： 关键字大写，表名，列名小写 索引从1开始 每条命令用分号隔开 注释 单行注释 # 单行注释 — 注释文 多行注释 / / 索引从1开始！ 三、常用命令指令执行顺序：SELECT FROM WHERE GROUP BY HAVING ORDER BY LIMIT. 开始 **->** FROM子句 **->** WHERE子句 **->** GROUP BY子句 **->** HAVING子句 **->** ORDER BY子句 **->** SELECT子句 **->** LIMIT子句 **->** 最终结果 每个步骤都会为下一个步骤生成一个虚拟表 1. 登陆系统, 选择数据库123456789101112131415mysql -u 用户名 -p 密码mysql -h localhost -P 3306 -p# 查看所有数据库列表SHOW DATABSASES;# 查看选择的数据库中的表的列表SHOW TABLES;# 查看表中的列有哪些SHOW COLUMNS FROM 表名; || DESCRIBE 表名;# 选择库USE 数据库的名字;# 查看表结构DESC 表名; 2 基础查询——检索 SELECT + DISTINCTSELECT 子句 固定的顺序: SELECT FROM WHERE （原始表有的字段） GROUP BY HAVING （分组后有的字段）ORDER BY LIMIT. 123456789101112131415161718192021222324# 选择多个字段 `着重号`当字段名与关键字冲突，用它可以避免冲突SELECT id, `name`, price FROM 表名; # 起别名 AS, 可以省略SELECT salary &quot;Month Salary&quot; from employees;SELECT salary AS &quot;Month Salary&quot; from employees;# 字符串拼接 concat# 特别注意+： # 1+9=10 两个数值型做加法# &apos;12&apos;+ 3 = 15 字符转整数，再加 # &apos;job&apos;+2 = 2 转换失败，则字符串变0# null+任何值 = nullSELECT concat(&apos;a&apos;, &apos;b&apos;, &apos;c&apos;) , price FROM 表名; # 检索所有字段SELECT * FROM 表名;# 字段去重 DISTINCTSELECT DISTINCT id FROM 表名; # 不能应用于多列# 限制检索结果 SELECT id FROM 表名 LIMIT 5; # 前5 SELECT id FROM 表名 LIMIT 2,5; #从 2+1 开始的 5个行 SELECT id FROM 表名 LIMIT 5 OFFSET 3; # 从行3取5 3. 排序检索 ORDER BY (默认升序) + LIMIT, OFFSET, DESC可以根据非 select 字段排序。 SELECT 查询列表 FROM 表 【where 筛选条件】 ORDER BY 排序列表 [asc | desc] 12345678910111213# 按照某列排序, 多条件排序SELECT id FROM 表名 ORDER BY id ASC, age DESC; # 可以根据其他列来排序# 按照多个条件排序SELECT id FROM 表名 ORDER BY age, size; # 优先age, 重复时才根据size排序.# 指定降序 DESCSELECT id FROM 表名 ORDER BY age DESC size; # DESC 只作用于在DESC 前面的, 所以 size仍然为升序# 找到最********的idSELECT id FROM 表名 ORDER BY age LIMIT 1;# 第二最的*******idSELECT id FROM 表名 ORDER BY age LIMIT 1 OFFSET 2; 4. 条件查询4.1 ——逻辑运算 WHERE + AND, OR,NOT IN 和EXISTS(看优化部分)作用： 连接条件表达式 **如果计算次序不加括号时, 优先 AND** 12345678910111213# = != &lt; &gt; &gt;= &quot;BETWEEN 1 AND 2&quot;在指定值之间,包含端点# 不等于 ！= 或者 &lt;&gt;SELECT id FROM 表名 WHERE age=12 ORDER BY size;# 组合筛选 AND ORSELECT id FROM 表名 WHERE age=12 AND size &lt; 10;# 计算次序, 不加括号时, 优先 AND# 解释: id&gt;3且age&gt;10, 或者 id=1SELECT id FROM 表名 WHERE id=1 OR id=3 AND age &gt; 10; # NOT 否定后跟的所有条件.SELECT id FROM 表名 NOT WHERE id IN (1002, 1003) 4.2 ——模糊查询 WHERE + LIKE, between and, in, is null like + 通配符： 参考7. between and ： 包含临界值， 不可颠倒顺序 in： 12345678# between andSELECT * FROM 表名 WHERE id BETWEEN 100 and 120;# IN 取值必须在括号内SELECT id FROM 表名 WHERE id IN (1002, 1003)# IS NULL 筛选出空值 IS NOT NULLSELECT id FROM 表名 WHERE age IS NULL; 补. 空值处理 IFNULL(字段，空值时返回值) SELECT IFNULL(price, 0) FROM 表名; 5. 通配符 LIKE + % , _ + 正则表达式 REGEXP 都**不区分大小写** 通配符速度慢, 不要放在搜索开始处 LIKE 匹配整个串, 正则表达式可以匹配子串 12345678910111213141516171819202122232425262728293031323334353637383940# 通配符# % 匹配0,1,多个字符SELECT id FROM 表名 WHERE string LIKE &apos;s%&apos;; # s开头 # 下划线 _ , 匹配单个字符# 需要匹配 _ 时， 用转义 \_SELECT id FROM 表名 WHERE string LIKE &apos;s_&apos;; ------------------------------------------------------------------# 正则表达式 REGEXP# 标准表达SELECT name FROM customers WHERE name REGEXP &apos;1000&apos;;&apos;.&apos; # 任意一个字符&apos;A1&apos;| &apos;B2&apos; # 匹配两个串之一&apos;[1-9]&apos; # 匹配 1~9 范围内的值&apos;[123]&apos; # 匹配1，2，3之一， 等价于【1 | 2 | 3】&apos;[^123]&apos; # 匹配非123的值&apos;\\.&apos; # 特殊字符转译 # 匹配多个实例, 不能单独出现,必须是指定上边的某种字符的匹配# 例如 &apos;.*&apos; 而不是&apos;*&apos;&apos;*&apos; # 0或多个匹配&apos;+&apos; # 1或多个匹配&apos;?&apos; # 0或者1个匹配&#123;n&#125; # 指定数目匹配&#123;n,&#125; # 不少于指定数目的匹配&#123;n,m&#125; # 数目范围,不超过255#定位元字符&apos;^&apos; # 开始位置&apos;$&apos; # 结尾&apos;[[:&lt;:]]&apos; # 词开始&apos;[[:&lt;:]]&apos; # 词结尾# 举例:&apos;^[1-9]&apos; BINARY &apos;J 1000&apos; # 指定区分大小写&apos;[a-zA-Z0-9]&apos; # 匹配所有字符 6. 数据处理常用函数 (不区分大小写) 字符函数 数学函数 日期函数 其他函数 流程函数 字符函数 12345678910111213141516171819202122232425262728293031# 文本处理函数# 1. 字符串字节个数, 汉字算三个字符Length() # 2. 拼接CONCAT(id, &quot;_&quot;, name)# 3. 大写 小写Lower() # 小写Upper() # 大写# 4. 返回子串的字符, 数据库索引从1开始SubStr(last_name, start) SubStr(last_name, start, length) # 长度# 5. 查找子串, 返回第一次出现的索引， 查不到返回0INSTR(&quot;待查子串abcd&quot;, &quot;a&quot;)# 6. 去空格Trim(), LTrim(), RTrim() # 7. 指定长度填充LPAD(name, length, &apos;*&apos;)RPAD(name, length, &apos;*&apos;)# 8. 替换REPLACE(原串, &apos;被替换串&apos;, &apos;新串&apos;)# 9. 字符串字符长度, 汉字也算 1个字符CHAR_LENGTH(s) Soundex() # 返回串的SOUNDEX值 数学函数 12345678910111213141516171819# 1. 四舍五入ROUND(1.6); # 2ROUND(1.567, 2) # 小数点保留两位# 2. 上取整， &gt;=该参数的最小整数CEIL(1.00) # 下取整, &lt;=该参数的最大整数FLOOR(-9.99) # -10# 3. 小数点直接截断TRUNCATE(1.69999, 1) # 1.6# 4. 取余数MOD(10, 3) # 10%3ABS(x) # 绝对值AVG(age) # 某列的平均值 EXP(x)RAND() # 0到1的随机数 日期函数 123456789101112131415NOW() # 当前日期和时间CURDATE() # 当前日期，不含时间Date() # 返回时间中的日期部分....Day() # 返回时间中的天数部分Year(NOW())Time()Month() Hour()DateDiff() # 计算日期差# 字符串转日期STR_TO_DATE(&apos;02-19-2020&apos;, &quot;%m-%d-%Y&quot;)# 日期的格式化输出DATE_FORMAT(NOW(), &apos;%y年%m月%d日&apos;) 其他函数 VERSION() 7. 流程控制函数IF(逻辑判断， 成立执行， 不成立执行) CASE: 123456789101112131415161718192021# IFIF(10&gt;5, &apos;大&apos;, &apos;小&apos;)SELECT name IF(salary IS NULL, &quot;没薪水&quot;, &quot;有薪水&quot;)# CASE 第一种使用CASE &apos;要判断的表达式&apos;WHEN &apos;常量1&apos; then 值(没有分号) / 表达式(有分号);WHEN &apos;常量2&apos; then 值(没有分号) / 表达式(有分号);WHEN &apos;常量3&apos; then 值(没有分号) / 表达式(有分号);...ELSE 值(没有分号) / 表达式(有分号);END;# CASE 第二种语句CASE WHEN 表达式 then 值(没有分号) / 表达式(有分号);WHEN 表达式 then 值(没有分号) / 表达式(有分号);WHEN 表达式 then 值(没有分号) / 表达式(有分号);...ELSE 值(没有分号) / 表达式(有分号);END; 8. 数据汇总-聚集函数 AVG, COUNT, MAX, MIN, SUM运行在行组, 计算和返回单个值的函数. 统计使用 12345678910111213141516# AVG() 针对单列, 对多列需要使用多个SELECT AVG(age) AS avg_age FROM 表名; # 忽略 NULL# COUNT() 函数COUNT(1); # 行数COUNT(*); # 表的行数, 含有 NULL的有数据行也可，但是不能全NULLCOUNT(column) # 某列非NULL 的个数COUNT(distinct 字段) # 统计不重复的# 效率对比：MYISAM 储存引擎下， COUNT(*) 效率高INNODB 存储引擎下，COUNT(*) 和COUNT(1) 差不多，比COUNT(字段高)# 聚集不同的值 + DISTINCT# 聚集函数默认ALLSELECT AVG(DISTINCT age) FROM 表名; # 对唯一值求均值SELECT COUNt(DISTINCT age) FROM 表名; # 对age列的唯一值统计个数 9. 数据分组查询 —— GROUP BY, HAVING1. GROUP BY 分组字段​ 如果分组列中具有 NULL, 则NULL 将作为一个分组返回. 2. HAVING 过滤条件 =====WHERE 条件3. 必加 ORDER BY, 因为G出来的结果不保证排序了.4. 能where 就不用having 按字段分组 GROUP BY id 按表达式或者函数 GROUP BY length(id) AS len HAVING len&gt;3; 按多个字段分组 12345# 分组统计值SELECT id, COUNT(*) AS num_ FROM 表名 GROUP BY id ORDER BY age;# 分组过滤 大于2的值SELECT age FROM 表名 GROUP BY id HAVING COUNT(*)&gt;=2 ORDER BY age; 10. 子查询 IN + 括号 查询的结果作为另一个查询的条件,然后多层嵌套. 内层查询建立一个临时表。费时间. 优化需要用join 联结表替代…. where 和 having 后可放的子查询： 子查询放在小括号内 标量子查询（单值），一般配合单行操作符使用： &gt; &lt; &gt;= = &lt;&gt; 列子查询（单列多行）， 一般配合多行操作符使用： IN 列表中的一个 ANY/SOME ALL select 后可以放的子查询： from 后可以放的子查询： 必须起别名 FROM (子查询表) newtable 12345678910# 标量子查询# 1. 谁工资比 elgong 高SELECT * FROM employee WHERE salary&gt;(select salary from emplot WHERE name = &apos;elgong&apos;);# 查询超过平均工资的员工信息select avg(sal) from emp; /* avg(sal)=2000 */select * from emp where sal &gt;= 2000;/* 子查询方法 */select * from emp where sal &gt;= (select avg(sal) from emp); 11. 连接查询——JOIN 联结表 ( INNER JOIN, LEFT JOIN, RIGHT JOIN)注意判断驱动表是哪个？ 查询计划 explain 正常情况下，from 后的表是驱动表，但是当出现下面情况，驱动表变成了 class。 分析： 当 where 后的条件使得表之间数据多少不一样时，数据少的为主表 小表驱动大表的原则 select * from student left join class on class.classid = student.classid where class.classid = 2; 概念: 联结指两张表,或者多张表之间组合在一起进行查询, 是在运行时做的操作; left JOIN (左联结)保证读取主表的全部数据 right JOIN (右联结) 保证读取主表的全部数据 inner JOIN (内部联结,等值联结) 只读取共有的数据 自联结: 常用来代替从同一个表检索数据的子查询. FROM table1 AS t1, table2 AS t2, WHERE t1.col = t2.col; 自然联结: 联结有主副表之分, 在INNER JOIN 之前的是主表, 待查询的输出中,排前边的内容所在的表为主表. 左外联结可通过颠倒FROM 或者WHERE 子句中表的顺序转换成右外部联结 连接的分类 SQL92语法 等值连接 FROM table1 AS t1, table2 AS t2, table3 AS t3 WHERE t1.col = t2.col AND t2.c = t3.c ; 自连接（单表） 同表不同名 FROM employees e1, employees e2 WHERE e1.id = e2.iidd SQL99语法 SELECT 查询列表 FROM 表1 别名 [连接类型] join 表2 别名 on 连接条件 内连接 inner join 外连接 left， right 主表全部显示 从表中没有与主表匹配的结果，显示NULL 等价于==== 内连接结果 + 主表有而从表没有的记录 左外和右外，交换表顺序可以等价效果 全外连接 full join 交叉连接 cross join 笛卡尔乘机 非等值连接 FROM e join g on e.salary BETWEEN g.low AND g.upper 自连接 一样的join on 不同名的同一张表 1234567891011121314151617# 创建联结表的两种方式# 1. 等值连接 WHERE 表1.id = 表2.id AND 表2.size = 表3.sizeSELECT vendors.vendor_name, product.prod_name, product.prod_price FROM product,vendors WHERE vendors.vendor_id = product.vendor_id ORDER BY vendor_name;# 2.内部联结(等值联结) 表的键数量相同 FROM 表1 INNER JOIN 表2 ON 表1.id = 表2.idSELECT v.vendor_name, p.prod_name, p.prod_price FROM product AS p INNER JOIN vendors AS v ON v.vendor_id = p.vendor_id ORDER BY v.vendor_name;# # 对联结的表使用聚合方法待补充..... 12. 分页查询 LIMIT 行X(从0开始), size; LIMIT size OFFSET size; 当要显示的数据，需要分页显示 从行0开始 从第四行开始，检索5行 LIMIT 3, 5 LIMIT 5 OFFSET 3 12345678# 查询前5条数据SELECT * FROM employees LIMIT 0, 5; # 查询11到25条数据SELECT * FROM employees LIMIT 10, 25-11+1;# 计算公式LIMIT (page-1)*size, size; 13. 联合查询 union （自动去重，union all 不去重）将多条查询语句合并成一个结果 特点： 查询 列数 和 列顺序 必须一致 自动去重 不去重 union all 123SELECT * FROM e1 WHERE UNIONSELECT * FROM e1 WHERE 14. 视图视图是虚拟的表, 是对其基表的封装. 使用的好处: 重用 SQL 语句 使用表的部分,即过滤掉部分数据 限制: 图名唯一 视图可以嵌套 视图的ORDER BY 次于 从该视图检索数据的ORDER 视图可以和表一起使用 123# 创建视图CREATE VIEW viewname ASSELECT * FROM table WHERE id!=1;]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql-必知必会系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql-必知必会2-数据操作语言DML]]></title>
    <url>%2F2020%2F04%2F11%2Fmysql-%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A2-%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C%E8%AF%AD%E8%A8%80DML%2F</url>
    <content type="text"><![CDATA[DML 数据操作语言 插入 INSERT 更新 UPDATE 删除 DELETE 1. 插入规则： 插入值的类型要一致 语法1： ​ INSERT INTO 表名（列名） VALUES ( 值1...) 语法2： ​ INSERT INTO 表名 SET 列名1=值1，列名2=值2 1234567891011121314151617181920212223242526272829303132# 插入 INSERT# 插入完整行,或者部分INSERT INTO customers( cust_name, cust_address, cust_city, cust_state)VALUES( &apos;elgong&apos;, &apos;1552460315&apos;, &apos;hangzhou&apos;, &apos;1&apos;);# 插入多行INSERT INTO customers( cust_name, cust_address, cust_city, cust_state)VALUES( &apos;elgong&apos;, &apos;1552460315&apos;, &apos;hangzhou&apos;, &apos;1&apos;),( &apos;gel&apos;, &apos;178905324&apos;, &apos;hangzhou&apos;, &apos;0&apos;); 2. 更新（缺了where 就全部更新啦，一定要注意） 单表更新语法： 1UPDATE 表名 SET 列名1=值 ... WHERE 筛选条件; 多表更新语法： 123UPDATE 表名 SET 列名1=值... WHERE 连接条件 AND 筛选条件; 123456# 更新 UPDATE SET# 更新某行的某些列值UPDATE customers SET cust_email = &quot;1552460315@qq.com&quot;, # 列1 cust_name = &quot;ELGONG&quot;WHERE cust_id = 1; 3. 删除 删除整行 1234567# 删除# 删除特定行DELETE FROM customers WHERE cust_id=1;# 删除所有行DELETE FROM customers ; TRUNCATE TABLE;]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql-必知必会系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql-必知必会1-数据定义语言DDL]]></title>
    <url>%2F2020%2F04%2F11%2Fmysql-%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A1-%E6%95%B0%E6%8D%AE%E5%AE%9A%E4%B9%89%E8%AF%AD%E8%A8%80DDL%2F</url>
    <content type="text"><![CDATA[数据库和表的创建，修改，删除 创建 create 修改 alter 删除 drop 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# ######## 数据库相关# 创建库CREATE DATABASE 库名 IF NOT EXISTS;# 修改库名RENAME DATABASE books TO 新库名;# 删除库DROP DATABASE IF EXISTS books;# ######### 表相关# 创建表CREATE TABLE IF NOT EXISTS customers( cust_id int NOT NULL AUTO_INCREMENT, # 自动增加 cust_name char(50) NOT NULL DEFAULT GEL, # 设默认值 cust_address char(50) NULL, cust_city char(50) NULL, cust_state char(5) NULL, cust_email char(255) NULL, PRIMARY KEY (cust_id) # 指定主键) ENGINE=InnoDB;CREATE TABLE IF NOT EXISTS student( id int NOT NULL AUTO_INCREMENT, name char(50) NOT NULL, address char(50) NULL, city char(50) NULL, email char(255) NULL, PRIMARY KEY (id) ) ENGINE=InnoDB;# 更新表# 添加一列 ADDALTER TABLE customers ADD cust_phone CHAR(20);# 删除表 DROP TABLE customers# 修改表名RENAME TABLE customers TO customers222; # 修改列名ALTER TABLE book CHANGE COLUMN 旧名 新名 类型;# 修改列名，约束ALTER TABLE book MODIFY COLUMN 列名 类型;# 删除一列 DROPALTER TABLE customers DROP COLUMN cust_phone;]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql-必知必会系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[版本控制-git]]></title>
    <url>%2F2020%2F03%2F28%2F%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6-git%2F</url>
    <content type="text"><![CDATA[1. git 的作用 版本控制 协同开发 2. 文件的状态 untracked (新建的文件) unmodified （提交后进入仓库的文件与当前文件相同，即没修过） modified (commit 之前) staged （commit 之后） 3. 一般使用流程初始化仓库 git init 变更的文件加入暂存区 git add . 提交变更 git commit -m 查看commit日志, 并返回某一次提交的版本 git log (### 弹出commit id) git reset 7hdadsu2qe21e921821e --hard 如果想恢复最新的 git relog 从暂存区 移除某些文件（add 的文件有多余） git reset &lt;fileName&gt; 4. 分支合作管理 创建分支 git checkout -b &lt;分支name&gt; &lt;template继承的commit,默认当前&gt; 切换分支 git checkout master 查看所有分支 git branch 合并分支的变更（合并到当前master） git meger branch-2 有冲突时，会提示====== 5. remote 仓库的使用 下载远端仓库到本地 git clone ......git 创建本地的分支 git checkout -b local-A 在远端仓库设置分支(第一次需要) git push -set-upstream origin local-A 提交本地分支到远端 git push 第一次拉取远端仓库的分支，到本地 git fetch git checkout -b &lt;name&gt;origin&lt;template继承的commit,默认当前&gt; 以后再从远端更新本地 git pull （自动fetch + merge） 其他命令 git merge git pull git fetch git rebase （版本合并时。。）]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>版本控制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合-目录]]></title>
    <url>%2F2020%2F01%2F01%2FJava-%E9%9B%86%E5%90%88%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Java 集合类都在java.util包中，提供了如可变长数组，集合，队列，堆栈，链表等数据结构。宏观上看整个结构可以分为两大部分： 属于单列集合Collection 接口和带映射的多列集合的Map接口。 Collection 接口：集合的基本操作和属性 List 接口：有序列表 ArrayList： 可变长的数组。 LinkedList： 双端队列的链表结构。 Set 接口： 不重复元素的集合 HashSet： HashMap 实现的，无序。 TreeSet： HashMTree 实现的，有序。 Queue 接口： Deque 接口： LinkedList Map 接口：key-value键值对的映射接口 HashMap：数组+链表 组成的哈希表，无序。 TreeMap：基于 红黑树的排序顺序 存储键/值对，有序。 Hashtable： HashMap类很相似，支持同步。 两个工具类： Arrays Collections]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机基础学习笔记]]></title>
    <url>%2F2019%2F12%2F16%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Computer-Basics-Notes-Linksgithub地址 学习笔记 我在学习计算机基础的过程中整理了一部分笔记，但都比较零散，有些稍微连贯的内容已经整理为博客，而大部分还没来得及整理成md格式，我先整理出链接。为了尽可能的清晰展示笔记脉络，下文将分门别类的列出，有需要请自取。笔记内容大部分都是参考了网上的博客以及书籍，整理笔记的习惯也是刚刚养成的，从开始整理笔记后，发现自己对于做过的东西，能够随着笔记很快的回忆起来，遇到一些问题能够快速从笔记里找到答案也是比较舒服的。笔记是用有道云分享的，我未来仍然会继续梳理这些笔记，如果有任何的错误或者意见，麻烦您联系我哈。 第一章 计算机基础篇 基础部分是一些语言相关的知识点。 1.1 Java 相关面向对象 Java面向对象-多态 部分源码分析 java.lang.Object 类 java.lang.Integer等基本类型包装类 接口和抽象类 接口和抽象类的概念 java.io.Serializable接口 java.lang.AbstractStringBuilder java.lang.CharSequence接口 java.lang.Comparable 接口 java.lang.Iterable 接口 集合类 Java集合的结构 java.util.Arrays工具类 java.util.Collections 工具类 集合类之间的转换 Collection java.util.Collection 接口 java.util.List 接口 java.util.ArrayList 类 java.util.LinkedList 类 java.util.Vector类-线程安全 java.util.Stack类-线程安全 java.util.Queue 接口 java.util.Deque 接口 java.util.ArrayDeque 类 java.util.PriorityQueue 类 java.util.Set 接口 java.util.HashSet 类 Map java.util.Map 接口 java.util.TreeMap 类 java.util.HashMap 类 java.util.HashSet 类 IO 模型与IO流 标准步骤： 针对oj系统中的输入问题 java IO之AutoCloseable接口 IO篇1-Java IO模型 IO篇2-Java NIO IO篇3-Java NIO 零拷贝 IO篇4-Linux 内核的 select&amp;epoll 多线程与并发 synchronized 锁的JVM中实现原理-偏向 线程的创建 线程间的通信 线程的生命周期 对象和变量的并发访问（可见性，原子性) java.util.concurrent.locks包 J.U.C多线程1-Executor 框架的梳理 J.U.C多线程2-ThreadPoolExecutor线程 实现BlockingQueue接口的阻塞队列 J.U.C多线程3-CAS比较和交换 J.U.C多线程3-AQS 同步器框架的梳理 J.U.C多线程4-AQS框架的应用 其他 泛型（泛型接口、泛型类、泛型方法） 字符串类型-String,StringBuilder java 编码规范- google Java8- Lambda 表达式 动手实现ArrayList java 知识点梳理 Java 刷题遇到的问题 基本数据类型and 初始化 jvm 虚拟机 jvm1-内存模型-运行时数据区 jvm2-类加载机制 jvm3-对象的创建过程 jvm4-垃圾回收机制 自定义类加载器实现热部署，热替换 其他 反射 Java 动态代理 1.2 数据库相关数据库的安装 windwos 环境中 mysql 数据库安装 ubuntu 环境中 mysql 安装 学习笔记 JDBC 使用 mysql必知必会 数据库-三大范式 数据库ER图基础 mysql必知必会1-DDL数据定义语言 mysql必知必会2-DML数据操作语句 mysql必知必会3-TCL事务控制语言 mysql必知必会4-数据类型和约束 mysql必知必会5-视图 mysql必知必会6-变量，存储过程，函数…. mysql必知必会7 索引优化1-索引的概念 索引优化2-Mysql索引的底层实现 索引优化3- explain 查看执行计划 Sql语句优化-查询截取分析 mysql-数据库锁的机制和原理 1.3 计算机网络相关 互联网协议入门1-模型分层 互联网协议入门2-访问网页的过程 互联网协议入门3-TCP协议详细内容 互联网协议入门4-Http协议（应用层) 1.4 Python 相关 Anaconda 使用 python 性能优化 python 内存管理 python 包管理 知识积累 python 数据模型 Set 集合 交并差运算 pandas 包 pandas pandas-1数据结构 pandas-2索引和选择数据 pandas-分层和多级索引 pandas-4分组与聚合 pandas-5缺失值处理 pandas-6重复值处理 pandas-7时间处理 1.5 数据结构与算法相关链表 链表入门-单链表 双向链表 树结构 树结构1-二叉树的种类 树结构2-二叉树的四种遍历 树结构3- 二叉排序树 树结构4- 完全二叉树-堆排序 树结构5-红黑树 栈 栈 - Stack 动态规划 动态规划-DP DFS和回溯算法- 暴力搜索的优化方案 一般算法 二分查找 最大公约数与最小公倍数 素数和合数 排序算法-Java版 蓄水池采样算法 1.6 设计模式 单例模式 汇总 第二章 计算机提高篇 提高篇是一些具体的学习方向，我学的比较杂。 2.1 机器学习与数据挖掘 特征工程中的问题 决策树 决策树-1基本概念 决策树-2 ID3算法 决策树-3 CART分类回归树 CART 分类回归树 sklearn 决策树使用技巧 sklearn整理-决策树 神经网络 常见的神经网络 集成学习 集成学习提升算法-Adaboost XGBoost -python package introduction 梯度提升树-GBDT sklearn整理-集成学习-随机森林 模型评价 ROC与AUC 机器学习的方差与偏差 参与的一些竞赛 ali-新人实战赛o2o优惠券使用预测 ccf-2019 dc-2019-商品购买转化率预测 竞赛提升方法-tricks sklearn 脑图 2.2 大数据与Hadoop多台机器的分布式环境安装 1.hadoop分布式集群安装 RPC Hadoop RPC mapreduce 实验 实验1. 多表关联 实验2. 最高温度统计 实验3. 单表关联 2.3 VUEVUE 安装 开发环境搭建 2.4 Spring 框架 Spring-1控制反转（IOC）和依赖注入 Spring-2AOP 面向切面编程-基于动态代 Spring-3事务管理Transaction Manager 第三章 工具使用 vscode 使用记录 第四章 收藏的书籍 书籍收藏]]></content>
      <categories>
        <category>计算机基础梳理</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树2-ID3算法]]></title>
    <url>%2F2019%2F09%2F12%2F%E5%86%B3%E7%AD%96%E6%A0%912-ID3%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[决策树-2 ID3算法 决策树-1基本概念中已经提到了ID3算法，这篇博客再梳理一遍，算法描述部分搬运了统计学习方法的内容，更详细内容可以参考这本书。 ID3 算法的思路 输入：训练集 D, 特征集 A， 阈值 ε； 分叉：最优属性划分依据是 最大信息增益； 结束条件：用完所有特征，特征信息增益很小，树的深度超过限制； 返回：一颗树T。 ID3 算法描述 这里的描述可作为编程实现时的指导，树的建立过程是递归实现。1234567891011121314151617181920212223def ID3Tree(D, A，ε ): if D 的实例属于同类别 K || 特征集 A 特征空: 1. 决策树 T为单结点树 2. 标记类别 K (数量最多的类别) return T else: 1. 计算所有特征相对于 D 的信息增益 2. 找到信息增益最大特征 Amax if Amax 小于 阈值 ε: 1. 决策树 T为单结点树 2. 标记类别 K (数量最多的类别) return T else: 依照特征 Amax 的每一个取值 ai，划分数据集Di, 并且标记Di类别，构建子节点 # 递归 对每个结点Di，以Di为训练集， A-Amax为特征集，递归调用 ID3Tree() return 由结点和子结点构成的树]]></content>
      <categories>
        <category>机器学习方法</category>
      </categories>
      <tags>
        <tag>ID3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树3-CART分类回归树]]></title>
    <url>%2F2019%2F09%2F12%2F%E5%86%B3%E7%AD%96%E6%A0%913-CART%E5%88%86%E7%B1%BB%E5%9B%9E%E5%BD%92%E6%A0%91%2F</url>
    <content type="text"><![CDATA[CART-分类回归树CART 算法的思路 特征选择：最优属性划分依据是 基尼系数（分类）/平方误差（回归）； CART 树是二叉树结构。 主要就两步骤： 树的生成 树的剪枝 分类树 分类树与ID3, C4.5的流程一致。 回归树 回归树选择最佳划分属性和划分点时的依据是 平方误差。 一张图即可理解。 与分类树的主要区别是选择最佳属性的评价指标变了。根据最小化均方误差的原则选择。]]></content>
      <categories>
        <category>机器学习方法</category>
      </categories>
      <tags>
        <tag>CART树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas-8分层和多级索引]]></title>
    <url>%2F2019%2F08%2F13%2Fpandas-%E5%88%86%E5%B1%82%E5%92%8C%E5%A4%9A%E7%BA%A7%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[pandas -8 分层和多级索引 Multi-level indexing. 在 “pandas -2 索引和选择数据” 一节中, 已经提到了如何选择行列元素, 而Series 和 Dataframe 是低维度的数据结构，对于更高维度的数据，可以通过分层和多级索引来实现。 分层索引的创建 创建分层索引的方式有很多, 这里直接摘抄自官方的文档，可以通过元组，列表，Dataframe, arrays 等方式生成分层索引。 同时要知道，通过 groupby 分组操作之后得到的也是这种分层结构。 123456789101112131415161718192021222324252627282930313233343536373839404142434445 // 1. 元组In: arrays = [[&apos;bar&apos;, &apos;bar&apos;, &apos;baz&apos;, &apos;baz&apos;, &apos;foo&apos;, &apos;foo&apos;,&apos;qux&apos;, &apos;qux&apos;], [&apos;one&apos;, &apos;two&apos;, &apos;one&apos;, &apos;two&apos;, &apos;one&apos;, &apos;two&apos;, &apos;one&apos;, &apos;two&apos;]] tuples = list(zip(*arrays))Out: [(&apos;bar&apos;, &apos;one&apos;), (&apos;bar&apos;, &apos;two&apos;), (&apos;baz&apos;, &apos;one&apos;), (&apos;baz&apos;, &apos;two&apos;), (&apos;foo&apos;, &apos;one&apos;), (&apos;foo&apos;, &apos;two&apos;), (&apos;qux&apos;, &apos;one&apos;), (&apos;qux&apos;, &apos;two&apos;)]In: index = pd.MultiIndex.from_tuples(tuples, names=[&apos;first&apos;, &apos;second&apos;]) df = pd.Series(np.random.randn(8), index=index) Out: first second bar one 0.469112 two -0.282863 baz one -1.509059 two -1.135632 foo one 1.212112 two -0.173215 qux one 0.119209 two -1.044236 dtype: float64 // 2. dataftame index = pd.MultiIndex.from_frame(df) // 3. arrays In: arrays = [np.array([&apos;bar&apos;, &apos;bar&apos;, &apos;baz&apos;, &apos;baz&apos;, &apos;foo&apos;, &apos;foo&apos;, &apos;qux&apos;, &apos;qux&apos;]), np.array([&apos;one&apos;, &apos;two&apos;, &apos;one&apos;, &apos;two&apos;, &apos;one&apos;, &apos;two&apos;, &apos;one&apos;, &apos;two&apos;])] s = pd.Series(np.random.randn(8), index=arrays) 从DataFrame 产生 MultiIndex1df = df.set_index([&apos;col1&apos;,&apos;col2&apos;]) MultiIndex 转化成 列1df = df.reset_index() 选择不同层 查看不同层的索引值。 1234567In: index.get_level_values(0) index.get_level_values(&quot;name&quot;) Out: Index([&apos;bar&apos;, &apos;bar&apos;, &apos;baz&apos;, &apos;baz&apos;, &apos;foo&apos;, &apos;foo&apos;, &apos;qux&apos;, &apos;qux&apos;], dtype=&apos;object&apos;, name=&apos;first&apos;) 根据不同层索引 123456df[&quot;bar&quot;]df[&quot;one&quot;]df[&quot;bar&quot;][&quot;one&quot;]// 元组df.loc[(&apos;bar&apos;, &apos;two&apos;)] 注意, 切片时不会改变 多层索引。]]></content>
      <categories>
        <category>pandas系列教程</category>
      </categories>
      <tags>
        <tag>pandas-MultiIndex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql-索引篇2-索引的底层实现]]></title>
    <url>%2F2019%2F08%2F12%2Fmysql-%E7%B4%A2%E5%BC%95%E7%AF%872-%E7%B4%A2%E5%BC%95%E7%9A%84%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[一、常用的索引底层结构有哪些？==索引是一种排序的，便于查找的数据结构。== 下面是一些常见数据结构的概念，具体每种类型的详细特点，再去看别的文档吧。 1. 二叉查找树：​ 左子树的键值小于根的键值，右子树的键值大于根的键值。 2. AVL 树：​ 平衡二叉树（AVL 树）在符合二叉查找树的条件下，还满足任何节点的两个子树的高度最大差为 1。 3. 红黑树：​ 每个节点都带有颜色属性的二叉查找树。 4. 平衡多路查找树（M阶的 B树）：为磁盘等外存储设备设计的一种平衡查找树。 每个节点最多有m-1个关键字（可以存有的键值对）。 根节点最少可以只有1个关键字。 非根节点至少有m/2个关键字。 每个节点中的关键字都按照从小到大的顺序排列，每个关键字的左子树中的所有关键字都小于它，而右子树中的所有关键字都大于它。 所有叶子节点都位于同一层，或者说根节点到每个叶子节点的长度都相同。 ==每个节点都存有索引和数据，也就是对应的key和value。== ==数据库中的节点 value 其实是指针。。== 5. B+树：B+树其实和B树是非常相似的，我们首先看看相同点。 根节点至少一个元素 非根节点元素范围：m/2 &lt;= k &lt;= m-1 不同点： B+树有两种类型的节点：==内部节点不存储数据，只存储索引，数据都存储在叶子节点。== 内部结点中的key都按照从小到大的顺序排列，对于内部结点中的一个key，左树中的所有key都小于它，右子树中的key都大于等于它。叶子结点中的记录也按照key的大小排列。 每个叶子结点都存有相邻叶子结点的指针，叶子结点本身依关键字的大小自小而大顺序链接。 父节点存有右孩子的第一个元素的索引。 二、Mysql 的两种存储引擎的索引文档主要介绍 MyISAM 和 InnoDB 两个存储引擎的索引实现方式。 两者均使用的B+ 树实现的索引。 2.1 InnoDB索引实现1）主键索引（聚簇索引，B+树）： 主键默认采用聚簇索引（聚簇索引是对磁盘上实际数据重新组织以按指定的一个或多个列的值排序的算法。特点是存储数据的顺序和索引顺序一致），且一张表只允许存在一个聚簇索引。 ​ ==聚簇索引的叶子节点就是数据节点（存放完整的行数据）== ​ ==非聚簇索引的叶子节点保存非完整行的数据（只有对应的键值数据） + 书签（主键的聚集索引的索引值）== ​ 叶子节点是数据页(默认16K)。==每个数据页上存放的是完整的行记录。B+ 树将 叶子页加载到内存后，再去查找对应数据行。== ​ 非叶子节点、存放的仅仅是键值及指向数据页的偏移量 聚集索引的好处： 对于主键的排序查找非常的快（因为其叶子节点是用双向链表链接的） 对于主键的范围查找非常的快（因为通过叶子节点的上层中间节点，就可以得到叶结点的范围值） 在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，**这棵树的叶节点data域保存了完整的数据记录。索引的key是数据表的主键（primary key），因此InnoDB表数据文件本身就是主索引。 InnoDB存储引擎的最小存储单元是页(16K)，页可以用于存放数据也可以用于存放键值+指针, 在InnoDB中B+树高度一般为1-3层，它就能满足千万级的数据存储。在查找数据时一次页的查找代表一次IO，所以通过主键索引查询通常只需要1-3次IO操作即可查找到数据。 因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。 2）InnoDB的辅助索引（非聚簇索引，B+树/ B树）B+树： 非聚簇索引的叶子节点保存非完整行的数据（只有对应的键值数据） + 书签（主键的聚集索引的索引值） 二级索引使用非聚簇索引。 通过二级索引查询首先，查到是主键值和对应的数据字段，如果查询的字段更多，InnoDB再根据查到的主键值通过主键索引找到相应的数据块。（ 这个流程叫 回表） B树：覆盖索引用到的。 InnoDB 表是基于聚簇索引建立的。因此InnoDB 的索引能提供一种非常快速的主键查找性能。它的辅助索引（Secondary Index， 也就是非主键索引）也会包含主键列，所以，如果主键定义的比较大，其他索引也将很大。如果想在表上定义 、很多索引，则争取尽量把主键定义得小一些。InnoDB 不会压缩索引。 文字符的ASCII码作为比较准则。聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。 1、为什么不建议使用过长的字段作为主键？ 因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。 2、用非单调的字段作为主键在InnoDB中不是个好主意？ 因为InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效。 2.2 MyISAM索引实现：MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。 1）主键索引（非聚簇索引，B+树）：MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址。下图是MyISAM主键索引的原理图： 2）辅助索引（Secondary key）在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求 key是唯一，而辅助索引的key可以重复。 同样也是一颗B+Tree，data域保存数据记录的地址。因此，MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。 MyISAM的索引方式也叫做“非聚簇”的，之所以这么称呼是为了与InnoDB的聚簇索引区分。 三、为什么说B+树比B树更适合数据库索引？1、 B+树的磁盘读写代价更低：B+树的内部节点并没有指向关键字具体信息的指针，因此其内部节点相对B树更小，如果把所有同一内部节点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多，一次性读入内存的需要查找的关键字也就越多，相对IO读写次数就降低了。（B 树 非叶子节点中还包含了数据指针，所以占空间大） 2、B+树的查询效率更加稳定（数据都存在叶子节点，路径都是从根到叶子的路径，一般3次以下的IO读取，等于树的高度） B树不管叶子节点还是非叶子节点，都会保存数据，这样导致在非叶子节点中能保存的指针数量变少），指针少的情况下要保存大量数据，只能增加树的高度，导致IO操作变多，查询性能变低； 3、由于B+树的数据都存储在叶子结点中，非叶子点均为索引，**方便全盘扫描和范围查找。** 但是B树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+树更加适合在区间查询的情况，所以通常B+树用于数据库索引。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>数据库索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql-索引篇1-索引的概念]]></title>
    <url>%2F2019%2F08%2F11%2Fmysql-%E7%B4%A2%E5%BC%95%E7%AF%871-%E7%B4%A2%E5%BC%95%E7%9A%84%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[一、索引的概念1.1 索引的作用1.2 索引的分类（还不清楚到底怎么归类）**查看有哪些索引： SHOW index; 聚簇索引（主键索引) 每张表只能有一个，数据和索引在同一个文件 ​ 按照每张表的主键构造一颗B+树，同时叶子节点中存放的即为整张表的记录数据。 辅助索引（二级索引）: 叶子节点并不包含行记录的全部数据 ​ 非主键索引，叶子节点=键值+书签（行的索引值） 覆盖索引： （extra 提示using index） ​ InnoDB存储引擎支持覆盖索引，即从辅助索引中就可以得到查询的记录，而不需要查询聚集索引中的记录了（不需要回表操作）。 ​ 覆盖索引并不适用于任意的索引类型，索引必须存储列的值，所以不需要回表操作。 MySQL只能使用B-树. 联合索引： ​ 联合索引也是一棵B+树，其键值数量大于等于2。键值都是排序的，通过叶子节点可以逻辑上顺序的读出所有数据。 单值索引： 一个索引只包含单个列 多值索引、复合索引（组合索引）: 即一个索包含多个列 复合索引只会对与创建索引时的排序顺序完全相同或相反的 order by语句进行优化 唯一索引: 索引唯一，但可以null， 声明unique关键字时,会为其字段自动添加唯一索引 123456789101112// 单值索引 #外部创建 CREATE INDEX [indexname]ON t1(colname); #创建表的时候创建 CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, INDEX [indexName] (username(length)) ); #alter语句添加 ALTER table tableName ADD INDEX indexName(columnName) // 复合索引 CREATE INDEX idx_c1_c2_c3ON tablename(c1,c2,c3) 1.3 创建 1.4 什么时候该创建索引？ 不该创建？ 对于最长使用的查询，可以针对性的建立索引来优化速度。 join查询在有索引条件下 驱动表有索引不会使用到索引 被驱动表建立索引会使用到索引 二、性能分析和优化策略]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>数据库索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas-6重复值处理]]></title>
    <url>%2F2019%2F08%2F09%2Fpandas-%E9%87%8D%E5%A4%8D%E5%80%BC%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[pandas -6 重复值处理 如果你想找到或者删除 DataFrame中重复的行, 可以使用 duplicated 和 drop_duplicates 查找重复值1234567891011121314151617181920212223242526272829example: col1 col2 c 0 one x -1.067137 1 one y 0.309500 2 two x -0.211056 3 two y -1.842023 4 two x -0.390820 5 three x -1.964475 6 four x 1.298329In: // 单列 df.duplicated(&quot;col1&quot;, keep=&quot;first&quot;) // 多列 // df.duplicated([&quot;col1&quot;, &quot;col2&quot;], keep=&quot;first&quot;) Out: 0 False 1 True 2 False 3 True 4 True 5 False 6 False dtype: bool // 默认 keep = &quot;first&quot;,第一次出现的不算重复，返回False // keep = &quot;last&quot;, 最后出现的不算重复 // keep = False, 重复值均返回 True 删除重复值123456789In: df.drop_duplicates(&apos;col1&apos;) Out: col1 col2 c 0 one x -1.067137 2 two x -0.211056 5 three x -1.964475 6 four x 1.298329]]></content>
      <categories>
        <category>pandas系列教程</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas-5缺失值处理]]></title>
    <url>%2F2019%2F08%2F09%2Fpandas-%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[pandas -5 缺失值处理 统计数据中存在缺失值是十分常见的问题, 而对于缺失值的处理，是数据挖掘的一个重要环节。pandas 有一系列的方法处理缺失值。 缺失值的类型判断方法只记住万能的 pd.isnull 即可。 数值 pd.isna pd.isnull np.isnan 字符串 pd.isna pd.isnull 时间 pd.isna pd.isnull np.isnat 缺失值的统计 df.isnull().sum() 丢掉缺失值 // 某列有缺失值, 删除 df[ pd.isnull(df[&quot;columns&quot;])] // Series df.columns.dropna() // DataFrame // axis: axis=0 （默认）表示操作行，axis=1 表示操作列; // how : any 表示一行/列有任意元素为空时即丢弃，all 一行/列所有值都为空时才丢弃。 // subset: 参数表示删除时只考虑的索引或列名。 // thresh: 比如 thresh=3，会在一行/列中至少有 3 个非空值时将其保留。 df.dropna(axis=0, how=&quot;any&quot;, subset=[&quot;city&quot;, &quot;sex&quot;]) 填充缺失值 数据量少的情况下，直接丢掉不可取，可以适当补充数据。 // 前值填充 ffill 后值填充 bfill df.columns.fillna(method = &quot;ffill&quot;) // 实值填充 df.fillna(0) // 均值填充 df[&quot;columns&quot;].fillna(df[&quot;columns&quot;].mean(), inplace=True) // 众数 mode()[0] // 中位数 median()]]></content>
      <categories>
        <category>pandas系列教程</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas-7时间&日期处理]]></title>
    <url>%2F2019%2F08%2F06%2Fpandas-%E6%97%B6%E9%97%B4-%E6%97%A5%E6%9C%9F%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[pandas -7 时间&amp;日期处理pandas 常出现的时间格式 字符串类型 object 一般是字符串类型，pandas 储存string时 使用 narray， 每一个object 是一个指针 datetime 类型 datetime64 timedelta 类型 表示时间差的数据类型 类型转换 object 2 datetime 123456789// 方法1df[1] = pd.to_datetime(df[1], format=&apos;%d.%m.%Y&apos;)# 指定format，速度会加快很多。// 方法2dateStr = &quot;2019-02-03&quot;myDate = datetime.strptime(dateStr, &quot;&quot;%Y-%m-%d&quot;&quot;) datetime 2 object 1234df[&quot;time_list&quot;] = df[&quot;time_list&quot;].strftime(&quot;%Y-%m-%d&quot;)// Y 2019// y 19 datetime 相关操作1234567891011// 查看列元素的年，月，日，星期（整数型）df[&quot;time&quot;].dt.yeardf[&quot;time&quot;].dt.monthdf[&quot;time&quot;].dt.daydf[&quot;time&quot;].dt.weekday # 星期一是0// 一年中的第几天,第几周df[&quot;time&quot;].dt.dayofyeardf[&quot;time&quot;].dt.weekofyear// 查看列元素 某年的数据数量df[df[&quot;time&quot;].dt.year == 2019].shape 时间运算 计算时间差 12345// 计算时间差， 结果为timedeltadf[&quot;时间差&quot;] = df[&quot;时间1&quot;] - df[&quot;时间2&quot;]// 转换成 天数差df[&quot;时间差&quot;].days 计算未来日期 123// N天后的日期// 天 days, 时 hours， 周 weeksdf[&quot;时间&quot;] = df[&quot;时间1&quot;] - timedelta(days=10)]]></content>
      <categories>
        <category>pandas系列教程</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习模型的偏差与方差]]></title>
    <url>%2F2019%2F08%2F01%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE%2F</url>
    <content type="text"><![CDATA[机器学习的方差与偏差 方差与偏差，总是迷迷糊糊的，每次看了就会，过了就忘。今天看到一个非常棒的解释，迫不及待马上整理下来了。 机器学习的目标函数机器学习模型学习的是数据集的条件概率分布，得到一个决策函数。整个学习过程围绕着最小化（或者最大化）目标函数进行优化参数，目标函数的通常形式的定义是： Obj = L(θ) + λΩ(θ) L(θ) 是损失函数，衡量模型对训练集拟合程度的好坏(对应偏差)； Ω(θ) 是正则项，是衡量模型的复杂程度（对应方差）； 目标函数定义为损失函数和正则项两部分，是为了平衡模型的偏差和方差（Bias Variance Trade-off）。 偏差与学习器 偏差描述了 学习器的拟合能力 （对训练集的）。 学习器在训练集表现越好，损失越低，则模型的偏差越小。 方差与学习器 方差描述了 学习器的泛化能力(对测试集)。 学习器在测试集表现越好，则模型的方差越低。 偏差与方差之间的关系 我们最想要的是低偏差，低方差的模型，然而现实很难达到两者都极致的低。有时候需要用提高偏差来降低方差，毕竟模型的泛化能力十分重要。 当损失函数达到极小值，模型对训练集的拟合达到了最佳效果，==对应着低偏差==，而这种情况下，往往对训练集的特点学的过于细微，而缺乏泛化能力。 根据 ==奥坎姆剃刀原则==, 同样准确率，模型越简单越好，所以通过正则化等方式，削弱模型的拟合能力，降低过拟合的风险。通过提高偏差，来主动降低方差。 最终我们要平衡方差与偏差，从而得到一个合理的模型。 调整方差与偏差的方法待补充。。。]]></content>
      <categories>
        <category>机器学习方法</category>
      </categories>
      <tags>
        <tag>方差与偏差</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost]]></title>
    <url>%2F2019%2F07%2F30%2FXGBoost%2F</url>
    <content type="text"><![CDATA[xgboost 学习笔记 主要内容均来自官方文档，官方文档是英文版，所以简单的翻译了一下，方便日后查看。详细内容见官方手册 安装 XGBoost 12345ubuntu -python3: pip3 install xgboost 导入: import xgboost as xgb 数据接口XGBoost 可以从以下结构中加载数据： LibSVM text format file CSV Numpy 2D array Scipy 2D sparse array Pandas XGBoost binary buffer file. 加载的数据都放在 DMatrix对象中，下面是具体加载的过程演示： LibSVM text format file 12dtrain = xgb.DMatrix('train.svm.txt')dtest = xgb.DMatrix('test.svm.buffer') CSV 123// 需要指定标签所在的列dtrain = xgb.DMatrix('train.csv?format=csv&amp;label_column=0')dtest = xgb.DMatrix('test.csv?format=csv&amp;label_column=0') XGBoost 不支持种类特征，需要先加载为Numpy数组，然后进行 `one-hot` 编码;推荐使用pandas 加载数据. Numpy 1234data = np.random.rand(5, 10) # 5个样本，每个样本10个特征label = np.random.randint(2, size=5) # 二值标签dtrain = xgb.DMatrix(data, label=label) Scipy 12csr = scipy.sparse.csr_matrix((dat, (row, col)))dtrain = xgb.DMatrix(csr) Pandas 123data = pandas.DataFrame(np.arange(12).reshape((4,3)), columns=['a', 'b', 'c'])label = pandas.DataFrame(np.random.randint(2, size=4))dtrain = xgb.DMatrix(data, label=label) 保存为 XGBoost 二进制文件 12dtrain = xgb.DMatrix('train.svm.txt')dtrain.save_binary('train.buffer') 缺失值处理 1dtrain = xgb.DMatrix(data, label=label, missing=-999.0) 样本权重 12w = np.random.rand(5, 1)dtrain = xgb.DMatrix(data, label=label, missing=-999.0, weight=w) 参数设置 XGBoost 可以通过列表或者字典来设置参数，例如： Booster 参数 123param = &#123;'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'&#125;param['nthread'] = 4param['eval_metric'] = 'auc' 指定多个评估指标 1param['eval_metric'] = ['auc', 'ams@0'] 指定验证集来监视性能 1evallist = [(dtest, 'eval'), (dtrain, 'train')] 训练 模型训练 12num_round = 10bst = xgb.train(param, dtrain, num_round, evallist) 模型保存 1bst.save_model('0001.model') 保存模型和特征 1234# dump modelbst.dump_model('dump.raw.txt')# dump model with feature mapbst.dump_model('dump.raw.txt', 'featmap.txt') 模型加载 12bst = xgb.Booster(&#123;&apos;nthread&apos;: 4&#125;) # init modelbst.load_model(&apos;model.bin&apos;) # load data 早停 如果你有验证集，则可以使用早停机制来寻找最佳的 num_round, 需要将 验证集传入 evals,如果传入多个，则使用最后一个。1train(..., evals=evals, early_stopping_rounds=10) 如果模型在 early_stopping_rounds次，监控的参数 param[&#39;eval_metric&#39;] 都没有提升，则会停止训练，train 返回的是最后一次训练的模型，而不是最佳模型，最佳模型可以通过一下方式找到： bst.best_score bst.best_iteration bst.best_ntree_limit # 使用最佳模型 同样的，监控多个参数时，最后一个参数起早停的作用。 预测已经训练好的模型，或者已经加载的模型可以拿来预测新数据：123data = np.random.rand(7, 10)dtest = xgb.DMatrix(data)ypred = bst.predict(dtest) 使用最佳的迭代次数的模型：1ypred = bst.predict(dtest, ntree_limit=bst.best_ntree_limit) 绘制你可以使用绘图模块来画出树结构： 绘制参数重要性 1xgb.plot_importance(bst) 绘制目标树 1xgb.plot_tree(bst, num_trees=2) Ipython 中绘制树 1xgb.to_graphviz(bst, num_trees=2)]]></content>
      <categories>
        <category>XGBoost</category>
      </categories>
      <tags>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas-2索引和选择数据]]></title>
    <url>%2F2019%2F07%2F25%2Fpandas-2%E7%B4%A2%E5%BC%95%E5%92%8C%E9%80%89%E6%8B%A9%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[pandas -2 索引和选择数据 对于一种数据结构,最基本的操作就应该是增删改查了。 1. 行列选择行选择和列选择有许多方法，很容易记混，常用的要记住。主要方法有三种： iloc, loc, [] 行选择 切片 12345// 切片df[a:b]// 隔1行选择df[::2] 指定位置 df.iloc[1, 1] df.iloc[1:10, 2:3] df.iloc[1:10][&#39;Price&#39;] 指定索引 df.loc[&quot;index1&quot;, &quot;index2&quot;] 按照条件查找 df[( df[&quot;row2&quot;] == 1) &amp; (df[&quot;row2&quot;] == &quot;null&quot;)] df.loc[( df[&quot;row2&quot;] == 1) &amp; (df[&quot;row2&quot;] == &quot;null&quot;)] 根据列的多个值,选择行 List = [1,2,3,4,5]train[train[&quot;customer_id&quot;].isin(List)] 列选择 通过列标签选择单列 df[&quot;price&quot;] 通过列标签选择多列 df[[&quot;price&quot;, &quot;time&quot;]] 通过列索引,选择前3列 df.iloc[:, :3] 行列选择 df.loc[&quot;index1&quot; : &quot;index2&quot;, [&quot;price&quot;]] df.iloc[a:b][&#39;Price&#39;] 随机采样行或者列1234567s.sample(frac=0.5)// 参数// 默认选择行，n = 行数， frac = 比例// replace: 默认False 无放回采样// weights: 样本采样权重// axis: 默认=0 行, =1 列// random_state=2 分类别等数量抽样1234567891011121314# 降采样 -- 分类抽样def subSample(df_x, splitAttribute = &quot;Attribute4&quot;): subsampleNum = min(df_x.groupby(splitAttribute).size()) print(subsampleNum) df_x_sub = df_x.iloc[1:2,:] #df_y_sub = df_y.iloc[1:2,:] for label in df_x[splitAttribute].unique(): tmp_x = df_x[df_x[splitAttribute] == label] random_list = random.sample(range(0,len(tmp_x)),subsampleNum ) df_x_sub = df_x_sub.append(tmp_x.iloc[random_list,:]) df_y_sub.append(tmp_y.iloc[random_list,:]) return df_x_sub #, df_y_sub 2. 行的增删改查增加 单列 12345// 末尾增加 df[&quot;new col&quot;] = None // 指定位置增加，在2列后 df.insert(2,&apos;city&apos;) 多列 pd.concat([df, pd.DataFrame(columns=[&quot;C&quot;,&quot;D&quot;])]) 单行（待验证） 12345// loc 添加 df.loc[‘5‘] = [3, 3, 3, 3] // set_value 添加 df.set_value(‘5‘, df.columns, [3,3,3,3], takeable=False) 多行 多行相当于合并两张表了,可以参考(merge,concat)方法。 test_ = pd.merge(tmp, data.loc[:,[&quot;customer_id&quot;, &quot;label&quot;]],on=[&#39;customer_id&#39;],how=&#39;left&#39;,copy=False&quot;) 删除 列 12345678910// del 方法 def df[&quot;col_name&quot;]//根据列名 drop 方法 df.drop([&quot;b&quot;, &quot;c&quot;], axis=1,inplace = True)axis = 1 列axis = 0 行// 根据列号 drop 方法 df.drop(df.columns[[1,2]], axis=1, inplace=True) 行 12345// 根据索引 删除行 df = df.drop([1, 2])// 根据value 删除行 df = df[~df[&quot;col&quot;].isin(5,9) 按照条件删除行 1df.drop(df[df[&quot;order_pay_time&quot;] &lt; pd.to_datetime(&quot;2013-11-12 23:59:44&quot;) ].index) 修改与查找 单值修改和查找时, 参考选择行列方法。 多值查找时， 按条件查找 df_train[( df_train[&quot;row&quot;] == 1) &amp;( == &quot;null&quot;)] query 查找 df.query(&#39;(a &lt; b) &amp; (b &lt; c)&#39;) 替换 单个替换，inplace = True 覆盖源文件 df.replace(to_replace, value, inplace = True) 多值替换—-字典 df.replace({&quot;A&quot;:&quot;B&quot;, 29:100}) 按条件替换 df.where(df &gt; 0, -df, inplace=True) 交换两列的位置1df[[&apos;B&apos;, &apos;A&apos;]] = df[[&apos;A&apos;, &apos;B&apos;]]]]></content>
      <categories>
        <category>pandas系列教程</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas -1数据结构]]></title>
    <url>%2F2019%2F07%2F22%2Fpandas-1%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[pandas -1 数据结构 pandas 基本操作都很简单，只是在刚开始学习的过程中，容易忘掉一些API，导致完成一些操作时，总会想着翻翻手册，这一系列博客，是对这些方法进行了梳理，可作为入门学习的参考材料。平时经常翻阅。 “index” (axis=0, default), “columns” (axis=1) 1. Series Series 是一个带有 名称 和索引的一维数组。 创建seriex1234567891011121314151617181920212223242526272829303132333435// Series 数组生成，指定数据类型In: user_age = pd.Series(data=[18, 30, 25, 40], dtype=float) Out: 0 18 1 30 2 25 3 40 dtype: int64// 增加索引 indexIn: user_age.index = [&quot;Tom&quot;, &quot;Bob&quot;, &quot;Mary&quot;, &quot;James&quot;] Out: Tom 18 Bob 30 Mary 25 James 40 dtype: int64 // 表头In: user_age.index.name(&quot;name&quot;) Out: name Tom 18 Bob 30 Mary 25 James 40 dtype: int64 像字典一样使用series1234567891011121314151617// index 当键值In: user_age[&quot;Tom&quot;] user_age.get(&quot;Tom&quot;)// 切片-列In: user_age[2:3] // 按条件查找In: user_age[user_age &gt; 30] Out: name James 40.0 Name: user_age_info, dtype: float64 像向量一样使用series 可以传递给np方法 1234567891011// 整列加减In: user_age + 1 Out: name Tom 19.0 Bob 31.0 Mary 26.0 James 41.0 Name: user_age_info, dtype: float64 2. DataFrame DataFrame 是一个带有 名称 和索引的二维数组，像一张Excel表格。 创建DataFrame1234567891011121314151617181920212223242526// DataFrame 根据字典生成In: index = pd.Index(data=[&quot;Tom&quot;, &quot;Bob&quot;, &quot;Mary&quot;, &quot;James&quot;], name=&quot;name&quot;) data = &#123; &quot;age&quot;: [18, 30, 40], &quot;city&quot;: [&quot;BeiJing&quot;, &quot;ShangHai&quot;, &quot;HangZhou&quot;] &#125; user_info = pd.DataFrame(data=data, index=index) user_infoOut: // DataFrame 根据二维列表生成In: data = [[18, &quot;BeiJing&quot;], [30, &quot;ShangHai&quot;], [25, &quot;GuangZhou&quot;], [40, &quot;ShenZhen&quot;]] columns = [&quot;age&quot;, &quot;city&quot;] user_info = pd.DataFrame(data=data, index=index, columns=columns) user_info]]></content>
      <categories>
        <category>pandas系列教程</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[评价指标 ROC与AUC]]></title>
    <url>%2F2019%2F07%2F20%2F%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-ROC%E4%B8%8EAUC%2F</url>
    <content type="text"><![CDATA[非均衡分类问题 非均衡分类问题指的是每个类别的错误代价不同。 比如疾病检测中,有病患者诊断健康的代价，要比健康人诊断成有病（可能性）造成的影响更为严重。 对于常用的预测模型，通常是有预测的概率值，我们找到一个合适的截断点作为正负类别的界限。显然再不同的任务下，截断点选择是不同的。我们使用Precison 和Recall的新度量指标来针对特定任务下选择合适的截断值。 真实标签 预测为正 预测为反 正例 TP FN 反例 FP TN Precison(查准率)： 1P = TP/(TP+FP) Recall(召回率)： 1R = TP/(TP+FN) 当正负样本不不均衡,人为修改测试集中的正负比例时, P-R曲线波动很大，但是ROC曲线变化很小。 ROC 曲线 可以研究学习器的泛化性能。 加图 横坐标：真阳率，正例被正确预测的概率 1FPR = FP/(TN+FP) 纵坐标：假阳率，负例被预测错误的概率1TPR = TP/(TP+FN) ==理解四点一线==： (0, 0): FP = TP = 0, 所有样本预测为负 (1, 1): FP = TP = 1, 所有样本预测为正 (1, 0): FP = 1, TP = 0, 所有正样本预测为负 (0, 1): FP = 0, TP = 1, 完美预测 对角线：随机猜测的值。 AUC值AUC(Area under Curve) 被定义为ROC曲线的下侧面积。一般在(0.5~1)之间。 计算方法 几何角度 直接计算曲线下的面积，梯形 概率角度 任取一对正负样本对，正样本score大于负样本score的概率 python 实现链接 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npfrom sklearn.metrics import roc_curvefrom sklearn.metrics import aucfrom time import time# y: 标签# pred： 预测值def myAUC(y, pred): auc = 0.0 p_list = [] # 正负例的索引 n_list = [] for i, y_ in enumerate(y): if y_ == 1: p_list.append(i) else: n_list.append(i) # 构成p-n对 p_n = [(i,j) for i in p_list for j in n_list] pn_len = len(p_n) for tup in p_n: if pred[tup[0]] &gt; pred[tup[1]]: auc += 1 elif pred[tup[0]] == pred[tup[1]]: auc += 0.5 auc = auc/pn_len return auc## 产生一组数据y = np.array([1,0,0,0,1,0,1,0,])pred = np.array([0.9, 0.8, 0.3, 0.1,0.4,0.9,0.66,0.7])## sklearn 结果fpr, tpr, thresholds = roc_curve(y, pred, pos_label=1)tim = time()print(&quot;sklearn AUC:&quot;,auc(fpr, tpr))print(&quot;sklearn AUC time:&quot;, time()-tim)## myAUC 结果tim = time()print(&quot;\nmyAUC:&quot;,myAUC(y,pred))print(&quot;myAUC time:&quot;, time()-tim)]]></content>
      <categories>
        <category>机器学习方法</category>
      </categories>
      <tags>
        <tag>ROC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成学习提升算法-Adaboost]]></title>
    <url>%2F2019%2F06%2F25%2F%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-Adaboost%2F</url>
    <content type="text"><![CDATA[Adaboost 算法原理及推导 Adaboost 是Boosting算法的代表。Boosting可将许多弱学习器组合达到强学习器的效果。 Adaboost 是通过提升错分数据的权重值来改善模型的不足。其主要的流程是： 先训练一个基学习器； 根据基学习器的表现，改变样本的分布，使得错误分类的样本得到更多的关注； 改变分布后的样本再训练新的基学习器，如此迭代； 加权组合这些基学习器。 一、Adaboost算法原理 “Adaptive Boosting”（自适应增强） Adaboost算法中，每个样本有对应的权重D,每个基分类器也有对应的权重α，然后是下边的三步骤： Step1：初始化训练集的权重； 如果有N个样本，则每一个训练样本最开始时都被赋予相同的权重：1/N。 迭代：Step2： 改变样本分布，训练基学习器； 错分的样本权重D会增加；准确率高的分类器的权重α会更大； Step3: 加权组合弱学习器。 二、Adaboost算法推导给定训练集 T=\{(x 1, y 1),(x 2, y 2) \ldots(\mathrm{xN}, y \mathrm{N})\}其中， y_{i} \in\{-1,1\}步骤1：初始化训练集的权重D。每个训练样本的初始权重w相同，均为1/N, D_{1}=\left(w_{11}, w_{12} \cdots w_{1 i} \cdots, w_{1 N}\right) w_{1 i}=\frac{1}{N}, i=1,2, \cdots, N步骤2：训练基学习器，改变训练样本分布，迭代训练新的学习器。用m=1,2…M 代表迭代的轮数，每轮产生的学习器为 h_{m}(x) 计算学习器 h_{m}(x) 在训练数据集上的分类错误率 E_{t} (误差的权值和): E_{t}=P\left(G_{m}(x) \neq y_{i}\right)=\sum_{i=1}^{N} w_{m i} I\left(G_{m}\left(x_{i}\right) \neq y_{i}\right) 计算学习器 h_{m}(x) 的权重α： \alpha_{m}=\frac{1}{2} \ln \frac{\left(1-E_{m}\right)}{E_{m}} 更新训练集样本权重。 D_{m+1}=\left(w_{m+1,1}, w_{m+1,2} \cdots w_{m+1, i} \cdots, w_{m+1, N}\right)w_{m+1, i}=\frac{w_{m i}}{Z_{m}} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right), i=1,2, \cdots, N这里的 Z_{m} 时规范化因子: Z_{m}=\sum_{i=1}^{N} w_{m i} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right) 迭代训练学习器 步骤3：加权组合弱学习器。 f(x)=\sum_{m=1}^{M} \alpha_{m} h_{m}(x)H(x)=\operatorname{sign}(f(x))=\operatorname{sign}\left(\sum_{m=1}^{M} \alpha_{m} h_{m}(x)\right)三、Adaboost算法解释 模型为加法模型(基函数的线性组合),损失函数为指数函数,学习算法是前想分布算法的二类分类学习法.]]></content>
      <categories>
        <category>机器学习方法</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归]]></title>
    <url>%2F2019%2F04%2F27%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[逻辑回归就是西瓜书里的对数几率回归，名为回归，实际则是分类算法。其实质是利用线性回归模型的预测结果来逼近真实标记的对数几率。 一句话概括就是: 逻辑回归假设数据服从 **伯努利分布** ,通过 **极大化似然函数** 的方法，运用 **梯度下降** 来求解参数，来达到将数据二分类的目的。 线性模型如何处理二分类问题？ 在处理二分类任务时，我们希望模型能预测样本属于某类别的概率[0, 1]，或者直接输出类别的标签{0, 1}。 线性模型不能直接来完成这项任务，因为其输出是一个实际值，范围也不一定在0~1之间，无法体现我们所想要的概率或者标签。 y=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b因此，我们需要找到一种能把线性模型输出映射到 概率 或者 标签 的方法； 如何转化为标签？ 单位阶跃函数 如何转化为概率？ sigmoid函数 (二分类): 将无穷范围的值限制在(0, 1)之间 softmax函数（多分类）: 所有类别概率和为1 由于单位阶跃函数存在跳跃点，在跳跃点不可导，想选择梯度下降法来优化时，只能选择sigmoid函数了。 逻辑回归模型 逻辑回归就是这样的一个过程：面对一个分类问题，建立代价函数，然后通过优化方法迭代求解出最优的模型参数，然后测试验证我们这个求解的模型的好坏。 逻辑回归的优缺点 优点 速度快，适合二分类问题 简单易于理解，直接看到各个特征的权重 能容易地更新模型吸收新的数据 缺点 对数据和场景的适应能力有局限性，不如决策树算法适应性那么强 逻辑回归的用途 寻找主要影响因素： 通过学习到的权重值，得到不同因素对结果的影响力大小 预测： 预测事件发生的概率 建模常规步骤 寻找 h 函数（预测函数） 构造 J 函数 (损失函数) 利用梯度下降等方法最小化 J 函数，并求取参数 LR基本模型以下就是逻辑回归的基本模型： y=\frac{1}{1+e^{-z}} z=w^{\top} x+b取倒数 \frac{1}{y}=1+e^{-z}取对数 \ln \left(\frac{1}{y}-1\right)=-z \ln \frac{y}{1-y}=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+bln(y/(1-y)) 就是对数几率 代价函数线性模型常用的目标函数 均方误差 用在逻辑回归模型是非凸函数，非凸函数通过梯度下降法容易陷入局部最小值，因此需要想办法找到代价函数，且代价函数为凸函数。 极大似然法定义代价函数这里通过极大似然估计的方法来定义目标函数： 极大似然估计： 这里想了很久才算有点理解，之前就一直卡在这里。。极大似然估计就是可以利用已知数据来推测出产生这些数据的最可能的环境条件。 举个栗子，当我们扔硬币时，出现的可能性有两个，要么正面朝上（事件A），要么反面朝上(事件B)，假设出现某事件的可能性只与硬币的 质量分布θ 有关系，假设我们做了n组实验（A出现m次），这样能得到两种情况的概率 P(A), P(B),我们要推测出 质量分布θ 是多少，也就是 参数估计 ,即 质量分布θ 是多少时，才最可能出现当前实验的结果。抛硬币的事件服从二项分布，那么给定了一组实验的情况下，似然函数是(似然函数与概率值相等)： L\left(\theta ; x_{1}, \ldots, x_{n}\right)=f\left(x_{1}, \ldots, x_{n} ; \theta\right)=\prod_{i=1}^{n} P\left(X=x_{i}\right)=\theta^{m}(1-\theta)^{n-m}假设一个数据集Cn,标签y∈{0,1}，预测值Θ,模型参数为w则似然函数可以写作: P\left( \theta _{\left( x_i \right)}|w \right) =\prod_{n=1}^N{\theta}_{xi}^{yi}\cdot \left( 1-\theta _{\left( x_i \right)} \right) ^{1-y_i} 取对数简化运算: \mathrm{L}(\mathrm{w}, \mathrm{b})=\sum_{i=1}^{N}\left[y_{i} \log \left(\emptyset\left(x_{i}\right)\right)+\left(1-y_{i}\right) \log \left(1-\emptyset\left(x_{i}\right)\right)\right]实际代价函数的样子: \mathrm{J}(\mathrm{w}, \mathrm{b})=-\frac{1}{N} \mathrm{L}(\mathrm{w}, \mathrm{b})=-\frac{1}{N} \sum_{i=1}^{N}\left[y_{i} \log \left(\emptyset\left(x_{i}\right)\right)+\left(1-y_{i}\right) \log \left(1-\emptyset\left(x_{i}\right)\right)\right]取对数之后的公式很符合理想的代价函数，当实际标签与预测结果相同，则代价为0，而相反时，会随着差值越大，损失越大。 因为 \operatorname{logit}(\mathrm{p})=\log \frac{p}{1-p} \log \frac{P(Y=1 | X)}{1-P(Y=1 | X)}=w \cdot x+b带入后继续化简 \mathrm{L}(\mathrm{w}, \mathrm{b})=\sum_{i=1}^{N}\left[y_{i}\left(\mathrm{w} \cdot x_{i}+\mathrm{b}\right)-\log \left(1+e^{\mathrm{w} \cdot x_{i}+b}\right)\right]直观解释直观上理解，代价函数就是分类错误的惩罚，那么当y=1时，prediction 越小时，损失应越大；y=0时prediction 越大时，损失应越小。 \operatorname{cost}=\left\{\begin{aligned}-\log (\hat{p}), & \text { if } y=1 \\-\log (1-\hat{p}), & \text { if } y=0 \end{aligned}\right.通过梯度下降来最小化代价函数因为我们找到的代价函数是凸函数，所以可以尝试用梯度下降来找到合适的参数w，b： \mathrm{w} :=\mathrm{w}-\alpha \frac{\partial J(w, b)}{\partial w} \mathrm{b} :=\mathrm{b}-\alpha \frac{\partial J(w, b)}{\partial b}偏导求解过程： 对w求偏导 \frac{\partial J(w, b)}{\partial w}=-\frac{1}{N} \sum_{i=1}^{N}\left[y_{i} \frac{1}{\partial\left(x_{i}\right)}-\left(1-y_{i}\right) \frac{1}{1-\emptyset\left(x_{i}\right)}\right] \frac{\partial \emptyset\left(x_{i}\right)}{\partial w} =-\frac{1}{N} \sum_{i=1}^{N}\left[y_{i} \frac{1}{\emptyset\left(x_{i}\right)}-\left(1-y_{i}\right) \frac{1}{1-\emptyset\left(x_{i}\right)}\right] \sigma\left(x_{i}\right)\left(1-\emptyset\left(x_{i}\right)\right) \cdot x_{i} =-\frac{1}{N} \Sigma_{i=1}^{N}\left(y_{i}-\emptyset\left(x_{i}\right)\right) \cdot x_{i^{*}}对b求偏导 \frac{\partial J(w, b)}{\partial b}=-\frac{1}{N} \sum_{i=1}^{N}\left[y_{i} \frac{1}{\emptyset\left(x_{i}\right)}-\left(1-y_{i}\right) \frac{1}{1-\emptyset\left(x_{i}\right)}\right] \frac{\partial \emptyset\left(x_{i}\right)}{\partial b} =-\frac{1}{N} \sum_{i=1}^{N}\left[y_{i} \frac{1}{\emptyset\left(x_{i}\right)}-\left(1-y_{i}\right) \frac{1}{1-\emptyset\left(x_{i}\right)}\right] \wp\left(x_{i}\right)\left(1-\emptyset\left(x_{i}\right)\right) =-\frac{1}{N} \Sigma_{i=1}^{N}\left(y_{i}-\emptyset\left(x_{i}\right)\right)带入公式后，得到最终推导的结果： \mathrm{w} :=\mathrm{w}+\alpha \frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\emptyset\left(x_{i}\right)\right) \cdot x_{i} \mathrm{b} :=\mathrm{b}+\alpha \frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\emptyset\left(x_{i}\right)\right) 通过以上公式可以看出，逻辑回归的梯度与 `sigmoid` 本身无关，只与 `y` 和`x`和 `w` 有关系。 进一步提高泛化能力 影响模型泛化能力的主因素是 过拟合, 过拟合问题比较容易理解，这里就不贴图了。下面分析一下产生过拟合的原因和解决办法。 过拟合产生的原因有哪些？过多的特征 怎么解决过拟合？ 减少特征数量 减少特征数量会导致部分信息丢失。 正则化 保留所有的特征，并且减小参数的大小。 正则化方法在代价函数上增加一个惩罚项，惩罚项应该是模型复杂度的单调递增函数，模型越复杂，惩罚项越大。 回归问题中，取平方损失（L2 范数），或者L1范数 J(\theta)=\frac{1}{2 m} \sum_{i=1}^{n}\left(\mathrm{h}_{\theta}\left(\mathrm{x}_{i}\right)-y_{i}\right)^{2}+\lambda \sum_{j=1}^{n} \theta_{j}^{2}这里的lambda 系数： 如果它的值很大，说明对模型的复杂度惩罚大，对拟合数据的损失惩罚小，这样它就不会过分拟合数据，在训练数据上的偏差较大，在未知数据上的方差较小，但是可能出现欠拟合的现象； 如果它的值很小，说明比较注重对训练数据的拟合，在训练数据上的偏差会小，但是可能会导致过拟合。 加入正则化后的参数更新： \theta_{j} :=\theta_{j}-\frac{\alpha}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x_{i}\right)-y_{i}\right) x_{i}^{j}-\frac{\lambda}{m} \theta_{j}python 实现空 参考资料极大似然估计]]></content>
      <categories>
        <category>机器学习方法</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树1-基本概念]]></title>
    <url>%2F2019%2F04%2F20%2F%E5%86%B3%E7%AD%96%E6%A0%91-1%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[决策树1- 基本概念决策树 上图来自西瓜书，是决策树的一种树形。生成决策树的过程，不断的根据样本的属性( 样本的某个特征 )划分样本子集。每个结点选择当前最优的属性作为划分依据，将样本集合不断的划分成更小的子集合，直到子集合中样本类别一致时或者没有可以划分的属性值时，则停止划分，标记为叶结点(叶节点代表一个类别)。 简单的介绍一下决策树的组成元素: 根节点: 所有的训练样本 内部节点: 对应某一个划分属性 叶节点： 对应某一种决策结果 判定测试序列： 某个样本在节点中传递的路径 所有节点都包含着不同数量的样本。 以上是分类树的例子，决策树也可以用作回归任务，如CART算法。决策树是GBDT,Xgboost等更高级结构的基础，所以尽量要掌握决策树的原理。 决策树算法的基本流程假设有一个数据集，其中的每个样本有多种特征，每个特征有不同的取值。通过这个数据集来生成一个决策树的一般流程可以归纳为: 特征选择 特征选择就是决策树分叉时，依据新节点的”纯度”，选择最优的划分属性; 决策树生成 树不断的分叉，直到样本的属性用光，或者树的深度达到了预定值，则结束分叉; 剪枝 如果一直树杈分下去，一定能够使得所有的样本都正确的归类，但这样会产生对训练集的过拟合，泛化能力变差，可以通过剪枝操作来改善泛化能力。 通过这三步，就可以生成一颗决策树了。下面来学习一下具体怎么进行特征的选择和剪枝。 如何选择最优的划分属性(分类树)? 决策树不断分叉的原因，是尽可能的让不同类别的样本划分到不同的节点，同类别的样本划分到同一个节点。而选择最优的划分属性（特征）的过程，相当于是遍历计算出所有特征的结果，找到能使分叉后子集合最 “纯” 的特征，就是最优的划分属性了。所以，该如何定义 “纯” ，需要借助信息论中 “信息熵” 的概念了。 熵 : 表示随机变量不确定性的度量,也就是混乱程度的一种度量。 假定数据集 D 中第 K 类样本所占的比例为 p_{k},则信息熵定义为: \operatorname{Ent}(D)=-\sum_{k=1}^{N} p_{k} \log _{2} p_{k}数据集包含的类别越少时越纯，Ent(D)也越小。 法1: 信息增益==ID3算法用到信息增益== 直白的讲就是决策树分叉前的信息熵减去分叉后的信息熵。 信息增益最大的特征就是最佳划分属性。 假定分叉前样本集 D 中的特征 a 有 V个可能的取值 \left\{a^{1}, a^{2}, \ldots, a^{V}\right\} ,当选择 a 做划分属性时，会分V个节点，每个节点上的子样本集合为 D^{v},同时为不同节点赋权重(按照样本的比例)，于是信息增益为: \operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)减数部分也叫 条件熵 缺点: 分叉时偏好取值较多的属性。 原因分析: 取值多的特征，样本更分散，所有得到的新节点”纯度” 趋于更高，熵更低，而划分前的增益不变的情况下，该特征增益更大。 比如，当特征的可能取值数量正好等于样本数量，那条件熵几乎为0，该特征一定会被选择。 法2: 信息增益率==C4.5算法用到信息增率== 相当于在法1基础上，增加了惩罚系数，可取值越多，系数越大。 Gain\_ratio\left( D,a \right) =\frac{Gain\left( D,a \right)}{IV\left( a \right)}\mathrm{IV}(a)=-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \log _{2} \frac{\left|D^{v}\right|}{|D|}IV(a) 是属性 a 的 “固有值”，内部属性。 缺点: 分叉时偏好取值较少的属性。 法3：基尼指数==CART决策树算法用到基尼指数== 反应从节点样本集合中随机抽取两个样本，类别不一致的概率。CART决策树默认为二叉树。 基尼值的定义: \operatorname{Gini}(D)=\sum_{k=1}^{|y|} \sum_{k^{\prime}=k} p_{k} p_{k^{\prime}}=\sum_{k=1}^{|y|} p_{k}\left(1-P_{k}\right)=1-\sum_{k=1}^{|\mathcal{Y}|} p_{k}^{2}选择特征 A 的情况下，针对 A 所有可能取值 a, 分别计算基尼指数： Gini\_index\left( D,a \right) =\sum_{v=1}^V{\frac{\left| D^v \right|}{|D|}}\text{}Gini\left( D^v \right)选择基尼指数最小的特征和切分点，作为最优划分属性。 三种决策树模型： 算法 特征选择标准 ID3 信息增益 C4.5 信息增益率 CART 基尼指数 对抗过拟合 — 剪枝处理 分支太多，容易过拟合，泛化能力变差。所以要适当剪枝，常用方法是预剪枝和后剪枝 剪枝操作包括的点也很多，这里只是简单描述一下，详细的参考未来的博客。www.elgong.top 预剪枝 预剪枝是在决策树生成的过程中，对每个结点在划分前先估计，根据划分前后验证集的精度，来决定是否划分； 只能估计当前结点可划分性，不能预测到未来节点划分的必要性，是贪心算法； 容易造成欠拟合。 后剪枝 先生成完整的树，再从叶结点往回计算，根据验证集精度是否提升决定是否剪枝； 泛化能力往往优于预剪枝，欠拟合风险小； 时间开销大。 属性为连续值时？ C4.5 算法采用二分法将连续值离散化 与离散属性不同，连续的属性可以在后代节点中再次使用 当数据中含有缺失值时？处理方法： 通过无缺失数据计算出三个参数： 无缺失样本占总样本比例 无缺失样中 K类别 占比 pk 无缺失样本中 v 属性样本占比 rv 对单样本增加一个权值 Wx, 无缺失样本的Wx = 1， 有缺失样本的Wx = rv*Wx。 在计算分支时，同一样本以不同的概率划分到不同的子节点中 当样本的属性已知：则把该样本划分进对应的子节点，权值=1； 当样本的该属性缺失：则把该样本同时划入所有的子节点，样本权值需要更新为`Wx = rv*Wx。 决策树的优缺点==优点==： 便于理解和可视化； 训练需要的数据少，不需要对数据进行规范化； 可同时处理数值型，类别型数据； 是白盒模型，可解释； ==缺点==： 容易产生过于复杂的模型 -&gt; 泛化能力差 （剪枝，限制叶节点所需要的最小样本数，最大深度） 决策树不稳定，微小变化会产生不同的树（集成多棵树可以缓解） 难学NP问题（启发式学习） 异或，奇偶，很难被学习到]]></content>
      <categories>
        <category>机器学习方法</category>
      </categories>
      <tags>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟机类加载机制]]></title>
    <url>%2F2019%2F04%2F14%2F%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[虚拟机类加载机制绑定 绑定指的是把一个方法的调用与方法所在的类(方法主体)关联起来。 静态绑定 前期绑定。在程序执行前方法已经被绑定，此时由编译器或其它连接程序实现java当中的方法只有final，static，private和构造方法 是前期绑定的。 动态绑定 运行时绑定。在运行时根据具体对象的类型进行绑定。在java中，几乎所有的方法都是后期绑定的。 类加载机制类从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期包括：加载、验证、准备、解析、初始化、使用和卸载 七个阶段，前五个阶段属于类加载的过程。这是开始顺序，每个阶段可能交错。 step1：加载加载阶段，虚拟机完成的任务： 通过一个类的全限定名来获取起定义的二进制字节流。 二进制字节流来源： Class文件，Jar包、从网络中获取（最典型的应用便是Applet）、由其他文件生成（JSP应用）等； 将该字节流的静态存储结构转换为方法区的运行时数据结构。 在Java堆中声称一个代表这个类的 java.lang.Class对象，作为对方法区中这些对象的入口。 三种主要的类加载器？ 类加载机制采用了委托模式。 类加载器与类本身一同确定这个类在Java 虚拟机中的唯一性。 启动类加载器 Bootstrap ClassLoader 该加载器由C++实现，不属于类，负责加载 /JDK/JRE/lib/rt.jar，主要加载 JVM 工作需要的类； 扩展类加载器 Extension ClassLoader Bootstrp loader 加载 ExtClassLoader, 该加载器由sun.misc.Launcher$ExtClassLoader 实现，它负责加载`/JDK\JRE\lib\ext目录中的类，自己的类打包jar放入也可以； 应用程序类加载器 Application ClassLoader Bootstrp loader加载完ExtClassLoader后，就会加载AppClassLoader,并且将AppClassLoader的父加载器指定为 ExtClassLoader， 负责加载classpath所指定的位置的类。 自定义类加载器 如果要自定义类加载器，需要继承 应用程序类加载器 三者如何协调工作？类加载机制采用了委托模式。启动类加载器加载其他类加载器，当需要加载类时，优先父类加载器工作。 双亲委派模型的工作流程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把请求委托给父加载器去完成，依次向上，因此，所有的类加载请求最终都应该被传递到顶层的启动类加载器中，只有当父加载器在它的搜索范围中没有找到所需的类时，即无法完成该加载，子加载器才会尝试自己去加载该类。 step2：验证验证的目的？保证class 文件的字节流符号JVM 虚拟机的要求，不危害虚拟机自身安全。虽然 Java 源码编译不会产生如数组越界之类的错误，但class 文件可以被编辑。 都需要哪些验证？ 文件格式验证 验证字节流是否符合 class文件规范（如开头是否为魔数0xCAFEBABE， 主次版本号是否可以被当前虚拟机处理等） 元数据验证 验证字节码描述的信息是否符合Java 语言规范（如类的继承实现是否符合语法规范） 字节码验证 该阶段验证的主要工作是进行数据流和控制流分析，对类的方法体进行校验分析，以保证被校验的类的方法在运行时不会做出危害虚拟机安全的行为。 符号引用验证 发生在虚拟机将符号引用转化为直接引用的时候（解析阶段中发生该转化，后面会有讲解），主要是对类自身以外的信息（常量池中的各种符号引用）进行匹配性的校验。 step3： 准备 准备阶段是正式为类变量（静态变量）分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。注意的是： 只为类变量分配内存； static 类变量初始值为默认初始值，而不是程序中的值； public static int value = 3； // 实际初始值为0 同时被final和static修饰，那么在准备阶段变量value就会被初始化为ConstValue属性所指定的值； public static final int value = 3； // 实际初始值为3 step4： 解析 解析阶段是虚拟机将常量池中的符号引用转化为直接引用的过程。 解析动作主要针对类或接口、字段、类方法、接口方法四类符号引用进行，分别对应于常量池中的CONSTANT_Class_info、CONSTANT_Fieldref_info、CONSTANT_Methodref_info、CONSTANT_InterfaceMethodref_info四种常量类型。 step5： 初始化 真正开始执行类中定义的Java程序代码,初始化阶段是执行类构造器()方法的过程。 类构造器 &lt;clinit&gt;() 执行规则： 按照在源文件中出现的顺序收集类变量 和 静态语句块 static{ }; 静态语句块中只能访问定义在之前的变量，而在块后定义的变量只能被赋值，但不能被访问； 优先构造父类； 父类中的 静态语句块 static{ } 优先于子类中的变量赋值操作； 不是必须的，当类或者接口中没有静态语句块或者没有变量赋值，则可以不生成&lt;clinit&gt;()； 双亲委派被破坏 3种情况下 第一次： JDK1.2之前还没有双亲委派，但是有用户自定义类加载器（通过loadClass（）加载），为了兼容老版本，设计者添加了 protected findClass()，直接调用用户loadClass（）方法； 第二次：原则上 越基础的类由越上层的加载器进行加载,但是有些情况下基础类需要调用用户的代码。如JNDI, JDBC，JCE,JAXB，JBI,这时候引入了线程上下文加载器； 第三次：“代码热替换”和“模块热部署”出现，希望程序中的功能模块像键盘鼠标一样直接更换，而不是重启。OSGi 是这个标准化模块，具体还没看。。。]]></content>
      <categories>
        <category>深入理解Java 虚拟机</category>
      </categories>
      <tags>
        <tag>Java类加载机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[华为软挑2019]]></title>
    <url>%2F2019%2F04%2F01%2F%E5%8D%8E%E4%B8%BA%E8%BD%AF%E6%8C%91%2F</url>
    <content type="text"><![CDATA[参加软挑的一些感悟写在前边的话 我本科一直在做嵌入式相关的项目,这是第一次参加软件类的竞赛,不得不说过程确实很刺激,最后止步杭厦赛区50强也很是遗憾,明明很接近,最后输在了代码效率上,本地成绩很好的 python代码 ,上传测评运行时间超限（官测环境比本地性能好，普遍情况是用时远超本地，其中华为云主机集体宕机半小时，很多人测过的最优代码，最后再上传就超时了https://bbs.huaweicloud.com/forum/thread-16237-1-1.html）。超限原因主要两点，一是自己在实现调度器时的代码臃肿，二是正式赛数据量大增。但进入32强的不少组并没有实现调度器，完全 随机时间发车+单车路径最优规划 , 感觉很(￣_,￣ )。这比赛不实现调度器，意义少了一半，再用上这种偷鸡方法，没觉出来还有啥意义。唉，菜鸡就是菜鸡。总之，这次经历对我还是有不少积极的影响的，下面就总结一下吧。 题目解读本次比赛主要做的是 动态路网下多车辆调度问题, 参赛者合理安排数万车辆在合理时间从出发点到达各自的目的地，程序上传至官方服务器，运行后得出 所有车辆出发时间和规划的路径 ，将在官方调度器中进行调度，完成车辆调度用时即为最终成绩。具体是比赛任务书中花了很大篇幅讲了官方调度器的规则，并且论坛前期几乎天天在更新规则补充，最终完全准确实现的队伍只见过一个，其他很对队伍是很接近，但总有差别。我们自己实现的调度器，调度时间完全对的上，但是所有车辆调度总时间总是差了一些。分析原因有以下两点: 我们实现的调度规则还有与官方一些差异；（但是我们实现的和部分队伍对比的结果完全一致，但和官网就有差异，猜测可能有些规则官方描述的有些差异，或者某细节被我们忽略了） python 即使版本相同，但是在不同机器上结果确实有差异，这个也被官方证实了https://bbs.huaweicloud.com/forum/thread-15889-1-1.html。 总体上参考任务书，下面只简单梳理一下思路（这里默认已经熟悉了任务书）： 下面先附上官方伪代码 for(/* 按时间片处理 */) { foreach(roads) { /* 调整所有道路上在道路上的车辆，让道路上车辆前进，只要不出路口且可以到达终止状态的车辆 * 分别标记出来等待的车辆（要出路口的车辆，或者因为要出路口的车辆阻挡而不能前进的车辆） * 和终止状态的车辆（在该车道内可以经过这一次调度可以行驶其最大可行驶距离的车辆）*/ driveAllCarJustOnRoadToEndState(allChannle);/* 对所有车道进行调整 */ /* driveAllCarJustOnRoadToEndState该处理内的算法与性能自行考虑 */ } while(/* all car in road run into end state */){ /* driveAllWaitCar() */ foreach(crosses){ foreach(roads){ while(/* wait car on the road */){ Direction dir = getDirection(); Car car = getCarFromRoad(road, dir); if (conflict）{ break; } channle = car.getChannel(); /* 这里只指因下一道路有等待车辆阻挡而导致该车辆无法进入的情况 */ /* 其他情况均返回true,比如下一车道满无法进入（全是终态），或才是下一车道限速不能进入，该车辆停留在其当前车道最前方 */ /* 该车辆也是移动至其所在车道最前方，只有有车辆由等待变以终止，就对其车道后续车辆状态进行调整 */ if(!car.moveToNextRoad()) { break; } /* driveAllCarJustOnRoadToEndState该处理内的算法与性能自行考虑 */ driveAllCarJustOnRoadToEndState(channel); } } } } /* 车库中的车辆上路行驶 */ driveCarInGarage(); } 要调度的车辆分两种：路上的车和要上路的车 每个时间片先处理路上车，在处理上路车 路上的车处理步骤： step1标记状态， step2 移动车辆 车的状态：每个时间片（一个时间片指的是所有车辆一次调度完成）路上车辆有三种状态，未调度过的车是 无状态， 调度过但是由于阻挡或者其他原因不能移动的车标记为 等待状态， 调度过并且完成移动的车标记为 终态。 step1: 怎样标记状态？ - 这个时间片车辆最大行驶速度能超过该道路长度（超过了但不一定就能进入下一条道路），直接标记为`等待状态` - 这个时间片车辆最大行驶速度不能超过该道路长度，但是前方有车辆挡住自己将要走的路，直接标记为`等待状态` - 这个时间片车辆最大行驶速度不能超过该道路长度，并且前方没车辆挡住自己，**移动该车**，直接标记为`终止状态` step2: 按什么顺序调度车辆？ 路上的车： 处理次序： 按照ID升序反复遍历路口，直到所有车辆变成终态 对每个路口，按照ID升序遍历朝向这个路口的道路（也反复遍历，直到所有车进入终止状态，或者被阻挡无法移动） 每个道路上有多条线，按照优先级顺序处理车辆，只有第一优先级车辆完成调度，才能调度优先级低的车。 不过马路而被标记等待的车，不受优先级限制，阻挡车辆离开，这种车立马跟上 上路的车： 上路车按照ID升序处理 规划的时间因为前方无空位而未上路的车，顺延到下一时刻优先上路，即不参与下一时刻车辆ID升序发车。 哪些车参与优先级的排序？ 要过马路的车和车速超过当前道路剩余长度，但是根据任务书10-5条，不能进入下一道路的车都参与优先级排序。 大概就这些了，其他更细微的只能遇到才想起来了。 思路总结 这里只总结一下初赛的思路。 这个比赛就是合理安排车辆调度，以最短时间让所有车都到达终点。所以要找到合适的方法让车辆快速充满道路而不至于 锁死,锁死也是赛题的最难点。 道路上流动的车越多，越容易出现锁死情况；道路上流动的车越少，最终调度时间就越长。所以优化的目标变成了保证不死锁的情况下，让更多的车在道路上流动起来。 什么是死锁？死锁指的是，某个时间片道路上的车辆由于循环等待（形成了环形等待情况），导致无法再进一步调度任何车辆，导致调度失败，成绩为0。体现在调度器里，就是step2 反复调度路口时，等待状态的车辆数量不再减少，即锁死了。 怎么避免死锁？唯一可以避免的方法是完全实现调度器，和官方调度器一致，就可以准确判断到锁死，并且在规划道路时动态规划新路，解开环形等待的死亡链。可惜大家完全模拟出来调度器的几乎没有（据我一直水群了解到的情况看，是这样，不排除潜水大佬真的实现了）。所以呢，大部分人都是想尽办法的尽量减少锁死，无法完全避免，下面会举例几种方法。 有不完全正确调度器的解决方案 单车最优路径静态规划 + 遇锁死时对部分车动态规划 如果调度器不太一致时，就当某道路调度同一车辆多次，就给这个车强制规划新路径。 单车最优路径静态规划 + 遇锁死时把锁死车辆从路上删除，未来重新发车 分批次发车 + 每个批次单独规划路径 + 动态路阻 + 锁死车辆动态规划 这个是效果比较好的一种方法，练习赛后期成绩能进入前15名的方法。动态路阻指的是道路情况拥堵，这里选择了几个因素： 动态路阻 = 这个批次经过该道路车辆数量a + abs(道路限速 - 车速)b + （1 - 道路中路线数/最大线数）*c 路阻每个批次都清除一次，这样在调度器不准确的情况下很大程度上抑制了死锁的发生，当时采用这种方法之后，每个批次发车量明显可以提高很多。这里a，b，c是需要调节的参数。 没有调度器的解决方案如果没实现调度器，也有一些不错的方法，但是不算偷鸡。这里把路网当成计算机的网络，网络的带宽就是道路的线数，我们想让网络传输最大量的数据，但是网络本身承载能力有限制，我们要找到均衡流量的方法，让网络上流动的流量尽可能的均衡，这样再找到合适的参数，即网络最适合的承载车辆数目，保证网络流量不超过这个限制，也可以减少死锁的情况。即 分批次发车 + 每个批次单独规划路径 + 动态路阻 还有一种不太有意义的方法这种也被很多人叫做偷鸡方法，就是 单车路径最优规划 + 随机时间发车，然后就是调调调。这种方法优势是答案生成快，可以反复调无数次。而实现调度器的同学，基本半小时才能调一次参数，因为模拟调度的过程比较费时间，又加上动态路径规划，时间代价大大提高。 对单车的寻路算法想当然的觉得地图是平面的，因为官方给的任务书全是平面图，并且每个路口对应的四个街道都是有方向的，所以对路口直接建立了坐标系，有了每个点的方位坐标信息，也就很自然的选择了A*算法。结果没想到，最后正式赛当天出现的地图是这。。。样。。。的。。。，出现了高空立交桥，这还算直行吗。开始不知道地图变了样子，结果递归建坐标系的部分爆了bug，改了半天，卒了。后来看到群里可视化后的效果是下图，吐血了，赶紧换了Dijkstra算法。这两个算法有时间再总结。 我们组的结果结果就是止步初赛了，调度器 + 动态规划 + 动态路阻 + 。。。+ python 真是很费时间，本地要15分钟勉强出结果，服务器上直接超时。 放弃了调度器和动态规划， 只用了动态路阻，最后所剩时间不多了，只调了几次参数就到时间了。 比赛的经验与教训 比赛运行环境一定要保证和官方一致，不然结果会出现不一致。 如果还是这种复杂规则的情况，不要再选择python，速度确实有问题，代码能力差的人体现的更明显~、~、 好好理解题目在行动。 感悟结果有点惨淡，但是这段时间确实收获了很多，也多亏了两位队友的倾力相助，以及师兄的思路指导。郭同学为团队提供了大部分算法上的思路和代码；丁同学从开始比赛到最后一天，也一直在和我讨论着调度以及算法，纠正了我很多错误的理解，比赛的日子也是近段时间来最开心的日子，期待大家下一次的合作。画江湖之绿皮车将要回归。。。]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>华为软挑初赛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Residual Learning for Image Recognition]]></title>
    <url>%2F2019%2F01%2F25%2FDeep-Residual-Learning-for-Image-Recognition%2F</url>
    <content type="text"><![CDATA[2015年 论文地址： https://arxiv.org/pdf/1512.03385.pdf 通常情况下 神经网络层越深，特征的抽象程度越高，模型的表达能力越强，语义信息越丰富； 同时，网络越深，退化问题 越难解决; 退化问题是网络加深的障碍 简单的增加深度，会导致 梯度弥散 或者 梯度爆炸 ,可以通过标准初始化和中间层正则化（batchNorm）解决；但是随着继续增加深度，训练集准确率饱和，甚至下降（退化问题，不属于过拟合）。 神经网络就像一个｀Function Set｀，网络是输入 x 到输出 out 的映射关系 F，out = F(x) ，所以网络层越深，这个 Set 包含的函数越复杂，那能够拟合的情况就更多（意思是解决的问题就更多），上图是作者在CIFAR-10 数据集上的对比实验，56层网络是在20层网络上的重复叠加（最差也应该是前22层的恒等映射），但是较深网络训练集的误差却比浅层网络的误差更大。这不是过拟合了，过深网络出现了 退化问题。 本文效果（很大程度上解决了退化问题）： 作者在ImageNet上实验了一个152层的残差网络，比VGG深8倍，取得了3.57%的错误率。 作者通过一系列实验证明了表示的深度（即网络的深度）对很多视觉识别任务都至关重要。仅仅由于使用了非常深的网络，作者就在COCO目标检测数据集上获得了28%的相对提升。 本文怎么解决退化问题？Resnet 结构分析ResNet 短连接块作者在《Identity Mapping in Deep Residual Networks 》一文中，推导了为什么短连接更容易训练，也提出了新的短连接块结构。 为什么 ResNet build block 更容易训练？前向传播中帮助网络中一些层更容易实现恒等映射： 出现了第一幅图的退化问题，这里假设深层网络的后边层都变成了恒等映射 ，那网络就退化成了浅层网络（因为网络很深，所以其中肯定包括了多余的层，这些层会形成恒等映射 关系），原来的直接多个层堆叠的非线性层去直接学习 恒等映射 优化起来复杂，而加了上图的 短连接块之后，学习 恒等映射 变容易了。 反向传播中 因为网络中存在恒等映射的短连接通道，假设不加residual模块的输出为h(x)。x=10,h(x)=11,h(x)简化为线性运算Wh​, Wh明显为1.1，加了redidual模块后，F(x)=1, H(x)=F(x)+x=11，F也简化为线性运算,对应的WF为0.1。当标签中的真实值为12，反向传播的损失为1，而对于F中的参数和h中参数回传的损失实际上是一样大的而且梯度都是x的值，但是对于F的参数就从0.1到1.1扩大了十倍多，而h的参数是从1.1到2.1扩大不到2倍，因此redidual模块会明显减小模块中参数的值从而让网络中的参数对反向传导的损失值有更敏感的响应能力，虽然根本上没有解决回传的损失小得问题，但是却让参数减小，相对而言增加了回传损失的效果，也产生了一定的正则化作用。https://blog.csdn.net/weixin_43624538/article/details/85049699?from=timeline&amp;isappinstalled=0]]></content>
      <categories>
        <category>计算机视觉论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习论文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Network in Network]]></title>
    <url>%2F2019%2F01%2F24%2FNetwork-in-Network%2F</url>
    <content type="text"><![CDATA[2014年 论文地址： https://arxiv.org/abs/1312.4400 论文核心NIN特点: 微型网络: 增强模型在感受野（receptive field）内对局部区域的辨别能力; GAP全局平均池化: 强化了特征图与分类的对应关系; GAP本身是结构化的正则化器，能避免整体结构的过拟合； 卷积层使用线性滤波器（卷积核）来扫描输入，后面接一个非线性激活函数。而卷积核是广义线性模型（generalized linear model ）GLM，抽象程度低（该特征对同一概念的变体是不变的）,用更有效的 非线性函数逼近器 代替 GLM 可以增强局部模型的抽象能力。当样本的隐含概念（latent concept）线性可分时，GLM可以达到很好的抽象程度，例如：这些概念的变体都在GLM分割平面的同一边，而传统的CNN就默认了这个假设——认为隐含概念（latent concept）是线性可分的。然而，同一概念的数据通常是非线性流形的（nonlinear manifold），捕捉这些概念的表达通常都是输入的高维非线性函数。在NIN中，GLM用“微型网络”结构替代，该结构是一个非线性函数逼近器。 本文作者选择多层感知器实例化微型网络，该感知器是一个通用函数逼近器，也是一个通过反向传播训练的神经网络。 该图是 单独的 mlpconv 层。这里有没有尝试过其他微型网络结构？？？？（可创新吗） NIN 网络结构 NIN 的整体结构是一系列 mlpconve层 的堆叠，最上层接一个 GAP层 和 分类层。 mlpconv层 间的子层可以被相加，像CNN和maxout网络一样。上图展示了一个包含三个mlpconv层的NIN。每个mlpconv层，包含一个三层的感知器，NIN和微型网络的层数都是灵活的，可以根据具体任务微调。 这里没有采用传统CNN的 全连接层 进行分类，而是直接通过 全局平均池化层（GAP）输出最后一个 mlpconv层特征图的空间平均值作为类别的置信度值，然后将得到的向量输入 softmax层。]]></content>
      <categories>
        <category>计算机视觉论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习论文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++梳理笔记]]></title>
    <url>%2F2019%2F01%2F20%2FC-%E6%A2%B3%E7%90%86%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[测试内容 删除线 链接 C++学习笔记类型转换： 隐式转换： 低类型转换为高类型 浮点数（直接舍掉小数，不四舍五入） + 整数 显式转换： int **(**z**) = (**int**)** z **= static_cast\&lt;**int**\&gt; (**z**)** 。。。 数据的输入和输出：信息的流动 输入： 输出： 流类库的操纵符： 程序控制： if, while, for, do-while , break, continue, { switch,case,default } ; do-while: do 语句 // 先执行一次 while(表达式)； for的范围，遍历容器： 自定义类型： 类型别名： typedef double Area, V; using Area = double 枚举类型： 有限的个数 不限定作用域： enum 类型名 { 变量值列表} 限定作用域： 注：枚举元素是常量，不能赋值 枚举元素有默认值，默认0,1,2,3,4，声明时可以另外指定 可以进行关系运算 auto类型 和decltyoe类型 decltype( float( i )) j = 2; // j值是2，类型是float; auto m = 2.5; // m 为float; 结构体( C语言中的)： struct struct MyTimeStruct{ //定义 结构体类型 unsigned int year,mouth,day,hour,min,sec; }; 函数： 可重用的功能模块（定义和调用）函数定义： 形参不占用空间，调用时分配； 函数调用： 调用前要函数声明： int sum( int a, int b); 1. 函数的嵌套调用： 2. 函数的递归调用： 直接或者间接调用自身 计算n! unsigned int fac( unsigned int n){ if (n == 0) return 1; return fac( n - 1) * n; } 汉诺塔 分析： 1. A 上的n-1个盘子移动到B上（借助C）; 2. A上剩下的盘子移动到C上； 3. B上的n-1个盘子移动到C上（借助A） void move(char src, char obj) { cout &lt;&lt; src &lt;&lt; &quot;---&gt;&gt;&gt;&quot; &lt;&lt; obj &lt;&lt; endl; } void hanoi(int n, char src, char medium, char obj) { if(n == 1) move(src, obj); else{ hanoi(n-1, src, obj, medium); move(src, obj); hanoi(n-1, medium, src, obj); } } 函数的参数： 形参不占用空间，调用时分配； 计算结果返回多个（利用引用） 多个参数时，从后开始传 引用类型（&amp;）： 必须初始化，该类型不可改变，是其他变量的别名 int i, j; int &amp; ri = i; // 定义int引用类型变量 ri, 初始化为i的引用 含有可变参数的函数：（两种方法） 所有实参类型相同：initializer_list&lt;int&gt; li; //类模板, 都是常量 具体看第九章 类型不同： 内联函数（inline）： 用函数体内的语句，替换函数调用表达式，编译时完成，类似 #define声明： inline int calArea(int a){ } 要求： 1. 不能有循环，switch语句 2. 定义在调用之前 3. 不能有异常接口声明 constexpr 函数：（常量表达式函数）带默认参数的函数： int getVa(int length, int weight = 2) 函数的重载：（C++多态性的重要机制，编译过程中实现）函数体同名，参数类型不同/参数个数不同 int add(int x, int y); float add(float x, float y); float add(float x, float y, float z); C++系统函数： #include &lt;cmath&gt; |_ |_ #include &lt;cstdlib&gt; |_ |_ #include &lt;cstdio&gt; |_ |_ #include &lt;ctime&gt; |_ |_ 类和对象类：构建对象的蓝图， 对象：由类创建，含有数据和方法 封装：对数据和操作数据的方法的组合绑定 继承：在已有类基础上，形成新的类 多态： 构造函数：定义对象时，通过构造函数初始化 析构函数：删除对象时，通过析构函数释放资源 类和对象的定义：定义类： class { //类名称 public: // 公有成员,外部接口 private: // 私有成员 protected: int hour = 0; // 类内初始化 // 保护型成员 } 注意：不指定类型，默认为私有； 成员函数： |_ 内联成员函数： 类内声明或者inline关键字 |_类外实现：void 类名称::成员函数名称（）{ } 构造函数： 在创建对象时，自动调用来初始化数据 与类名相同 构造函数有初始化列表 格式 类名（string s, lei i）：s(初始值)，i(初始值){ }； 委托构造函数：一个构造函数 通过另一个构造函数 初始化复制构造函数：用途： 用存在的对象 去初始化新对象 （通过引用旧的对象） 函数f的形参是类的对象，调用f时，将用实参对象初始化形参对象 函数g的返回值是类的对象，用return的对象来在主调函数中初始化一个无名对象 析构函数：生存期结束，删除清理工作，不能有return，不能有参数 class 类名{ public: 类名（形参）； // 构造函数 类名（const 类名&amp; 旧对象名）； // 复制构造函数 =delete是不生成 ~ 类名（）； } 注：未声明时，编译器自己生成一个默认的 前向引用声明：两个类相互引用时，某个类在引用之前就声明 class A; //前向引用声明，只是一个标识符，不是万能的 class B{ public: void A(B b); } class A{ public： void B（A a）; } 结构体：特殊的类，默认是公有的，可以有函数成员 //公有成员 int a; protected: int b; private: int c; }; 联合体：目的：存储空间的共用，成员不能同时有效，比如某人语文课成绩，只有一种可能； union Mark{ // 成绩的联合体， 只有一个成立 char grade; //等级类的成绩 bool pass; // 是否通过的成绩 int percent; //百分制成绩 } 枚举类：enum class 枚举类型名： 底层类型（int）{ 枚举列表 }; //默认 int 优势： 强制作用域 —必须在枚举类 枚举类型名：：枚举值，不同枚举类可以有同名值了 转换限制 —枚举对象不能与整型 隐式转换 底层类型 —可以指定 数据共享和保护：作用域分类：函数原型作用域： 形参的范围在（）内，所以不需要名字也行，int area( int ); 局部作用域 函数{ }内 if、for、while { }内 类作用域： 类外访问类的成员 静态成员：通过 对象名.成员名 访问 非静态成员： 文件作用域 命名空间作用域： 10章 对象的生存期：静态生存期： 整个程序结束后消失 函数内的静态对象， 用static ，全局寿命，只局部可见 动态生存期： 离开作用域后消失 下次进函数重新生成对象 类的静态数据成员： static 声明 为该类所有对象共享，具有静态生存期 必须在类外定义和初始化，类内声明，用：：指明所属于的类 比如记录 类产生了多少对象；opencv中的Mat对象好像用到了？？？？ class base{ public : static int _num;//声明 }; int base::_num=0; //真正定义 类的友元： 破坏数据封装和数据隐藏的机制 尽量不用 友元函数： 类声明中由关键字 friend 修饰说明的非成员函数 可以在其函数体内访问对象的private,protected成员 但必须通过对象名：：访问，函数参数为类的引用 友元类：class A{ friend B; public: void display(){ count &lt;&lt; x &lt;&lt; enld; } private: int x; } class B{ public: void set(int i); void display(); private: A a; } void B::set(int i){ a.x = i; // B类中改变 A类私有值 } void B::display(){ a.display() } 共享数据的保护：常类型：const常对象：必须初始化，不可更新 class A{ } A const a; // a是常对象 常成员：(不可以放在构造函数体内复制，可以在初始化列表中) A：：A(int i):a(i){ } 常数据成员：const修饰的 静态常数据成员： static const int b; 常函数成员（用来处理常对象的函数） 不更新对象的数据成员 声明和实现都带const class A{ void f（int a）const; } void A::f(int a) const{ }; // f是常对象函数, 处理常对象 常引用：不可更新 引用是双向传递的，避免修改原值的方法就是常引用； const A&amp; a; 常数组： 常指针： 多文件结构和预编译命令： .h 系统使用 .hpp 个人使用(类的声明,函数的声明) .cpp (类的实现，函数的实现) 外部变量：文件作用域中定义的变量默认是外部变量，其他文件使用前，extern声明 将变量和函数限制在编译单元内：namespcae: namespace{ //匿名的命名空间，外部不可调用任何东西 int i; void fun(){ i++; } } 预编译命令： #include&lt; &gt; 标准方式搜索，从系统目录include #include”” 先当前目录搜索，没有再标准搜索 #define #undef 删除有#define的宏 #if 表达式 // 条件编译指令 --- #else --- #endif #ifndef 标识符 --- #else --- #endif 数组，指针与字符串：数组：定义： int arr**[**m**][**n**]**…; 注：二维数组中 arr[1] 第二行首地址 数组作为函数参数： 数组名做参数： 形参，实参都是数组名，传入的是地址 对象数组： 定义：类名 数组名[对象元素个数] 访问：数组名[下标].成员名 基于范围的for循环：c++11,自动遍历整个容器 for( auto x : 容器){ } for( auto &amp;x : 容器){ } 注意： auto &amp;x是元素引用，auto x是元素的副本 auto推导出的类型是容器中的值类型 ：冒号后的表达式只执行一次 指针：定义： static int i; static int * p = &amp;I; 指针的初始化和赋值：指针的算术运算，关系运算：指针数组： 类名 *p[2]; 指向数组的指针： int **p; 指向二维数组的指针 指针与函数： 指针做参数：大批量数据提高效率 指针类型的函数：返回类型是指针 int * function(int i){return 全局或者静态的 }；// 不能返回非静态局部变量 指向函数的指针：实现函数回调的功能 定义： 数据类型 (*f)(参数表); 数据类型：返回值 对象指针： 定义： 类名 *对象指针名 = &amp; 对象； 访问对象： 对象指针名->成员名 （*对象指针名）.成员名 this 指针：成员函数的一个隐士参数，初始化为对象的地址，不可改变 隐含于类的每个非静态成员函数中 指出成员函数所操作的当前的对象 *this 是当前对象地址 动态内存分配：new 类型名 (初始化列表) // 返回首字节地址 delete 指针p //p一直在，删除的只是p指向的对象申请的空间 动态数组：new 类型名[数组长度] delete[] 数组首地址p指针 智能指针：C++11内存管理 unique_ptr: 不允许多个指针共享资源，标准库中move可以转移指针，但原来指针会失效 shared_ptr: 多指针共享 weak_ptr: 可复制共享 Vector对象：类模板 优势： 封装任何形式的动态数组，自动创建，删除 下标越界检查 定义： vector &lt;元素类型&gt; object（长度） object.begin() object.end() object.size() auto 遍历vector for(auto e: object); 对象的复制和移动： 浅层复制和深层复制：复制对象用到复制构造函数，默认的复制构造只传递了指针，两个变量指向同一块内存，释放其中一个，再释放第二个会出错； 浅层：实现对象间数据一一对应的复制，但两个对象指向同一内存 深层：当对象成员是指针类型，应该对指针所指对象进行复制。 类名::类名(const 类名&amp; v){ size = v.size; data_ptr = new Ponit[size]; for(int i=0; i \&lt; size; ++i){ data_ptr[i] = v.data_ptr[i]; } } 移动构造：C++11,省去了构造和删除临时对象的过程 class_name(class_name &amp;&amp;old)::xptr(old.xptr){ n.xptr = NULL; // 原来的指针清空 } C风格字符串：字符数组string类：常用构造函数： string(); //默认构造，长度为0 string s1; string(const char *s) //指针s所指向的字符串常量初始化该对象 string s2 = “abc”; string(const string &amp;rhs) //复制构造函数 string s3 = s2; 访问：下标访问 整行字符串的输入： cin 被空格隔开 getline(cin,s2); //包含#include\ getline(cin,s2,’,’); 继承和派生： 充分利用原有的继承：保持已有类的特征来构造新类 派生：在已有类基础上新增自己的特性 基类：父类 派生类：子类 直接基类和间接基类 单继承： class 派生类名：继承方式 基类名{ //继承方式， 成员声明；//新增成员的声明 } 多继承： class 派生类名：继承方式1 基类1，继承方式2 基类2{ 成员声明； } 继承的方式：控制：派生类对基类成员的访问权限 公有继承 public 基类中的pubilc和protected访问属性在派生类中不变 基类的pravate不可被对象直接访问 私有继承 ：内部可以访问基类的公有和保护成员，但是其对象不再可以访问 保护继承 ：基类的公有和保护，到这都成了保护成员，类内可以访问，但对象不能 派生类的构成： 吸收基类成员 改造基类成员 增加同名成员，基类成员被覆盖（重新定义继承的成员函数必须用虚函数） 添加新成员 类型转换：基类和派生类之间： 派生类的对象可以隐含转换为基类对象； 派生类的对象可以初始化基类的引用； 派生类的指针可以隐含转换为基类的指针； 派生类的构造函数：默认情况下，基类的构造函数不被继承，派生类需要自己构造 c++11，using语句继承基类构造函数 派生类的复制构造函数：派生类的析构函数：虚基类：多态性运算符重载：//双目运算符 函数类型 operator 运算符（参数） { // 参数个数 = 原操作数个数 - 1 } //前置单目运算符，返回引用所以可以当左值 函数类型 &amp; operator ++（无参数） { return * this; } //后置单目运算符， 函数类型 operator ++（参数为int类型） { old = *this; ++(*this); //调用的前置 return old; } 重载为非成员函数： 列出所有操作数 至少有一个自定义类型参数 后置单目运算，参数要增加int,但不用写形参名 要操作某类对象的私有成员，则可声明为该类的友元函数 虚函数：virtual改造基类成员，实现动态绑定；必须是非静态成员 原理：编译时先不确定和哪个类的成员对应，在程序运行时刻，再对应； #include &lt;iostream&gt; using namespace std; class Base1{ public: virtual void display() const; //虚函数，不要用内联 }; void Base1::display() const{ cout &lt;&lt; &quot;Base1 &quot; &lt;&lt; endl; } class Base2:public Base1{ public: virtual void display() const; } void Base2::display() const{ cout &lt;&lt; &quot;Base2&quot; &lt;&lt; endl; } 虚析构函数：打算通过基类指针调用某一个对象的析构函数（执行delete）虚表和动态绑定： 虚表： 每个多态类都有虚表； 存放各个数函数的入口地址； 每个对象有指向当前类的虚表的指针（虚指针vptr）； 动态绑定： 构造函数为对象的虚指针赋值 抽象类：含有纯虚函数的类,不能直接定义对象 纯虚函数： 基类中声明的虚函数，在基类中没有定义具体的操作，要求在派生类中根据实际需求完 成自己的版本： virtual 函数类型 函数名**(**参数名**) =** 0**;** override 和 final :C++11override声明的函数，必须在基类中找到原型； final 不允许继承或者覆盖； 模板函数魔板：整数和浮点数求绝对值，需要多次重载函数，但是用函数模板，只需要设计通用功能；template\&lt;模板参数表> // 类型：class或者typename 常量： 函数定义 template&lt;typename T&gt; T abs(T x){ return x&lt;0?-x:x; } 类模板：template&lt;模板参数表&gt; class 类名{ 类成员声明; } //类成员定义 template &lt;模板参数表&gt; 类型名 类名&lt;模板参数标识符列表&gt; :: 函数名(参数表) { } 线性群体：按位置顺序有序排列直接访问： 数组类模板： 索引访问： 顺序访问： 链表类和结点类模板： 单链表：每个结点包括数据和指针，只有一个指向后续结点的称为单链表； 单链表结点类模板： template &lt;class T&gt; class Node{ private: Node&lt;T&gt; *next; public: T data; Node(const T&amp;item,Node&lt;T&gt;* next = 0); //构造函数 void insertAfter(Node&lt;T&gt; *p); //插入 Node&lt;T&gt; *deleteAfter(); //删除 Node&lt;T&gt; *nextNode() const; } template &lt;class T&gt; void Node&lt;T&gt;::insertAfter(Node&lt;T&gt; *p){ // *p是要插入的结点 // p节点的指针指向当前节点的后续结点 p-&gt;next = next; // next是原链表待插入位置的结点的指针 next = p; } template &lt;class T&gt; Node&lt;T&gt; *deleteAfter(){ Node&lt;T&gt; * tempPtr = next; if (next == NULL) //判断是否是删除最后的元素 return 0; next = tempPtr = next; return tempPtr; } 插入： 头插法：可以当队列 尾插法：栈 删除： 待查询： explicit关键字 构造函数 explicit可以抑制内置类型隐式转换 泛型设计基本概念： 编写不依赖具体数据类型的程序，通用的； STL简介：(Standard Template Library) C++ string类库入门： #include &lt;iostream&gt; #include &lt;string&gt; using namespace std; int main() { // 构造函数： string str1 = &quot;Yesterday&quot;; string str2(&quot;Today&quot;); string str3(&quot;Hello&quot;,2); //取c风格字符串 长度为 2 作为初值，即&quot;He&quot; string str4(str1, 6); // 始于位置6开始的字符串，即&quot;day&quot; string str5(str1,6,1); // 始于6，长度1，即&quot;d&quot; string str6(1,&#39;a&#39;); //6个&#39;a&#39; // 赋值，交换 str1.assign(&quot;hahahaha&quot;); //重新赋值 swap(str1,str2); //交换两个字符串内容 str1=&quot;Today&quot; str2=&quot;hahahaha&quot; // 追加 str1 += &quot; we&quot;; // += 可追加 string对象，字符串，字符 str1.append(&quot; ar&quot;); // append 可追加 string对象，字符串 str1.push_back(&#39;e&#39;); //push_back 只能追加字符 str1 = &quot;Today we are&quot; // 插入 str1.insert(0,&quot; family&quot;); //str1 = &quot;Today we are family&quot; // 删除 str1.erase(2,1); //第2个位置开始， len = 1 个字符 str1.clear(); //删除全部 // 访问字符串 string s = &quot;asdfgh&quot;; cout &lt;&lt; s[1]; // &#39;s&#39; cout &lt;&lt; s.at(2); // &#39;d&#39; // 查找 int position = s.find(&#39;f&#39;,0); // 从0开始查找第一次出现‘f’的坐标 // 替换 s.replace(s.find(&#39;f&#39;),3,&quot;ZZZ&quot;); //替换find的位置处 3个字符串为 “ZZZ” // 分割 getchar(); return 0; }]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[eclipse-and-github]]></title>
    <url>%2F2019%2F01%2F19%2Fgit%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[内容概要 本地 首次上传到 github 本地 更新到 github github 首次下载到 本地 github 更新到 本地 本地首次上传到 github 进入 github官网，选择 New repository 复制地址 http:XXXXXXXXXX.git 本地 右键自己的项目文件夹，选择 git bash here 克隆 github 仓库到本地(执行如下命令), 会在本地产生一个 github 上仓库同名的文件夹 XXX，将工程所有内容移入文件夹内 git clone http:XXXXXXXXXX.git cd XXX, 进入该目录，执行以下操作： git add . // git status git commit -m &quot;此次提交的备注信息&quot; git push -u origin master 本地更新到 github方法与上节中的5一致。 首次下载到本地git clone http:XXXXXXXXXX.git 更新到本地git pull]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[欢迎来我的小屋！]]></title>
    <url>%2F2019%2F01%2F18%2F%E6%AC%A2%E8%BF%8E%E6%9D%A5%E6%88%91%E7%9A%84%E5%B0%8F%E5%B1%8B%2F</url>
    <content type="text"><![CDATA[欢迎来到我的小屋做客]]></content>
      <categories>
        <category>闲聊</category>
      </categories>
      <tags>
        <tag>闲聊</tag>
      </tags>
  </entry>
</search>
