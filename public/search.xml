<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[版本控制-git]]></title>
    <url>%2F2020%2F03%2F28%2F%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6-git%2F</url>
    <content type="text"><![CDATA[1. git 的作用 版本控制 协同开发 2. 文件的状态 untracked (新建的文件) unmodified （提交后进入仓库的文件与当前文件相同，即没修过） modified (commit 之前) staged （commit 之后） 3. 一般使用流程初始化仓库 git init 变更的文件加入暂存区 git add . 提交变更 git commit -m 查看commit日志, 并返回某一次提交的版本 git log (### 弹出commit id) git reset 7hdadsu2qe21e921821e --hard 如果想恢复最新的 git relog 从暂存区 移除某些文件（add 的文件有多余） git reset &lt;fileName&gt; 4. 分支合作管理 创建分支 git checkout -b &lt;分支name&gt; &lt;template继承的commit,默认当前&gt; 切换分支 git checkout master 查看所有分支 git branch 合并分支的变更（合并到当前master） git meger branch-2 有冲突时，会提示====== 5. remote 仓库的使用 下载远端仓库到本地 git clone ......git 创建本地的分支 git checkout -b local-A 在远端仓库设置分支(第一次需要) git push -set-upstream origin local-A 提交本地分支到远端 git push 第一次拉取远端仓库的分支，到本地 git fetch git checkout -b &lt;name&gt;origin&lt;template继承的commit,默认当前&gt; 以后再从远端更新本地 git pull （自动fetch + merge） 其他命令 git merge git pull git fetch git rebase （版本合并时。。）]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>版本控制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合-目录]]></title>
    <url>%2F2020%2F01%2F01%2FJava-%E9%9B%86%E5%90%88%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Java 集合类都在java.util包中，提供了如可变长数组，集合，队列，堆栈，链表等数据结构。宏观上看整个结构可以分为两大部分： 属于单列集合Collection 接口和带映射的多列集合的Map接口。 Collection 接口：集合的基本操作和属性 List 接口：有序列表 ArrayList： 可变长的数组。 LinkedList： 双端队列的链表结构。 Set 接口： 不重复元素的集合 HashSet： HashMap 实现的，无序。 TreeSet： HashMTree 实现的，有序。 Queue 接口： Deque 接口： LinkedList Map 接口：key-value键值对的映射接口 HashMap：数组+链表 组成的哈希表，无序。 TreeMap：基于 红黑树的排序顺序 存储键/值对，有序。 Hashtable： HashMap类很相似，支持同步。 两个工具类： Arrays Collections]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机基础学习笔记]]></title>
    <url>%2F2019%2F12%2F16%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Computer-Basics-Notes-Linksgithub地址 学习笔记 我在学习计算机基础的过程中整理了一部分笔记，但都比较零散，有些稍微连贯的内容已经整理为博客，而大部分还没来得及整理成md格式，我先整理出链接。为了尽可能的清晰展示笔记脉络，下文将分门别类的列出，有需要请自取。笔记内容大部分都是参考了网上的博客以及书籍，整理笔记的习惯也是刚刚养成的，从开始整理笔记后，发现自己对于做过的东西，能够随着笔记很快的回忆起来，遇到一些问题能够快速从笔记里找到答案也是比较舒服的。笔记是用有道云分享的，我未来仍然会继续梳理这些笔记，如果有任何的错误或者意见，麻烦您联系我哈。 1.计算机基础篇 基础部分是一些语言相关的知识点。 1.1 Java 相关面向对象Java面向对象-多态 部分源码分析java.lang.Object 类 java.lang.Integer等基本类型包装类 接口和抽象类接口和抽象类的概念 java.io.Serializable接口 java.lang.AbstractStringBuilder java.lang.CharSequence接口 java.lang.Comparable 接口 java.lang.Iterable 接口 集合类Java集合的结构 java.util.Arrays工具类 java.util.Collections 工具类 集合类之间的转换 ##### Collectionjava.util.Collection 接口 java.util.List 接口 java.util.ArrayList 类 java.util.LinkedList 类 java.util.Vector类-线程安全 java.util.Stack类-线程安全 java.util.Queue 接口 java.util.Deque 接口 java.util.ArrayDeque 类 java.util.PriorityQueue 类 java.util.Set 接口 java.util.HashSet 类 Mapjava.util.Map 接口 java.util.TreeMap 类 java.util.HashMap 类 java.util.HashSet 类 IO 流标准步骤： 针对oj系统中的输入问题 java IO之AutoCloseable接口 多线程与并发synchronized 锁的JVM中实现原理-偏向 线程的创建 线程间的通信 线程的生命周期 对象和变量的并发访问（可见性，原子性 java.util.concurrent.locks包 J.U.C多线程1-Executor 框架的梳理 J.U.C多线程2-ThreadPoolExecutor线程 实现BlockingQueue接口的阻塞队列 J.U.C多线程3-CAS比较和交换 J.U.C多线程3-AQS 同步器框架的梳理 J.U.C多线程4-AQS框架的应用 其他泛型（泛型接口、泛型类、泛型方法） 字符串类型-String,StringBuilder.. java 编码规范- google Java8- Lambda 表达式 动手实现ArrayList java 知识点梳理 Java 刷题遇到的问题 基本数据类型and 初始化 jvm 虚拟机jvm1-内存模型-运行时数据区 jvm2-类加载机制 jvm3-对象的创建过程 jvm4-垃圾回收机制 自定义类加载器实现热部署，热替换 其他反射 Java 动态代理 1.2 数据库相关数据库的安装windwos 环境中 mysql 数据库安装 ubuntu 环境中 mysql 安装 学习笔记JDBC 使用 mysql必知必会 数据库-三大范式 数据库ER图基础 mysql必知必会1-DDL数据定义语言 mysql必知必会2-DML数据操作语句 mysql必知必会3-TCL事务控制语言 mysql必知必会4-数据类型和约束 mysql必知必会5-视图 mysql必知必会6-变量，存储过程，函数…. mysql必知必会7 (http://note.youdao.com/noteshare?id=79d83c8dcc01b75a80003dceb059f72b&amp;sub=FE5D2429B49241419E96AD94536EB55C) 索引优化1-索引的概念 索引优化2-Mysql索引的底层实现 索引优化3- explain 查看执行计划 Sql语句优化-查询截取分析 mysql-数据库锁的机制和原理 1.3 计算机网络相关互联网协议入门1-模型分层 互联网协议入门2-访问网页的过程 互联网协议入门3-TCP协议详细内容（传?.. 互联网协议入门4-Http协议（应用层）.n… 1.4 Python 相关Anaconda 使用 python 性能优化 python 内存管理 python 包管理 知识积累 python 数据模型 Set 集合 交并差运算 pandas 包pandas pandas-5缺失值处理 pandas-2索引和选择数据 pandas-分层和多级索引 pandas-6重复值处理 pandas-7时间处理 pandas-4分组与聚合 pandas-1数据结构 1.5 数据结构与算法相关链表链表入门-单链表 双向链表 树结构[树结构1-二叉树的种类 ](http://note.youdao.com/noteshare?id=3d87798249099424c8f688932e054d34&amp;sub=962de575938af485eb5896712a3fa88bhttp://note.youdao.com/noteshare?id=3d87798249099424c8f688932e054d34&amp;sub=962de575938af485eb5896712a3fa88b) 树结构2-二叉树的四种遍历 树结构3- 二叉排序树 树结构4- 完全二叉树-堆排序 树结构5-红黑树 栈栈 - Stack 动态规划动态规划-DP DFS和回溯算法- 暴力搜索的优化方案 一般算法二分查找 最大公约数与最小公倍数 素数和合数 排序算法-Java版 1.6 设计模式单例模式 汇总 2 计算机提高篇 提高篇是一些具体的学习方向，我学的比较杂。 2.1 机器学习与数据挖掘特征工程中的问题 决策树决策树-1基本概念 决策树-2 ID3算法 决策树-3 CART分类回归树 CART 分类回归树 sklearn 决策树使用技巧 sklearn整理-决策树 神经网络常见的神经网络 集成学习集成学习提升算法-Adaboost XGBoost -python package introduction 梯度提升树-GBDT sklearn整理-集成学习-随机森林 模型评价ROC与AUC 机器学习的方差与偏差 参与的一些竞赛ali-新人实战赛o2o优惠券使用预测 ccf-2019 dc-2019-商品购买转化率预测 竞赛提升方法-tricks sklearn 脑图 2.2 大数据与Hadoop多台机器的分布式环境安装1.hadoop分布式集群安装 RPCHadoop RPC mapreduce 实验实验1. 多表关联 实验2. 最高温度统计 实验3. 单表关联 2.3 VUEVUE 安装开发环境搭建 2.4 Spring 框架Spring-1控制反转（IOC）和依赖注入 Spring-2AOP 面向切面编程-基于动态代 Spring-3事务管理Transaction Manager 3. 工具使用vscode 使用记录 4. 收藏的书籍书籍收藏 http://note.youdao.com/noteshare?id=57e1d26f1f078dc70a5b18392b29a841)]]></content>
      <categories>
        <category>计算机基础梳理</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树2-ID3算法]]></title>
    <url>%2F2019%2F09%2F12%2F%E5%86%B3%E7%AD%96%E6%A0%912-ID3%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[决策树-2 ID3算法 决策树-1基本概念中已经提到了ID3算法，这篇博客再梳理一遍，算法描述部分搬运了统计学习方法的内容，更详细内容可以参考这本书。 ID3 算法的思路 输入：训练集 D, 特征集 A， 阈值 ε； 分叉：最优属性划分依据是 最大信息增益； 结束条件：用完所有特征，特征信息增益很小，树的深度超过限制； 返回：一颗树T。 ID3 算法描述 这里的描述可作为编程实现时的指导，树的建立过程是递归实现。1234567891011121314151617181920212223def ID3Tree(D, A，ε ): if D 的实例属于同类别 K || 特征集 A 特征空: 1. 决策树 T为单结点树 2. 标记类别 K (数量最多的类别) return T else: 1. 计算所有特征相对于 D 的信息增益 2. 找到信息增益最大特征 Amax if Amax 小于 阈值 ε: 1. 决策树 T为单结点树 2. 标记类别 K (数量最多的类别) return T else: 依照特征 Amax 的每一个取值 ai，划分数据集Di, 并且标记Di类别，构建子节点 # 递归 对每个结点Di，以Di为训练集， A-Amax为特征集，递归调用 ID3Tree() return 由结点和子结点构成的树]]></content>
      <categories>
        <category>机器学习方法</category>
      </categories>
      <tags>
        <tag>ID3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树3-CART分类回归树]]></title>
    <url>%2F2019%2F09%2F12%2F%E5%86%B3%E7%AD%96%E6%A0%913-CART%E5%88%86%E7%B1%BB%E5%9B%9E%E5%BD%92%E6%A0%91%2F</url>
    <content type="text"><![CDATA[CART-分类回归树CART 算法的思路 特征选择：最优属性划分依据是 基尼系数（分类）/平方误差（回归）； CART 树是二叉树结构。 主要就两步骤： 树的生成 树的剪枝 分类树 分类树与ID3, C4.5的流程一致。 回归树 回归树选择最佳划分属性和划分点时的依据是 平方误差。 一张图即可理解。 与分类树的主要区别是选择最佳属性的评价指标变了。根据最小化均方误差的原则选择。]]></content>
      <categories>
        <category>机器学习方法</category>
      </categories>
      <tags>
        <tag>CART树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas-8分层和多级索引]]></title>
    <url>%2F2019%2F08%2F13%2Fpandas-%E5%88%86%E5%B1%82%E5%92%8C%E5%A4%9A%E7%BA%A7%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[pandas -8 分层和多级索引 Multi-level indexing. 在 “pandas -2 索引和选择数据” 一节中, 已经提到了如何选择行列元素, 而Series 和 Dataframe 是低维度的数据结构，对于更高维度的数据，可以通过分层和多级索引来实现。 分层索引的创建 创建分层索引的方式有很多, 这里直接摘抄自官方的文档，可以通过元组，列表，Dataframe, arrays 等方式生成分层索引。 同时要知道，通过 groupby 分组操作之后得到的也是这种分层结构。 123456789101112131415161718192021222324252627282930313233343536373839404142434445 // 1. 元组In: arrays = [[&apos;bar&apos;, &apos;bar&apos;, &apos;baz&apos;, &apos;baz&apos;, &apos;foo&apos;, &apos;foo&apos;,&apos;qux&apos;, &apos;qux&apos;], [&apos;one&apos;, &apos;two&apos;, &apos;one&apos;, &apos;two&apos;, &apos;one&apos;, &apos;two&apos;, &apos;one&apos;, &apos;two&apos;]] tuples = list(zip(*arrays))Out: [(&apos;bar&apos;, &apos;one&apos;), (&apos;bar&apos;, &apos;two&apos;), (&apos;baz&apos;, &apos;one&apos;), (&apos;baz&apos;, &apos;two&apos;), (&apos;foo&apos;, &apos;one&apos;), (&apos;foo&apos;, &apos;two&apos;), (&apos;qux&apos;, &apos;one&apos;), (&apos;qux&apos;, &apos;two&apos;)]In: index = pd.MultiIndex.from_tuples(tuples, names=[&apos;first&apos;, &apos;second&apos;]) df = pd.Series(np.random.randn(8), index=index) Out: first second bar one 0.469112 two -0.282863 baz one -1.509059 two -1.135632 foo one 1.212112 two -0.173215 qux one 0.119209 two -1.044236 dtype: float64 // 2. dataftame index = pd.MultiIndex.from_frame(df) // 3. arrays In: arrays = [np.array([&apos;bar&apos;, &apos;bar&apos;, &apos;baz&apos;, &apos;baz&apos;, &apos;foo&apos;, &apos;foo&apos;, &apos;qux&apos;, &apos;qux&apos;]), np.array([&apos;one&apos;, &apos;two&apos;, &apos;one&apos;, &apos;two&apos;, &apos;one&apos;, &apos;two&apos;, &apos;one&apos;, &apos;two&apos;])] s = pd.Series(np.random.randn(8), index=arrays) 从DataFrame 产生 MultiIndex1df = df.set_index([&apos;col1&apos;,&apos;col2&apos;]) MultiIndex 转化成 列1df = df.reset_index() 选择不同层 查看不同层的索引值。 1234567In: index.get_level_values(0) index.get_level_values(&quot;name&quot;) Out: Index([&apos;bar&apos;, &apos;bar&apos;, &apos;baz&apos;, &apos;baz&apos;, &apos;foo&apos;, &apos;foo&apos;, &apos;qux&apos;, &apos;qux&apos;], dtype=&apos;object&apos;, name=&apos;first&apos;) 根据不同层索引 123456df[&quot;bar&quot;]df[&quot;one&quot;]df[&quot;bar&quot;][&quot;one&quot;]// 元组df.loc[(&apos;bar&apos;, &apos;two&apos;)] 注意, 切片时不会改变 多层索引。]]></content>
      <categories>
        <category>pandas系列教程</category>
      </categories>
      <tags>
        <tag>pandas-MultiIndex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas-6重复值处理]]></title>
    <url>%2F2019%2F08%2F09%2Fpandas-%E9%87%8D%E5%A4%8D%E5%80%BC%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[pandas -6 重复值处理 如果你想找到或者删除 DataFrame中重复的行, 可以使用 duplicated 和 drop_duplicates 查找重复值1234567891011121314151617181920212223242526272829example: col1 col2 c 0 one x -1.067137 1 one y 0.309500 2 two x -0.211056 3 two y -1.842023 4 two x -0.390820 5 three x -1.964475 6 four x 1.298329In: // 单列 df.duplicated(&quot;col1&quot;, keep=&quot;first&quot;) // 多列 // df.duplicated([&quot;col1&quot;, &quot;col2&quot;], keep=&quot;first&quot;) Out: 0 False 1 True 2 False 3 True 4 True 5 False 6 False dtype: bool // 默认 keep = &quot;first&quot;,第一次出现的不算重复，返回False // keep = &quot;last&quot;, 最后出现的不算重复 // keep = False, 重复值均返回 True 删除重复值123456789In: df.drop_duplicates(&apos;col1&apos;) Out: col1 col2 c 0 one x -1.067137 2 two x -0.211056 5 three x -1.964475 6 four x 1.298329]]></content>
      <categories>
        <category>pandas系列教程</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas-5缺失值处理]]></title>
    <url>%2F2019%2F08%2F09%2Fpandas-%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[pandas -5 缺失值处理 统计数据中存在缺失值是十分常见的问题, 而对于缺失值的处理，是数据挖掘的一个重要环节。pandas 有一系列的方法处理缺失值。 缺失值的类型判断方法只记住万能的 pd.isnull 即可。 数值 pd.isna pd.isnull np.isnan 字符串 pd.isna pd.isnull 时间 pd.isna pd.isnull np.isnat 缺失值的统计 df.isnull().sum() 丢掉缺失值 // 某列有缺失值, 删除 df[ pd.isnull(df[&quot;columns&quot;])] // Series df.columns.dropna() // DataFrame // axis: axis=0 （默认）表示操作行，axis=1 表示操作列; // how : any 表示一行/列有任意元素为空时即丢弃，all 一行/列所有值都为空时才丢弃。 // subset: 参数表示删除时只考虑的索引或列名。 // thresh: 比如 thresh=3，会在一行/列中至少有 3 个非空值时将其保留。 df.dropna(axis=0, how=&quot;any&quot;, subset=[&quot;city&quot;, &quot;sex&quot;]) 填充缺失值 数据量少的情况下，直接丢掉不可取，可以适当补充数据。 // 前值填充 ffill 后值填充 bfill df.columns.fillna(method = &quot;ffill&quot;) // 实值填充 df.fillna(0) // 均值填充 df[&quot;columns&quot;].fillna(df[&quot;columns&quot;].mean(), inplace=True) // 众数 mode()[0] // 中位数 median()]]></content>
      <categories>
        <category>pandas系列教程</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas-7时间&日期处理]]></title>
    <url>%2F2019%2F08%2F06%2Fpandas-%E6%97%B6%E9%97%B4-%E6%97%A5%E6%9C%9F%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[pandas -7 时间&amp;日期处理pandas 常出现的时间格式 字符串类型 object 一般是字符串类型，pandas 储存string时 使用 narray， 每一个object 是一个指针 datetime 类型 datetime64 timedelta 类型 表示时间差的数据类型 类型转换 object 2 datetime 123456789// 方法1df[1] = pd.to_datetime(df[1], format=&apos;%d.%m.%Y&apos;)# 指定format，速度会加快很多。// 方法2dateStr = &quot;2019-02-03&quot;myDate = datetime.strptime(dateStr, &quot;&quot;%Y-%m-%d&quot;&quot;) datetime 2 object 1234df[&quot;time_list&quot;] = df[&quot;time_list&quot;].strftime(&quot;%Y-%m-%d&quot;)// Y 2019// y 19 datetime 相关操作1234567891011// 查看列元素的年，月，日，星期（整数型）df[&quot;time&quot;].dt.yeardf[&quot;time&quot;].dt.monthdf[&quot;time&quot;].dt.daydf[&quot;time&quot;].dt.weekday # 星期一是0// 一年中的第几天,第几周df[&quot;time&quot;].dt.dayofyeardf[&quot;time&quot;].dt.weekofyear// 查看列元素 某年的数据数量df[df[&quot;time&quot;].dt.year == 2019].shape 时间运算 计算时间差 12345// 计算时间差， 结果为timedeltadf[&quot;时间差&quot;] = df[&quot;时间1&quot;] - df[&quot;时间2&quot;]// 转换成 天数差df[&quot;时间差&quot;].days 计算未来日期 123// N天后的日期// 天 days, 时 hours， 周 weeksdf[&quot;时间&quot;] = df[&quot;时间1&quot;] - timedelta(days=10)]]></content>
      <categories>
        <category>pandas系列教程</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习模型的偏差与方差]]></title>
    <url>%2F2019%2F08%2F01%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE%2F</url>
    <content type="text"><![CDATA[机器学习的方差与偏差 方差与偏差，总是迷迷糊糊的，每次看了就会，过了就忘。今天看到一个非常棒的解释，迫不及待马上整理下来了。 机器学习的目标函数机器学习模型学习的是数据集的条件概率分布，得到一个决策函数。整个学习过程围绕着最小化（或者最大化）目标函数进行优化参数，目标函数的通常形式的定义是： Obj = L(θ) + λΩ(θ) L(θ) 是损失函数，衡量模型对训练集拟合程度的好坏(对应偏差)； Ω(θ) 是正则项，是衡量模型的复杂程度（对应方差）； 目标函数定义为损失函数和正则项两部分，是为了平衡模型的偏差和方差（Bias Variance Trade-off）。 偏差与学习器 偏差描述了 学习器的拟合能力 （对训练集的）。 学习器在训练集表现越好，损失越低，则模型的偏差越小。 方差与学习器 方差描述了 学习器的泛化能力(对测试集)。 学习器在测试集表现越好，则模型的方差越低。 偏差与方差之间的关系 我们最想要的是低偏差，低方差的模型，然而现实很难达到两者都极致的低。有时候需要用提高偏差来降低方差，毕竟模型的泛化能力十分重要。 当损失函数达到极小值，模型对训练集的拟合达到了最佳效果，==对应着低偏差==，而这种情况下，往往对训练集的特点学的过于细微，而缺乏泛化能力。 根据 ==奥坎姆剃刀原则==, 同样准确率，模型越简单越好，所以通过正则化等方式，削弱模型的拟合能力，降低过拟合的风险。通过提高偏差，来主动降低方差。 最终我们要平衡方差与偏差，从而得到一个合理的模型。 调整方差与偏差的方法待补充。。。]]></content>
      <categories>
        <category>机器学习方法</category>
      </categories>
      <tags>
        <tag>方差与偏差</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost]]></title>
    <url>%2F2019%2F07%2F30%2FXGBoost%2F</url>
    <content type="text"><![CDATA[xgboost 学习笔记 主要内容均来自官方文档，官方文档是英文版，所以简单的翻译了一下，方便日后查看。详细内容见官方手册 安装 XGBoost 12345ubuntu -python3: pip3 install xgboost 导入: import xgboost as xgb 数据接口XGBoost 可以从以下结构中加载数据： LibSVM text format file CSV Numpy 2D array Scipy 2D sparse array Pandas XGBoost binary buffer file. 加载的数据都放在 DMatrix对象中，下面是具体加载的过程演示： LibSVM text format file 12dtrain = xgb.DMatrix('train.svm.txt')dtest = xgb.DMatrix('test.svm.buffer') CSV 123// 需要指定标签所在的列dtrain = xgb.DMatrix('train.csv?format=csv&amp;label_column=0')dtest = xgb.DMatrix('test.csv?format=csv&amp;label_column=0') XGBoost 不支持种类特征，需要先加载为Numpy数组，然后进行 `one-hot` 编码;推荐使用pandas 加载数据. Numpy 1234data = np.random.rand(5, 10) # 5个样本，每个样本10个特征label = np.random.randint(2, size=5) # 二值标签dtrain = xgb.DMatrix(data, label=label) Scipy 12csr = scipy.sparse.csr_matrix((dat, (row, col)))dtrain = xgb.DMatrix(csr) Pandas 123data = pandas.DataFrame(np.arange(12).reshape((4,3)), columns=['a', 'b', 'c'])label = pandas.DataFrame(np.random.randint(2, size=4))dtrain = xgb.DMatrix(data, label=label) 保存为 XGBoost 二进制文件 12dtrain = xgb.DMatrix('train.svm.txt')dtrain.save_binary('train.buffer') 缺失值处理 1dtrain = xgb.DMatrix(data, label=label, missing=-999.0) 样本权重 12w = np.random.rand(5, 1)dtrain = xgb.DMatrix(data, label=label, missing=-999.0, weight=w) 参数设置 XGBoost 可以通过列表或者字典来设置参数，例如： Booster 参数 123param = &#123;'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'&#125;param['nthread'] = 4param['eval_metric'] = 'auc' 指定多个评估指标 1param['eval_metric'] = ['auc', 'ams@0'] 指定验证集来监视性能 1evallist = [(dtest, 'eval'), (dtrain, 'train')] 训练 模型训练 12num_round = 10bst = xgb.train(param, dtrain, num_round, evallist) 模型保存 1bst.save_model('0001.model') 保存模型和特征 1234# dump modelbst.dump_model('dump.raw.txt')# dump model with feature mapbst.dump_model('dump.raw.txt', 'featmap.txt') 模型加载 12bst = xgb.Booster(&#123;&apos;nthread&apos;: 4&#125;) # init modelbst.load_model(&apos;model.bin&apos;) # load data 早停 如果你有验证集，则可以使用早停机制来寻找最佳的 num_round, 需要将 验证集传入 evals,如果传入多个，则使用最后一个。1train(..., evals=evals, early_stopping_rounds=10) 如果模型在 early_stopping_rounds次，监控的参数 param[&#39;eval_metric&#39;] 都没有提升，则会停止训练，train 返回的是最后一次训练的模型，而不是最佳模型，最佳模型可以通过一下方式找到： bst.best_score bst.best_iteration bst.best_ntree_limit # 使用最佳模型 同样的，监控多个参数时，最后一个参数起早停的作用。 预测已经训练好的模型，或者已经加载的模型可以拿来预测新数据：123data = np.random.rand(7, 10)dtest = xgb.DMatrix(data)ypred = bst.predict(dtest) 使用最佳的迭代次数的模型：1ypred = bst.predict(dtest, ntree_limit=bst.best_ntree_limit) 绘制你可以使用绘图模块来画出树结构： 绘制参数重要性 1xgb.plot_importance(bst) 绘制目标树 1xgb.plot_tree(bst, num_trees=2) Ipython 中绘制树 1xgb.to_graphviz(bst, num_trees=2)]]></content>
      <categories>
        <category>XGBoost</category>
      </categories>
      <tags>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas-2索引和选择数据]]></title>
    <url>%2F2019%2F07%2F25%2Fpandas-2%E7%B4%A2%E5%BC%95%E5%92%8C%E9%80%89%E6%8B%A9%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[pandas -2 索引和选择数据 对于一种数据结构,最基本的操作就应该是增删改查了。 1. 行列选择行选择和列选择有许多方法，很容易记混，常用的要记住。主要方法有三种： iloc, loc, [] 行选择 切片 12345// 切片df[a:b]// 隔1行选择df[::2] 指定位置 df.iloc[1, 1] df.iloc[1:10, 2:3] df.iloc[1:10][&#39;Price&#39;] 指定索引 df.loc[&quot;index1&quot;, &quot;index2&quot;] 按照条件查找 df[( df[&quot;row2&quot;] == 1) &amp; (df[&quot;row2&quot;] == &quot;null&quot;)] df.loc[( df[&quot;row2&quot;] == 1) &amp; (df[&quot;row2&quot;] == &quot;null&quot;)] 根据列的多个值,选择行 List = [1,2,3,4,5]train[train[&quot;customer_id&quot;].isin(List)] 列选择 通过列标签选择单列 df[&quot;price&quot;] 通过列标签选择多列 df[[&quot;price&quot;, &quot;time&quot;]] 通过列索引,选择前3列 df.iloc[:, :3] 行列选择 df.loc[&quot;index1&quot; : &quot;index2&quot;, [&quot;price&quot;]] df.iloc[a:b][&#39;Price&#39;] 随机采样行或者列1234567s.sample(frac=0.5)// 参数// 默认选择行，n = 行数， frac = 比例// replace: 默认False 无放回采样// weights: 样本采样权重// axis: 默认=0 行, =1 列// random_state=2 分类别等数量抽样1234567891011121314# 降采样 -- 分类抽样def subSample(df_x, splitAttribute = &quot;Attribute4&quot;): subsampleNum = min(df_x.groupby(splitAttribute).size()) print(subsampleNum) df_x_sub = df_x.iloc[1:2,:] #df_y_sub = df_y.iloc[1:2,:] for label in df_x[splitAttribute].unique(): tmp_x = df_x[df_x[splitAttribute] == label] random_list = random.sample(range(0,len(tmp_x)),subsampleNum ) df_x_sub = df_x_sub.append(tmp_x.iloc[random_list,:]) df_y_sub.append(tmp_y.iloc[random_list,:]) return df_x_sub #, df_y_sub 2. 行的增删改查增加 单列 12345// 末尾增加 df[&quot;new col&quot;] = None // 指定位置增加，在2列后 df.insert(2,&apos;city&apos;) 多列 pd.concat([df, pd.DataFrame(columns=[&quot;C&quot;,&quot;D&quot;])]) 单行（待验证） 12345// loc 添加 df.loc[‘5‘] = [3, 3, 3, 3] // set_value 添加 df.set_value(‘5‘, df.columns, [3,3,3,3], takeable=False) 多行 多行相当于合并两张表了,可以参考(merge,concat)方法。 test_ = pd.merge(tmp, data.loc[:,[&quot;customer_id&quot;, &quot;label&quot;]],on=[&#39;customer_id&#39;],how=&#39;left&#39;,copy=False&quot;) 删除 列 12345678910// del 方法 def df[&quot;col_name&quot;]//根据列名 drop 方法 df.drop([&quot;b&quot;, &quot;c&quot;], axis=1,inplace = True)axis = 1 列axis = 0 行// 根据列号 drop 方法 df.drop(df.columns[[1,2]], axis=1, inplace=True) 行 12345// 根据索引 删除行 df = df.drop([1, 2])// 根据value 删除行 df = df[~df[&quot;col&quot;].isin(5,9) 按照条件删除行 1df.drop(df[df[&quot;order_pay_time&quot;] &lt; pd.to_datetime(&quot;2013-11-12 23:59:44&quot;) ].index) 修改与查找 单值修改和查找时, 参考选择行列方法。 多值查找时， 按条件查找 df_train[( df_train[&quot;row&quot;] == 1) &amp;( == &quot;null&quot;)] query 查找 df.query(&#39;(a &lt; b) &amp; (b &lt; c)&#39;) 替换 单个替换，inplace = True 覆盖源文件 df.replace(to_replace, value, inplace = True) 多值替换—-字典 df.replace({&quot;A&quot;:&quot;B&quot;, 29:100}) 按条件替换 df.where(df &gt; 0, -df, inplace=True) 交换两列的位置1df[[&apos;B&apos;, &apos;A&apos;]] = df[[&apos;A&apos;, &apos;B&apos;]]]]></content>
      <categories>
        <category>pandas系列教程</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas -1数据结构]]></title>
    <url>%2F2019%2F07%2F22%2Fpandas-1%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[pandas -1 数据结构 pandas 基本操作都很简单，只是在刚开始学习的过程中，容易忘掉一些API，导致完成一些操作时，总会想着翻翻手册，这一系列博客，是对这些方法进行了梳理，可作为入门学习的参考材料。平时经常翻阅。 “index” (axis=0, default), “columns” (axis=1) 1. Series Series 是一个带有 名称 和索引的一维数组。 创建seriex1234567891011121314151617181920212223242526272829303132333435// Series 数组生成，指定数据类型In: user_age = pd.Series(data=[18, 30, 25, 40], dtype=float) Out: 0 18 1 30 2 25 3 40 dtype: int64// 增加索引 indexIn: user_age.index = [&quot;Tom&quot;, &quot;Bob&quot;, &quot;Mary&quot;, &quot;James&quot;] Out: Tom 18 Bob 30 Mary 25 James 40 dtype: int64 // 表头In: user_age.index.name(&quot;name&quot;) Out: name Tom 18 Bob 30 Mary 25 James 40 dtype: int64 像字典一样使用series1234567891011121314151617// index 当键值In: user_age[&quot;Tom&quot;] user_age.get(&quot;Tom&quot;)// 切片-列In: user_age[2:3] // 按条件查找In: user_age[user_age &gt; 30] Out: name James 40.0 Name: user_age_info, dtype: float64 像向量一样使用series 可以传递给np方法 1234567891011// 整列加减In: user_age + 1 Out: name Tom 19.0 Bob 31.0 Mary 26.0 James 41.0 Name: user_age_info, dtype: float64 2. DataFrame DataFrame 是一个带有 名称 和索引的二维数组，像一张Excel表格。 创建DataFrame1234567891011121314151617181920212223242526// DataFrame 根据字典生成In: index = pd.Index(data=[&quot;Tom&quot;, &quot;Bob&quot;, &quot;Mary&quot;, &quot;James&quot;], name=&quot;name&quot;) data = &#123; &quot;age&quot;: [18, 30, 40], &quot;city&quot;: [&quot;BeiJing&quot;, &quot;ShangHai&quot;, &quot;HangZhou&quot;] &#125; user_info = pd.DataFrame(data=data, index=index) user_infoOut: // DataFrame 根据二维列表生成In: data = [[18, &quot;BeiJing&quot;], [30, &quot;ShangHai&quot;], [25, &quot;GuangZhou&quot;], [40, &quot;ShenZhen&quot;]] columns = [&quot;age&quot;, &quot;city&quot;] user_info = pd.DataFrame(data=data, index=index, columns=columns) user_info]]></content>
      <categories>
        <category>pandas系列教程</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[评价指标 ROC与AUC]]></title>
    <url>%2F2019%2F07%2F20%2F%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-ROC%E4%B8%8EAUC%2F</url>
    <content type="text"><![CDATA[非均衡分类问题 非均衡分类问题指的是每个类别的错误代价不同。 比如疾病检测中,有病患者诊断健康的代价，要比健康人诊断成有病（可能性）造成的影响更为严重。 对于常用的预测模型，通常是有预测的概率值，我们找到一个合适的截断点作为正负类别的界限。显然再不同的任务下，截断点选择是不同的。我们使用Precison 和Recall的新度量指标来针对特定任务下选择合适的截断值。 真实标签 预测为正 预测为反 正例 TP FN 反例 FP TN Precison(查准率)： 1P = TP/(TP+FP) Recall(召回率)： 1R = TP/(TP+FN) 当正负样本不不均衡,人为修改测试集中的正负比例时, P-R曲线波动很大，但是ROC曲线变化很小。 ROC 曲线 可以研究学习器的泛化性能。 加图 横坐标：真阳率，正例被正确预测的概率 1FPR = FP/(TN+FP) 纵坐标：假阳率，负例被预测错误的概率1TPR = TP/(TP+FN) ==理解四点一线==： (0, 0): FP = TP = 0, 所有样本预测为负 (1, 1): FP = TP = 1, 所有样本预测为正 (1, 0): FP = 1, TP = 0, 所有正样本预测为负 (0, 1): FP = 0, TP = 1, 完美预测 对角线：随机猜测的值。 AUC值AUC(Area under Curve) 被定义为ROC曲线的下侧面积。一般在(0.5~1)之间。 计算方法 几何角度 直接计算曲线下的面积，梯形 概率角度 任取一对正负样本对，正样本score大于负样本score的概率 python 实现链接 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npfrom sklearn.metrics import roc_curvefrom sklearn.metrics import aucfrom time import time# y: 标签# pred： 预测值def myAUC(y, pred): auc = 0.0 p_list = [] # 正负例的索引 n_list = [] for i, y_ in enumerate(y): if y_ == 1: p_list.append(i) else: n_list.append(i) # 构成p-n对 p_n = [(i,j) for i in p_list for j in n_list] pn_len = len(p_n) for tup in p_n: if pred[tup[0]] &gt; pred[tup[1]]: auc += 1 elif pred[tup[0]] == pred[tup[1]]: auc += 0.5 auc = auc/pn_len return auc## 产生一组数据y = np.array([1,0,0,0,1,0,1,0,])pred = np.array([0.9, 0.8, 0.3, 0.1,0.4,0.9,0.66,0.7])## sklearn 结果fpr, tpr, thresholds = roc_curve(y, pred, pos_label=1)tim = time()print(&quot;sklearn AUC:&quot;,auc(fpr, tpr))print(&quot;sklearn AUC time:&quot;, time()-tim)## myAUC 结果tim = time()print(&quot;\nmyAUC:&quot;,myAUC(y,pred))print(&quot;myAUC time:&quot;, time()-tim)]]></content>
      <categories>
        <category>机器学习方法</category>
      </categories>
      <tags>
        <tag>ROC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成学习提升算法-Adaboost]]></title>
    <url>%2F2019%2F06%2F25%2F%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-Adaboost%2F</url>
    <content type="text"><![CDATA[Adaboost 算法原理及推导 Adaboost 是Boosting算法的代表。Boosting可将许多弱学习器组合达到强学习器的效果。 Adaboost 是通过提升错分数据的权重值来改善模型的不足。其主要的流程是： 先训练一个基学习器； 根据基学习器的表现，改变样本的分布，使得错误分类的样本得到更多的关注； 改变分布后的样本再训练新的基学习器，如此迭代； 加权组合这些基学习器。 一、Adaboost算法原理 “Adaptive Boosting”（自适应增强） Adaboost算法中，每个样本有对应的权重D,每个基分类器也有对应的权重α，然后是下边的三步骤： Step1：初始化训练集的权重； 如果有N个样本，则每一个训练样本最开始时都被赋予相同的权重：1/N。 迭代：Step2： 改变样本分布，训练基学习器； 错分的样本权重D会增加；准确率高的分类器的权重α会更大； Step3: 加权组合弱学习器。 二、Adaboost算法推导给定训练集 T=\{(x 1, y 1),(x 2, y 2) \ldots(\mathrm{xN}, y \mathrm{N})\}其中， y_{i} \in\{-1,1\}步骤1：初始化训练集的权重D。每个训练样本的初始权重w相同，均为1/N, D_{1}=\left(w_{11}, w_{12} \cdots w_{1 i} \cdots, w_{1 N}\right) w_{1 i}=\frac{1}{N}, i=1,2, \cdots, N步骤2：训练基学习器，改变训练样本分布，迭代训练新的学习器。用m=1,2…M 代表迭代的轮数，每轮产生的学习器为 h_{m}(x) 计算学习器 h_{m}(x) 在训练数据集上的分类错误率 E_{t} (误差的权值和): E_{t}=P\left(G_{m}(x) \neq y_{i}\right)=\sum_{i=1}^{N} w_{m i} I\left(G_{m}\left(x_{i}\right) \neq y_{i}\right) 计算学习器 h_{m}(x) 的权重α： \alpha_{m}=\frac{1}{2} \ln \frac{\left(1-E_{m}\right)}{E_{m}} 更新训练集样本权重。 D_{m+1}=\left(w_{m+1,1}, w_{m+1,2} \cdots w_{m+1, i} \cdots, w_{m+1, N}\right)w_{m+1, i}=\frac{w_{m i}}{Z_{m}} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right), i=1,2, \cdots, N这里的 Z_{m} 时规范化因子: Z_{m}=\sum_{i=1}^{N} w_{m i} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right) 迭代训练学习器 步骤3：加权组合弱学习器。 f(x)=\sum_{m=1}^{M} \alpha_{m} h_{m}(x)H(x)=\operatorname{sign}(f(x))=\operatorname{sign}\left(\sum_{m=1}^{M} \alpha_{m} h_{m}(x)\right)三、Adaboost算法解释 模型为加法模型(基函数的线性组合),损失函数为指数函数,学习算法是前想分布算法的二类分类学习法.]]></content>
      <categories>
        <category>机器学习方法</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归]]></title>
    <url>%2F2019%2F04%2F27%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[逻辑回归就是西瓜书里的对数几率回归，名为回归，实际则是分类算法。其实质是利用线性回归模型的预测结果来逼近真实标记的对数几率。 一句话概括就是: 逻辑回归假设数据服从 **伯努利分布** ,通过 **极大化似然函数** 的方法，运用 **梯度下降** 来求解参数，来达到将数据二分类的目的。 线性模型如何处理二分类问题？ 在处理二分类任务时，我们希望模型能预测样本属于某类别的概率[0, 1]，或者直接输出类别的标签{0, 1}。 线性模型不能直接来完成这项任务，因为其输出是一个实际值，范围也不一定在0~1之间，无法体现我们所想要的概率或者标签。 y=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b因此，我们需要找到一种能把线性模型输出映射到 概率 或者 标签 的方法； 如何转化为标签？ 单位阶跃函数 如何转化为概率？ sigmoid函数 (二分类): 将无穷范围的值限制在(0, 1)之间 softmax函数（多分类）: 所有类别概率和为1 由于单位阶跃函数存在跳跃点，在跳跃点不可导，想选择梯度下降法来优化时，只能选择sigmoid函数了。 逻辑回归模型 逻辑回归就是这样的一个过程：面对一个分类问题，建立代价函数，然后通过优化方法迭代求解出最优的模型参数，然后测试验证我们这个求解的模型的好坏。 逻辑回归的优缺点 优点 速度快，适合二分类问题 简单易于理解，直接看到各个特征的权重 能容易地更新模型吸收新的数据 缺点 对数据和场景的适应能力有局限性，不如决策树算法适应性那么强 逻辑回归的用途 寻找主要影响因素： 通过学习到的权重值，得到不同因素对结果的影响力大小 预测： 预测事件发生的概率 建模常规步骤 寻找 h 函数（预测函数） 构造 J 函数 (损失函数) 利用梯度下降等方法最小化 J 函数，并求取参数 LR基本模型以下就是逻辑回归的基本模型： y=\frac{1}{1+e^{-z}} z=w^{\top} x+b取倒数 \frac{1}{y}=1+e^{-z}取对数 \ln \left(\frac{1}{y}-1\right)=-z \ln \frac{y}{1-y}=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+bln(y/(1-y)) 就是对数几率 代价函数线性模型常用的目标函数 均方误差 用在逻辑回归模型是非凸函数，非凸函数通过梯度下降法容易陷入局部最小值，因此需要想办法找到代价函数，且代价函数为凸函数。 极大似然法定义代价函数这里通过极大似然估计的方法来定义目标函数： 极大似然估计： 这里想了很久才算有点理解，之前就一直卡在这里。。极大似然估计就是可以利用已知数据来推测出产生这些数据的最可能的环境条件。 举个栗子，当我们扔硬币时，出现的可能性有两个，要么正面朝上（事件A），要么反面朝上(事件B)，假设出现某事件的可能性只与硬币的 质量分布θ 有关系，假设我们做了n组实验（A出现m次），这样能得到两种情况的概率 P(A), P(B),我们要推测出 质量分布θ 是多少，也就是 参数估计 ,即 质量分布θ 是多少时，才最可能出现当前实验的结果。抛硬币的事件服从二项分布，那么给定了一组实验的情况下，似然函数是(似然函数与概率值相等)： L\left(\theta ; x_{1}, \ldots, x_{n}\right)=f\left(x_{1}, \ldots, x_{n} ; \theta\right)=\prod_{i=1}^{n} P\left(X=x_{i}\right)=\theta^{m}(1-\theta)^{n-m}假设一个数据集Cn,标签y∈{0,1}，预测值Θ,模型参数为w则似然函数可以写作: P\left( \theta _{\left( x_i \right)}|w \right) =\prod_{n=1}^N{\theta}_{xi}^{yi}\cdot \left( 1-\theta _{\left( x_i \right)} \right) ^{1-y_i} 取对数简化运算: \mathrm{L}(\mathrm{w}, \mathrm{b})=\sum_{i=1}^{N}\left[y_{i} \log \left(\emptyset\left(x_{i}\right)\right)+\left(1-y_{i}\right) \log \left(1-\emptyset\left(x_{i}\right)\right)\right]实际代价函数的样子: \mathrm{J}(\mathrm{w}, \mathrm{b})=-\frac{1}{N} \mathrm{L}(\mathrm{w}, \mathrm{b})=-\frac{1}{N} \sum_{i=1}^{N}\left[y_{i} \log \left(\emptyset\left(x_{i}\right)\right)+\left(1-y_{i}\right) \log \left(1-\emptyset\left(x_{i}\right)\right)\right]取对数之后的公式很符合理想的代价函数，当实际标签与预测结果相同，则代价为0，而相反时，会随着差值越大，损失越大。 因为 \operatorname{logit}(\mathrm{p})=\log \frac{p}{1-p} \log \frac{P(Y=1 | X)}{1-P(Y=1 | X)}=w \cdot x+b带入后继续化简 \mathrm{L}(\mathrm{w}, \mathrm{b})=\sum_{i=1}^{N}\left[y_{i}\left(\mathrm{w} \cdot x_{i}+\mathrm{b}\right)-\log \left(1+e^{\mathrm{w} \cdot x_{i}+b}\right)\right]直观解释直观上理解，代价函数就是分类错误的惩罚，那么当y=1时，prediction 越小时，损失应越大；y=0时prediction 越大时，损失应越小。 \operatorname{cost}=\left\{\begin{aligned}-\log (\hat{p}), & \text { if } y=1 \\-\log (1-\hat{p}), & \text { if } y=0 \end{aligned}\right.通过梯度下降来最小化代价函数因为我们找到的代价函数是凸函数，所以可以尝试用梯度下降来找到合适的参数w，b： \mathrm{w} :=\mathrm{w}-\alpha \frac{\partial J(w, b)}{\partial w} \mathrm{b} :=\mathrm{b}-\alpha \frac{\partial J(w, b)}{\partial b}偏导求解过程： 对w求偏导 \frac{\partial J(w, b)}{\partial w}=-\frac{1}{N} \sum_{i=1}^{N}\left[y_{i} \frac{1}{\partial\left(x_{i}\right)}-\left(1-y_{i}\right) \frac{1}{1-\emptyset\left(x_{i}\right)}\right] \frac{\partial \emptyset\left(x_{i}\right)}{\partial w} =-\frac{1}{N} \sum_{i=1}^{N}\left[y_{i} \frac{1}{\emptyset\left(x_{i}\right)}-\left(1-y_{i}\right) \frac{1}{1-\emptyset\left(x_{i}\right)}\right] \sigma\left(x_{i}\right)\left(1-\emptyset\left(x_{i}\right)\right) \cdot x_{i} =-\frac{1}{N} \Sigma_{i=1}^{N}\left(y_{i}-\emptyset\left(x_{i}\right)\right) \cdot x_{i^{*}}对b求偏导 \frac{\partial J(w, b)}{\partial b}=-\frac{1}{N} \sum_{i=1}^{N}\left[y_{i} \frac{1}{\emptyset\left(x_{i}\right)}-\left(1-y_{i}\right) \frac{1}{1-\emptyset\left(x_{i}\right)}\right] \frac{\partial \emptyset\left(x_{i}\right)}{\partial b} =-\frac{1}{N} \sum_{i=1}^{N}\left[y_{i} \frac{1}{\emptyset\left(x_{i}\right)}-\left(1-y_{i}\right) \frac{1}{1-\emptyset\left(x_{i}\right)}\right] \wp\left(x_{i}\right)\left(1-\emptyset\left(x_{i}\right)\right) =-\frac{1}{N} \Sigma_{i=1}^{N}\left(y_{i}-\emptyset\left(x_{i}\right)\right)带入公式后，得到最终推导的结果： \mathrm{w} :=\mathrm{w}+\alpha \frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\emptyset\left(x_{i}\right)\right) \cdot x_{i} \mathrm{b} :=\mathrm{b}+\alpha \frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\emptyset\left(x_{i}\right)\right) 通过以上公式可以看出，逻辑回归的梯度与 `sigmoid` 本身无关，只与 `y` 和`x`和 `w` 有关系。 进一步提高泛化能力 影响模型泛化能力的主因素是 过拟合, 过拟合问题比较容易理解，这里就不贴图了。下面分析一下产生过拟合的原因和解决办法。 过拟合产生的原因有哪些？过多的特征 怎么解决过拟合？ 减少特征数量 减少特征数量会导致部分信息丢失。 正则化 保留所有的特征，并且减小参数的大小。 正则化方法在代价函数上增加一个惩罚项，惩罚项应该是模型复杂度的单调递增函数，模型越复杂，惩罚项越大。 回归问题中，取平方损失（L2 范数），或者L1范数 J(\theta)=\frac{1}{2 m} \sum_{i=1}^{n}\left(\mathrm{h}_{\theta}\left(\mathrm{x}_{i}\right)-y_{i}\right)^{2}+\lambda \sum_{j=1}^{n} \theta_{j}^{2}这里的lambda 系数： 如果它的值很大，说明对模型的复杂度惩罚大，对拟合数据的损失惩罚小，这样它就不会过分拟合数据，在训练数据上的偏差较大，在未知数据上的方差较小，但是可能出现欠拟合的现象； 如果它的值很小，说明比较注重对训练数据的拟合，在训练数据上的偏差会小，但是可能会导致过拟合。 加入正则化后的参数更新： \theta_{j} :=\theta_{j}-\frac{\alpha}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x_{i}\right)-y_{i}\right) x_{i}^{j}-\frac{\lambda}{m} \theta_{j}python 实现空 参考资料极大似然估计]]></content>
      <categories>
        <category>机器学习方法</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树1-基本概念]]></title>
    <url>%2F2019%2F04%2F20%2F%E5%86%B3%E7%AD%96%E6%A0%91-1%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[决策树1- 基本概念决策树 上图来自西瓜书，是决策树的一种树形。生成决策树的过程，不断的根据样本的属性( 样本的某个特征 )划分样本子集。每个结点选择当前最优的属性作为划分依据，将样本集合不断的划分成更小的子集合，直到子集合中样本类别一致时或者没有可以划分的属性值时，则停止划分，标记为叶结点(叶节点代表一个类别)。 简单的介绍一下决策树的组成元素: 根节点: 所有的训练样本 内部节点: 对应某一个划分属性 叶节点： 对应某一种决策结果 判定测试序列： 某个样本在节点中传递的路径 所有节点都包含着不同数量的样本。 以上是分类树的例子，决策树也可以用作回归任务，如CART算法。决策树是GBDT,Xgboost等更高级结构的基础，所以尽量要掌握决策树的原理。 决策树算法的基本流程假设有一个数据集，其中的每个样本有多种特征，每个特征有不同的取值。通过这个数据集来生成一个决策树的一般流程可以归纳为: 特征选择 特征选择就是决策树分叉时，依据新节点的”纯度”，选择最优的划分属性; 决策树生成 树不断的分叉，直到样本的属性用光，或者树的深度达到了预定值，则结束分叉; 剪枝 如果一直树杈分下去，一定能够使得所有的样本都正确的归类，但这样会产生对训练集的过拟合，泛化能力变差，可以通过剪枝操作来改善泛化能力。 通过这三步，就可以生成一颗决策树了。下面来学习一下具体怎么进行特征的选择和剪枝。 如何选择最优的划分属性(分类树)? 决策树不断分叉的原因，是尽可能的让不同类别的样本划分到不同的节点，同类别的样本划分到同一个节点。而选择最优的划分属性（特征）的过程，相当于是遍历计算出所有特征的结果，找到能使分叉后子集合最 “纯” 的特征，就是最优的划分属性了。所以，该如何定义 “纯” ，需要借助信息论中 “信息熵” 的概念了。 熵 : 表示随机变量不确定性的度量,也就是混乱程度的一种度量。 假定数据集 D 中第 K 类样本所占的比例为 p_{k},则信息熵定义为: \operatorname{Ent}(D)=-\sum_{k=1}^{N} p_{k} \log _{2} p_{k}数据集包含的类别越少时越纯，Ent(D)也越小。 法1: 信息增益==ID3算法用到信息增益== 直白的讲就是决策树分叉前的信息熵减去分叉后的信息熵。 信息增益最大的特征就是最佳划分属性。 假定分叉前样本集 D 中的特征 a 有 V个可能的取值 \left\{a^{1}, a^{2}, \ldots, a^{V}\right\} ,当选择 a 做划分属性时，会分V个节点，每个节点上的子样本集合为 D^{v},同时为不同节点赋权重(按照样本的比例)，于是信息增益为: \operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)减数部分也叫 条件熵 缺点: 分叉时偏好取值较多的属性。 原因分析: 取值多的特征，样本更分散，所有得到的新节点”纯度” 趋于更高，熵更低，而划分前的增益不变的情况下，该特征增益更大。 比如，当特征的可能取值数量正好等于样本数量，那条件熵几乎为0，该特征一定会被选择。 法2: 信息增益率==C4.5算法用到信息增率== 相当于在法1基础上，增加了惩罚系数，可取值越多，系数越大。 Gain\_ratio\left( D,a \right) =\frac{Gain\left( D,a \right)}{IV\left( a \right)}\mathrm{IV}(a)=-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \log _{2} \frac{\left|D^{v}\right|}{|D|}IV(a) 是属性 a 的 “固有值”，内部属性。 缺点: 分叉时偏好取值较少的属性。 法3：基尼指数==CART决策树算法用到基尼指数== 反应从节点样本集合中随机抽取两个样本，类别不一致的概率。CART决策树默认为二叉树。 基尼值的定义: \operatorname{Gini}(D)=\sum_{k=1}^{|y|} \sum_{k^{\prime}=k} p_{k} p_{k^{\prime}}=\sum_{k=1}^{|y|} p_{k}\left(1-P_{k}\right)=1-\sum_{k=1}^{|\mathcal{Y}|} p_{k}^{2}选择特征 A 的情况下，针对 A 所有可能取值 a, 分别计算基尼指数： Gini\_index\left( D,a \right) =\sum_{v=1}^V{\frac{\left| D^v \right|}{|D|}}\text{}Gini\left( D^v \right)选择基尼指数最小的特征和切分点，作为最优划分属性。 三种决策树模型： 算法 特征选择标准 ID3 信息增益 C4.5 信息增益率 CART 基尼指数 对抗过拟合 — 剪枝处理 分支太多，容易过拟合，泛化能力变差。所以要适当剪枝，常用方法是预剪枝和后剪枝 剪枝操作包括的点也很多，这里只是简单描述一下，详细的参考未来的博客。www.elgong.top 预剪枝 预剪枝是在决策树生成的过程中，对每个结点在划分前先估计，根据划分前后验证集的精度，来决定是否划分； 只能估计当前结点可划分性，不能预测到未来节点划分的必要性，是贪心算法； 容易造成欠拟合。 后剪枝 先生成完整的树，再从叶结点往回计算，根据验证集精度是否提升决定是否剪枝； 泛化能力往往优于预剪枝，欠拟合风险小； 时间开销大。 属性为连续值时？ C4.5 算法采用二分法将连续值离散化 与离散属性不同，连续的属性可以在后代节点中再次使用 当数据中含有缺失值时？处理方法： 通过无缺失数据计算出三个参数： 无缺失样本占总样本比例 无缺失样中 K类别 占比 pk 无缺失样本中 v 属性样本占比 rv 对单样本增加一个权值 Wx, 无缺失样本的Wx = 1， 有缺失样本的Wx = rv*Wx。 在计算分支时，同一样本以不同的概率划分到不同的子节点中 当样本的属性已知：则把该样本划分进对应的子节点，权值=1； 当样本的该属性缺失：则把该样本同时划入所有的子节点，样本权值需要更新为`Wx = rv*Wx。 决策树的优缺点==优点==： 便于理解和可视化； 训练需要的数据少，不需要对数据进行规范化； 可同时处理数值型，类别型数据； 是白盒模型，可解释； ==缺点==： 容易产生过于复杂的模型 -&gt; 泛化能力差 （剪枝，限制叶节点所需要的最小样本数，最大深度） 决策树不稳定，微小变化会产生不同的树（集成多棵树可以缓解） 难学NP问题（启发式学习） 异或，奇偶，很难被学习到]]></content>
      <categories>
        <category>机器学习方法</category>
      </categories>
      <tags>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟机类加载机制]]></title>
    <url>%2F2019%2F04%2F14%2F%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[虚拟机类加载机制绑定 绑定指的是把一个方法的调用与方法所在的类(方法主体)关联起来。 静态绑定 前期绑定。在程序执行前方法已经被绑定，此时由编译器或其它连接程序实现java当中的方法只有final，static，private和构造方法 是前期绑定的。 动态绑定 运行时绑定。在运行时根据具体对象的类型进行绑定。在java中，几乎所有的方法都是后期绑定的。 类加载机制类从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期包括：加载、验证、准备、解析、初始化、使用和卸载 七个阶段，前五个阶段属于类加载的过程。这是开始顺序，每个阶段可能交错。 step1：加载加载阶段，虚拟机完成的任务： 通过一个类的全限定名来获取起定义的二进制字节流。 二进制字节流来源： Class文件，Jar包、从网络中获取（最典型的应用便是Applet）、由其他文件生成（JSP应用）等； 将该字节流的静态存储结构转换为方法区的运行时数据结构。 在Java堆中声称一个代表这个类的 java.lang.Class对象，作为对方法区中这些对象的入口。 三种主要的类加载器？ 类加载机制采用了委托模式。 类加载器与类本身一同确定这个类在Java 虚拟机中的唯一性。 启动类加载器 Bootstrap ClassLoader 该加载器由C++实现，不属于类，负责加载 /JDK/JRE/lib/rt.jar，主要加载 JVM 工作需要的类； 扩展类加载器 Extension ClassLoader Bootstrp loader 加载 ExtClassLoader, 该加载器由sun.misc.Launcher$ExtClassLoader 实现，它负责加载`/JDK\JRE\lib\ext目录中的类，自己的类打包jar放入也可以； 应用程序类加载器 Application ClassLoader Bootstrp loader加载完ExtClassLoader后，就会加载AppClassLoader,并且将AppClassLoader的父加载器指定为 ExtClassLoader， 负责加载classpath所指定的位置的类。 自定义类加载器 如果要自定义类加载器，需要继承 应用程序类加载器 三者如何协调工作？类加载机制采用了委托模式。启动类加载器加载其他类加载器，当需要加载类时，优先父类加载器工作。 双亲委派模型的工作流程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把请求委托给父加载器去完成，依次向上，因此，所有的类加载请求最终都应该被传递到顶层的启动类加载器中，只有当父加载器在它的搜索范围中没有找到所需的类时，即无法完成该加载，子加载器才会尝试自己去加载该类。 step2：验证验证的目的？保证class 文件的字节流符号JVM 虚拟机的要求，不危害虚拟机自身安全。虽然 Java 源码编译不会产生如数组越界之类的错误，但class 文件可以被编辑。 都需要哪些验证？ 文件格式验证 验证字节流是否符合 class文件规范（如开头是否为魔数0xCAFEBABE， 主次版本号是否可以被当前虚拟机处理等） 元数据验证 验证字节码描述的信息是否符合Java 语言规范（如类的继承实现是否符合语法规范） 字节码验证 该阶段验证的主要工作是进行数据流和控制流分析，对类的方法体进行校验分析，以保证被校验的类的方法在运行时不会做出危害虚拟机安全的行为。 符号引用验证 发生在虚拟机将符号引用转化为直接引用的时候（解析阶段中发生该转化，后面会有讲解），主要是对类自身以外的信息（常量池中的各种符号引用）进行匹配性的校验。 step3： 准备 准备阶段是正式为类变量（静态变量）分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。注意的是： 只为类变量分配内存； static 类变量初始值为默认初始值，而不是程序中的值； public static int value = 3； // 实际初始值为0 同时被final和static修饰，那么在准备阶段变量value就会被初始化为ConstValue属性所指定的值； public static final int value = 3； // 实际初始值为3 step4： 解析 解析阶段是虚拟机将常量池中的符号引用转化为直接引用的过程。 解析动作主要针对类或接口、字段、类方法、接口方法四类符号引用进行，分别对应于常量池中的CONSTANT_Class_info、CONSTANT_Fieldref_info、CONSTANT_Methodref_info、CONSTANT_InterfaceMethodref_info四种常量类型。 step5： 初始化 真正开始执行类中定义的Java程序代码,初始化阶段是执行类构造器()方法的过程。 类构造器 &lt;clinit&gt;() 执行规则： 按照在源文件中出现的顺序收集类变量 和 静态语句块 static{ }; 静态语句块中只能访问定义在之前的变量，而在块后定义的变量只能被赋值，但不能被访问； 优先构造父类； 父类中的 静态语句块 static{ } 优先于子类中的变量赋值操作； 不是必须的，当类或者接口中没有静态语句块或者没有变量赋值，则可以不生成&lt;clinit&gt;()； 双亲委派被破坏 3种情况下 第一次： JDK1.2之前还没有双亲委派，但是有用户自定义类加载器（通过loadClass（）加载），为了兼容老版本，设计者添加了 protected findClass()，直接调用用户loadClass（）方法； 第二次：原则上 越基础的类由越上层的加载器进行加载,但是有些情况下基础类需要调用用户的代码。如JNDI, JDBC，JCE,JAXB，JBI,这时候引入了线程上下文加载器； 第三次：“代码热替换”和“模块热部署”出现，希望程序中的功能模块像键盘鼠标一样直接更换，而不是重启。OSGi 是这个标准化模块，具体还没看。。。]]></content>
      <categories>
        <category>深入理解Java 虚拟机</category>
      </categories>
      <tags>
        <tag>Java类加载机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[华为软挑2019]]></title>
    <url>%2F2019%2F04%2F01%2F%E5%8D%8E%E4%B8%BA%E8%BD%AF%E6%8C%91%2F</url>
    <content type="text"><![CDATA[参加软挑的一些感悟写在前边的话 我本科一直在做嵌入式相关的项目,这是第一次参加软件类的竞赛,不得不说过程确实很刺激,最后止步杭厦赛区50强也很是遗憾,明明很接近,最后输在了代码效率上,本地成绩很好的 python代码 ,上传测评运行时间超限（官测环境比本地性能好，普遍情况是用时远超本地，其中华为云主机集体宕机半小时，很多人测过的最优代码，最后再上传就超时了https://bbs.huaweicloud.com/forum/thread-16237-1-1.html）。超限原因主要两点，一是自己在实现调度器时的代码臃肿，二是正式赛数据量大增。但进入32强的不少组并没有实现调度器，完全 随机时间发车+单车路径最优规划 , 感觉很(￣_,￣ )。这比赛不实现调度器，意义少了一半，再用上这种偷鸡方法，没觉出来还有啥意义。唉，菜鸡就是菜鸡。总之，这次经历对我还是有不少积极的影响的，下面就总结一下吧。 题目解读本次比赛主要做的是 动态路网下多车辆调度问题, 参赛者合理安排数万车辆在合理时间从出发点到达各自的目的地，程序上传至官方服务器，运行后得出 所有车辆出发时间和规划的路径 ，将在官方调度器中进行调度，完成车辆调度用时即为最终成绩。具体是比赛任务书中花了很大篇幅讲了官方调度器的规则，并且论坛前期几乎天天在更新规则补充，最终完全准确实现的队伍只见过一个，其他很对队伍是很接近，但总有差别。我们自己实现的调度器，调度时间完全对的上，但是所有车辆调度总时间总是差了一些。分析原因有以下两点: 我们实现的调度规则还有与官方一些差异；（但是我们实现的和部分队伍对比的结果完全一致，但和官网就有差异，猜测可能有些规则官方描述的有些差异，或者某细节被我们忽略了） python 即使版本相同，但是在不同机器上结果确实有差异，这个也被官方证实了https://bbs.huaweicloud.com/forum/thread-15889-1-1.html。 总体上参考任务书，下面只简单梳理一下思路（这里默认已经熟悉了任务书）： 下面先附上官方伪代码 for(/* 按时间片处理 */) { foreach(roads) { /* 调整所有道路上在道路上的车辆，让道路上车辆前进，只要不出路口且可以到达终止状态的车辆 * 分别标记出来等待的车辆（要出路口的车辆，或者因为要出路口的车辆阻挡而不能前进的车辆） * 和终止状态的车辆（在该车道内可以经过这一次调度可以行驶其最大可行驶距离的车辆）*/ driveAllCarJustOnRoadToEndState(allChannle);/* 对所有车道进行调整 */ /* driveAllCarJustOnRoadToEndState该处理内的算法与性能自行考虑 */ } while(/* all car in road run into end state */){ /* driveAllWaitCar() */ foreach(crosses){ foreach(roads){ while(/* wait car on the road */){ Direction dir = getDirection(); Car car = getCarFromRoad(road, dir); if (conflict）{ break; } channle = car.getChannel(); /* 这里只指因下一道路有等待车辆阻挡而导致该车辆无法进入的情况 */ /* 其他情况均返回true,比如下一车道满无法进入（全是终态），或才是下一车道限速不能进入，该车辆停留在其当前车道最前方 */ /* 该车辆也是移动至其所在车道最前方，只有有车辆由等待变以终止，就对其车道后续车辆状态进行调整 */ if(!car.moveToNextRoad()) { break; } /* driveAllCarJustOnRoadToEndState该处理内的算法与性能自行考虑 */ driveAllCarJustOnRoadToEndState(channel); } } } } /* 车库中的车辆上路行驶 */ driveCarInGarage(); } 要调度的车辆分两种：路上的车和要上路的车 每个时间片先处理路上车，在处理上路车 路上的车处理步骤： step1标记状态， step2 移动车辆 车的状态：每个时间片（一个时间片指的是所有车辆一次调度完成）路上车辆有三种状态，未调度过的车是 无状态， 调度过但是由于阻挡或者其他原因不能移动的车标记为 等待状态， 调度过并且完成移动的车标记为 终态。 step1: 怎样标记状态？ - 这个时间片车辆最大行驶速度能超过该道路长度（超过了但不一定就能进入下一条道路），直接标记为`等待状态` - 这个时间片车辆最大行驶速度不能超过该道路长度，但是前方有车辆挡住自己将要走的路，直接标记为`等待状态` - 这个时间片车辆最大行驶速度不能超过该道路长度，并且前方没车辆挡住自己，**移动该车**，直接标记为`终止状态` step2: 按什么顺序调度车辆？ 路上的车： 处理次序： 按照ID升序反复遍历路口，直到所有车辆变成终态 对每个路口，按照ID升序遍历朝向这个路口的道路（也反复遍历，直到所有车进入终止状态，或者被阻挡无法移动） 每个道路上有多条线，按照优先级顺序处理车辆，只有第一优先级车辆完成调度，才能调度优先级低的车。 不过马路而被标记等待的车，不受优先级限制，阻挡车辆离开，这种车立马跟上 上路的车： 上路车按照ID升序处理 规划的时间因为前方无空位而未上路的车，顺延到下一时刻优先上路，即不参与下一时刻车辆ID升序发车。 哪些车参与优先级的排序？ 要过马路的车和车速超过当前道路剩余长度，但是根据任务书10-5条，不能进入下一道路的车都参与优先级排序。 大概就这些了，其他更细微的只能遇到才想起来了。 思路总结 这里只总结一下初赛的思路。 这个比赛就是合理安排车辆调度，以最短时间让所有车都到达终点。所以要找到合适的方法让车辆快速充满道路而不至于 锁死,锁死也是赛题的最难点。 道路上流动的车越多，越容易出现锁死情况；道路上流动的车越少，最终调度时间就越长。所以优化的目标变成了保证不死锁的情况下，让更多的车在道路上流动起来。 什么是死锁？死锁指的是，某个时间片道路上的车辆由于循环等待（形成了环形等待情况），导致无法再进一步调度任何车辆，导致调度失败，成绩为0。体现在调度器里，就是step2 反复调度路口时，等待状态的车辆数量不再减少，即锁死了。 怎么避免死锁？唯一可以避免的方法是完全实现调度器，和官方调度器一致，就可以准确判断到锁死，并且在规划道路时动态规划新路，解开环形等待的死亡链。可惜大家完全模拟出来调度器的几乎没有（据我一直水群了解到的情况看，是这样，不排除潜水大佬真的实现了）。所以呢，大部分人都是想尽办法的尽量减少锁死，无法完全避免，下面会举例几种方法。 有不完全正确调度器的解决方案 单车最优路径静态规划 + 遇锁死时对部分车动态规划 如果调度器不太一致时，就当某道路调度同一车辆多次，就给这个车强制规划新路径。 单车最优路径静态规划 + 遇锁死时把锁死车辆从路上删除，未来重新发车 分批次发车 + 每个批次单独规划路径 + 动态路阻 + 锁死车辆动态规划 这个是效果比较好的一种方法，练习赛后期成绩能进入前15名的方法。动态路阻指的是道路情况拥堵，这里选择了几个因素： 动态路阻 = 这个批次经过该道路车辆数量a + abs(道路限速 - 车速)b + （1 - 道路中路线数/最大线数）*c 路阻每个批次都清除一次，这样在调度器不准确的情况下很大程度上抑制了死锁的发生，当时采用这种方法之后，每个批次发车量明显可以提高很多。这里a，b，c是需要调节的参数。 没有调度器的解决方案如果没实现调度器，也有一些不错的方法，但是不算偷鸡。这里把路网当成计算机的网络，网络的带宽就是道路的线数，我们想让网络传输最大量的数据，但是网络本身承载能力有限制，我们要找到均衡流量的方法，让网络上流动的流量尽可能的均衡，这样再找到合适的参数，即网络最适合的承载车辆数目，保证网络流量不超过这个限制，也可以减少死锁的情况。即 分批次发车 + 每个批次单独规划路径 + 动态路阻 还有一种不太有意义的方法这种也被很多人叫做偷鸡方法，就是 单车路径最优规划 + 随机时间发车，然后就是调调调。这种方法优势是答案生成快，可以反复调无数次。而实现调度器的同学，基本半小时才能调一次参数，因为模拟调度的过程比较费时间，又加上动态路径规划，时间代价大大提高。 对单车的寻路算法想当然的觉得地图是平面的，因为官方给的任务书全是平面图，并且每个路口对应的四个街道都是有方向的，所以对路口直接建立了坐标系，有了每个点的方位坐标信息，也就很自然的选择了A*算法。结果没想到，最后正式赛当天出现的地图是这。。。样。。。的。。。，出现了高空立交桥，这还算直行吗。开始不知道地图变了样子，结果递归建坐标系的部分爆了bug，改了半天，卒了。后来看到群里可视化后的效果是下图，吐血了，赶紧换了Dijkstra算法。这两个算法有时间再总结。 我们组的结果结果就是止步初赛了，调度器 + 动态规划 + 动态路阻 + 。。。+ python 真是很费时间，本地要15分钟勉强出结果，服务器上直接超时。 放弃了调度器和动态规划， 只用了动态路阻，最后所剩时间不多了，只调了几次参数就到时间了。 比赛的经验与教训 比赛运行环境一定要保证和官方一致，不然结果会出现不一致。 如果还是这种复杂规则的情况，不要再选择python，速度确实有问题，代码能力差的人体现的更明显~、~、 好好理解题目在行动。 感悟结果有点惨淡，但是这段时间确实收获了很多，也多亏了两位队友的倾力相助，以及师兄的思路指导。郭同学为团队提供了大部分算法上的思路和代码；丁同学从开始比赛到最后一天，也一直在和我讨论着调度以及算法，纠正了我很多错误的理解，比赛的日子也是近段时间来最开心的日子，期待大家下一次的合作。画江湖之绿皮车将要回归。。。]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>华为软挑初赛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Residual Learning for Image Recognition]]></title>
    <url>%2F2019%2F01%2F25%2FDeep-Residual-Learning-for-Image-Recognition%2F</url>
    <content type="text"><![CDATA[2015年 论文地址： https://arxiv.org/pdf/1512.03385.pdf 通常情况下 神经网络层越深，特征的抽象程度越高，模型的表达能力越强，语义信息越丰富； 同时，网络越深，退化问题 越难解决; 退化问题是网络加深的障碍 简单的增加深度，会导致 梯度弥散 或者 梯度爆炸 ,可以通过标准初始化和中间层正则化（batchNorm）解决；但是随着继续增加深度，训练集准确率饱和，甚至下降（退化问题，不属于过拟合）。 神经网络就像一个｀Function Set｀，网络是输入 x 到输出 out 的映射关系 F，out = F(x) ，所以网络层越深，这个 Set 包含的函数越复杂，那能够拟合的情况就更多（意思是解决的问题就更多），上图是作者在CIFAR-10 数据集上的对比实验，56层网络是在20层网络上的重复叠加（最差也应该是前22层的恒等映射），但是较深网络训练集的误差却比浅层网络的误差更大。这不是过拟合了，过深网络出现了 退化问题。 本文效果（很大程度上解决了退化问题）： 作者在ImageNet上实验了一个152层的残差网络，比VGG深8倍，取得了3.57%的错误率。 作者通过一系列实验证明了表示的深度（即网络的深度）对很多视觉识别任务都至关重要。仅仅由于使用了非常深的网络，作者就在COCO目标检测数据集上获得了28%的相对提升。 本文怎么解决退化问题？Resnet 结构分析ResNet 短连接块作者在《Identity Mapping in Deep Residual Networks 》一文中，推导了为什么短连接更容易训练，也提出了新的短连接块结构。 为什么 ResNet build block 更容易训练？前向传播中帮助网络中一些层更容易实现恒等映射： 出现了第一幅图的退化问题，这里假设深层网络的后边层都变成了恒等映射 ，那网络就退化成了浅层网络（因为网络很深，所以其中肯定包括了多余的层，这些层会形成恒等映射 关系），原来的直接多个层堆叠的非线性层去直接学习 恒等映射 优化起来复杂，而加了上图的 短连接块之后，学习 恒等映射 变容易了。 反向传播中 因为网络中存在恒等映射的短连接通道，假设不加residual模块的输出为h(x)。x=10,h(x)=11,h(x)简化为线性运算Wh​, Wh明显为1.1，加了redidual模块后，F(x)=1, H(x)=F(x)+x=11，F也简化为线性运算,对应的WF为0.1。当标签中的真实值为12，反向传播的损失为1，而对于F中的参数和h中参数回传的损失实际上是一样大的而且梯度都是x的值，但是对于F的参数就从0.1到1.1扩大了十倍多，而h的参数是从1.1到2.1扩大不到2倍，因此redidual模块会明显减小模块中参数的值从而让网络中的参数对反向传导的损失值有更敏感的响应能力，虽然根本上没有解决回传的损失小得问题，但是却让参数减小，相对而言增加了回传损失的效果，也产生了一定的正则化作用。https://blog.csdn.net/weixin_43624538/article/details/85049699?from=timeline&amp;isappinstalled=0]]></content>
      <categories>
        <category>计算机视觉论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习论文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Network in Network]]></title>
    <url>%2F2019%2F01%2F24%2FNetwork-in-Network%2F</url>
    <content type="text"><![CDATA[2014年 论文地址： https://arxiv.org/abs/1312.4400 论文核心NIN特点: 微型网络: 增强模型在感受野（receptive field）内对局部区域的辨别能力; GAP全局平均池化: 强化了特征图与分类的对应关系; GAP本身是结构化的正则化器，能避免整体结构的过拟合； 卷积层使用线性滤波器（卷积核）来扫描输入，后面接一个非线性激活函数。而卷积核是广义线性模型（generalized linear model ）GLM，抽象程度低（该特征对同一概念的变体是不变的）,用更有效的 非线性函数逼近器 代替 GLM 可以增强局部模型的抽象能力。当样本的隐含概念（latent concept）线性可分时，GLM可以达到很好的抽象程度，例如：这些概念的变体都在GLM分割平面的同一边，而传统的CNN就默认了这个假设——认为隐含概念（latent concept）是线性可分的。然而，同一概念的数据通常是非线性流形的（nonlinear manifold），捕捉这些概念的表达通常都是输入的高维非线性函数。在NIN中，GLM用“微型网络”结构替代，该结构是一个非线性函数逼近器。 本文作者选择多层感知器实例化微型网络，该感知器是一个通用函数逼近器，也是一个通过反向传播训练的神经网络。 该图是 单独的 mlpconv 层。这里有没有尝试过其他微型网络结构？？？？（可创新吗） NIN 网络结构 NIN 的整体结构是一系列 mlpconve层 的堆叠，最上层接一个 GAP层 和 分类层。 mlpconv层 间的子层可以被相加，像CNN和maxout网络一样。上图展示了一个包含三个mlpconv层的NIN。每个mlpconv层，包含一个三层的感知器，NIN和微型网络的层数都是灵活的，可以根据具体任务微调。 这里没有采用传统CNN的 全连接层 进行分类，而是直接通过 全局平均池化层（GAP）输出最后一个 mlpconv层特征图的空间平均值作为类别的置信度值，然后将得到的向量输入 softmax层。]]></content>
      <categories>
        <category>计算机视觉论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习论文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++梳理笔记]]></title>
    <url>%2F2019%2F01%2F20%2FC-%E6%A2%B3%E7%90%86%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[测试内容 删除线 链接 C++学习笔记类型转换： 隐式转换： 低类型转换为高类型 浮点数（直接舍掉小数，不四舍五入） + 整数 显式转换： int **(**z**) = (**int**)** z **= static_cast\&lt;**int**\&gt; (**z**)** 。。。 数据的输入和输出：信息的流动 输入： 输出： 流类库的操纵符： 程序控制： if, while, for, do-while , break, continue, { switch,case,default } ; do-while: do 语句 // 先执行一次 while(表达式)； for的范围，遍历容器： 自定义类型： 类型别名： typedef double Area, V; using Area = double 枚举类型： 有限的个数 不限定作用域： enum 类型名 { 变量值列表} 限定作用域： 注：枚举元素是常量，不能赋值 枚举元素有默认值，默认0,1,2,3,4，声明时可以另外指定 可以进行关系运算 auto类型 和decltyoe类型 decltype( float( i )) j = 2; // j值是2，类型是float; auto m = 2.5; // m 为float; 结构体( C语言中的)： struct struct MyTimeStruct{ //定义 结构体类型 unsigned int year,mouth,day,hour,min,sec; }; 函数： 可重用的功能模块（定义和调用）函数定义： 形参不占用空间，调用时分配； 函数调用： 调用前要函数声明： int sum( int a, int b); 1. 函数的嵌套调用： 2. 函数的递归调用： 直接或者间接调用自身 计算n! unsigned int fac( unsigned int n){ if (n == 0) return 1; return fac( n - 1) * n; } 汉诺塔 分析： 1. A 上的n-1个盘子移动到B上（借助C）; 2. A上剩下的盘子移动到C上； 3. B上的n-1个盘子移动到C上（借助A） void move(char src, char obj) { cout &lt;&lt; src &lt;&lt; &quot;---&gt;&gt;&gt;&quot; &lt;&lt; obj &lt;&lt; endl; } void hanoi(int n, char src, char medium, char obj) { if(n == 1) move(src, obj); else{ hanoi(n-1, src, obj, medium); move(src, obj); hanoi(n-1, medium, src, obj); } } 函数的参数： 形参不占用空间，调用时分配； 计算结果返回多个（利用引用） 多个参数时，从后开始传 引用类型（&amp;）： 必须初始化，该类型不可改变，是其他变量的别名 int i, j; int &amp; ri = i; // 定义int引用类型变量 ri, 初始化为i的引用 含有可变参数的函数：（两种方法） 所有实参类型相同：initializer_list&lt;int&gt; li; //类模板, 都是常量 具体看第九章 类型不同： 内联函数（inline）： 用函数体内的语句，替换函数调用表达式，编译时完成，类似 #define声明： inline int calArea(int a){ } 要求： 1. 不能有循环，switch语句 2. 定义在调用之前 3. 不能有异常接口声明 constexpr 函数：（常量表达式函数）带默认参数的函数： int getVa(int length, int weight = 2) 函数的重载：（C++多态性的重要机制，编译过程中实现）函数体同名，参数类型不同/参数个数不同 int add(int x, int y); float add(float x, float y); float add(float x, float y, float z); C++系统函数： #include &lt;cmath&gt; |_ |_ #include &lt;cstdlib&gt; |_ |_ #include &lt;cstdio&gt; |_ |_ #include &lt;ctime&gt; |_ |_ 类和对象类：构建对象的蓝图， 对象：由类创建，含有数据和方法 封装：对数据和操作数据的方法的组合绑定 继承：在已有类基础上，形成新的类 多态： 构造函数：定义对象时，通过构造函数初始化 析构函数：删除对象时，通过析构函数释放资源 类和对象的定义：定义类： class { //类名称 public: // 公有成员,外部接口 private: // 私有成员 protected: int hour = 0; // 类内初始化 // 保护型成员 } 注意：不指定类型，默认为私有； 成员函数： |_ 内联成员函数： 类内声明或者inline关键字 |_类外实现：void 类名称::成员函数名称（）{ } 构造函数： 在创建对象时，自动调用来初始化数据 与类名相同 构造函数有初始化列表 格式 类名（string s, lei i）：s(初始值)，i(初始值){ }； 委托构造函数：一个构造函数 通过另一个构造函数 初始化复制构造函数：用途： 用存在的对象 去初始化新对象 （通过引用旧的对象） 函数f的形参是类的对象，调用f时，将用实参对象初始化形参对象 函数g的返回值是类的对象，用return的对象来在主调函数中初始化一个无名对象 析构函数：生存期结束，删除清理工作，不能有return，不能有参数 class 类名{ public: 类名（形参）； // 构造函数 类名（const 类名&amp; 旧对象名）； // 复制构造函数 =delete是不生成 ~ 类名（）； } 注：未声明时，编译器自己生成一个默认的 前向引用声明：两个类相互引用时，某个类在引用之前就声明 class A; //前向引用声明，只是一个标识符，不是万能的 class B{ public: void A(B b); } class A{ public： void B（A a）; } 结构体：特殊的类，默认是公有的，可以有函数成员 //公有成员 int a; protected: int b; private: int c; }; 联合体：目的：存储空间的共用，成员不能同时有效，比如某人语文课成绩，只有一种可能； union Mark{ // 成绩的联合体， 只有一个成立 char grade; //等级类的成绩 bool pass; // 是否通过的成绩 int percent; //百分制成绩 } 枚举类：enum class 枚举类型名： 底层类型（int）{ 枚举列表 }; //默认 int 优势： 强制作用域 —必须在枚举类 枚举类型名：：枚举值，不同枚举类可以有同名值了 转换限制 —枚举对象不能与整型 隐式转换 底层类型 —可以指定 数据共享和保护：作用域分类：函数原型作用域： 形参的范围在（）内，所以不需要名字也行，int area( int ); 局部作用域 函数{ }内 if、for、while { }内 类作用域： 类外访问类的成员 静态成员：通过 对象名.成员名 访问 非静态成员： 文件作用域 命名空间作用域： 10章 对象的生存期：静态生存期： 整个程序结束后消失 函数内的静态对象， 用static ，全局寿命，只局部可见 动态生存期： 离开作用域后消失 下次进函数重新生成对象 类的静态数据成员： static 声明 为该类所有对象共享，具有静态生存期 必须在类外定义和初始化，类内声明，用：：指明所属于的类 比如记录 类产生了多少对象；opencv中的Mat对象好像用到了？？？？ class base{ public : static int _num;//声明 }; int base::_num=0; //真正定义 类的友元： 破坏数据封装和数据隐藏的机制 尽量不用 友元函数： 类声明中由关键字 friend 修饰说明的非成员函数 可以在其函数体内访问对象的private,protected成员 但必须通过对象名：：访问，函数参数为类的引用 友元类：class A{ friend B; public: void display(){ count &lt;&lt; x &lt;&lt; enld; } private: int x; } class B{ public: void set(int i); void display(); private: A a; } void B::set(int i){ a.x = i; // B类中改变 A类私有值 } void B::display(){ a.display() } 共享数据的保护：常类型：const常对象：必须初始化，不可更新 class A{ } A const a; // a是常对象 常成员：(不可以放在构造函数体内复制，可以在初始化列表中) A：：A(int i):a(i){ } 常数据成员：const修饰的 静态常数据成员： static const int b; 常函数成员（用来处理常对象的函数） 不更新对象的数据成员 声明和实现都带const class A{ void f（int a）const; } void A::f(int a) const{ }; // f是常对象函数, 处理常对象 常引用：不可更新 引用是双向传递的，避免修改原值的方法就是常引用； const A&amp; a; 常数组： 常指针： 多文件结构和预编译命令： .h 系统使用 .hpp 个人使用(类的声明,函数的声明) .cpp (类的实现，函数的实现) 外部变量：文件作用域中定义的变量默认是外部变量，其他文件使用前，extern声明 将变量和函数限制在编译单元内：namespcae: namespace{ //匿名的命名空间，外部不可调用任何东西 int i; void fun(){ i++; } } 预编译命令： #include&lt; &gt; 标准方式搜索，从系统目录include #include”” 先当前目录搜索，没有再标准搜索 #define #undef 删除有#define的宏 #if 表达式 // 条件编译指令 --- #else --- #endif #ifndef 标识符 --- #else --- #endif 数组，指针与字符串：数组：定义： int arr**[**m**][**n**]**…; 注：二维数组中 arr[1] 第二行首地址 数组作为函数参数： 数组名做参数： 形参，实参都是数组名，传入的是地址 对象数组： 定义：类名 数组名[对象元素个数] 访问：数组名[下标].成员名 基于范围的for循环：c++11,自动遍历整个容器 for( auto x : 容器){ } for( auto &amp;x : 容器){ } 注意： auto &amp;x是元素引用，auto x是元素的副本 auto推导出的类型是容器中的值类型 ：冒号后的表达式只执行一次 指针：定义： static int i; static int * p = &amp;I; 指针的初始化和赋值：指针的算术运算，关系运算：指针数组： 类名 *p[2]; 指向数组的指针： int **p; 指向二维数组的指针 指针与函数： 指针做参数：大批量数据提高效率 指针类型的函数：返回类型是指针 int * function(int i){return 全局或者静态的 }；// 不能返回非静态局部变量 指向函数的指针：实现函数回调的功能 定义： 数据类型 (*f)(参数表); 数据类型：返回值 对象指针： 定义： 类名 *对象指针名 = &amp; 对象； 访问对象： 对象指针名->成员名 （*对象指针名）.成员名 this 指针：成员函数的一个隐士参数，初始化为对象的地址，不可改变 隐含于类的每个非静态成员函数中 指出成员函数所操作的当前的对象 *this 是当前对象地址 动态内存分配：new 类型名 (初始化列表) // 返回首字节地址 delete 指针p //p一直在，删除的只是p指向的对象申请的空间 动态数组：new 类型名[数组长度] delete[] 数组首地址p指针 智能指针：C++11内存管理 unique_ptr: 不允许多个指针共享资源，标准库中move可以转移指针，但原来指针会失效 shared_ptr: 多指针共享 weak_ptr: 可复制共享 Vector对象：类模板 优势： 封装任何形式的动态数组，自动创建，删除 下标越界检查 定义： vector &lt;元素类型&gt; object（长度） object.begin() object.end() object.size() auto 遍历vector for(auto e: object); 对象的复制和移动： 浅层复制和深层复制：复制对象用到复制构造函数，默认的复制构造只传递了指针，两个变量指向同一块内存，释放其中一个，再释放第二个会出错； 浅层：实现对象间数据一一对应的复制，但两个对象指向同一内存 深层：当对象成员是指针类型，应该对指针所指对象进行复制。 类名::类名(const 类名&amp; v){ size = v.size; data_ptr = new Ponit[size]; for(int i=0; i \&lt; size; ++i){ data_ptr[i] = v.data_ptr[i]; } } 移动构造：C++11,省去了构造和删除临时对象的过程 class_name(class_name &amp;&amp;old)::xptr(old.xptr){ n.xptr = NULL; // 原来的指针清空 } C风格字符串：字符数组string类：常用构造函数： string(); //默认构造，长度为0 string s1; string(const char *s) //指针s所指向的字符串常量初始化该对象 string s2 = “abc”; string(const string &amp;rhs) //复制构造函数 string s3 = s2; 访问：下标访问 整行字符串的输入： cin 被空格隔开 getline(cin,s2); //包含#include\ getline(cin,s2,’,’); 继承和派生： 充分利用原有的继承：保持已有类的特征来构造新类 派生：在已有类基础上新增自己的特性 基类：父类 派生类：子类 直接基类和间接基类 单继承： class 派生类名：继承方式 基类名{ //继承方式， 成员声明；//新增成员的声明 } 多继承： class 派生类名：继承方式1 基类1，继承方式2 基类2{ 成员声明； } 继承的方式：控制：派生类对基类成员的访问权限 公有继承 public 基类中的pubilc和protected访问属性在派生类中不变 基类的pravate不可被对象直接访问 私有继承 ：内部可以访问基类的公有和保护成员，但是其对象不再可以访问 保护继承 ：基类的公有和保护，到这都成了保护成员，类内可以访问，但对象不能 派生类的构成： 吸收基类成员 改造基类成员 增加同名成员，基类成员被覆盖（重新定义继承的成员函数必须用虚函数） 添加新成员 类型转换：基类和派生类之间： 派生类的对象可以隐含转换为基类对象； 派生类的对象可以初始化基类的引用； 派生类的指针可以隐含转换为基类的指针； 派生类的构造函数：默认情况下，基类的构造函数不被继承，派生类需要自己构造 c++11，using语句继承基类构造函数 派生类的复制构造函数：派生类的析构函数：虚基类：多态性运算符重载：//双目运算符 函数类型 operator 运算符（参数） { // 参数个数 = 原操作数个数 - 1 } //前置单目运算符，返回引用所以可以当左值 函数类型 &amp; operator ++（无参数） { return * this; } //后置单目运算符， 函数类型 operator ++（参数为int类型） { old = *this; ++(*this); //调用的前置 return old; } 重载为非成员函数： 列出所有操作数 至少有一个自定义类型参数 后置单目运算，参数要增加int,但不用写形参名 要操作某类对象的私有成员，则可声明为该类的友元函数 虚函数：virtual改造基类成员，实现动态绑定；必须是非静态成员 原理：编译时先不确定和哪个类的成员对应，在程序运行时刻，再对应； #include &lt;iostream&gt; using namespace std; class Base1{ public: virtual void display() const; //虚函数，不要用内联 }; void Base1::display() const{ cout &lt;&lt; &quot;Base1 &quot; &lt;&lt; endl; } class Base2:public Base1{ public: virtual void display() const; } void Base2::display() const{ cout &lt;&lt; &quot;Base2&quot; &lt;&lt; endl; } 虚析构函数：打算通过基类指针调用某一个对象的析构函数（执行delete）虚表和动态绑定： 虚表： 每个多态类都有虚表； 存放各个数函数的入口地址； 每个对象有指向当前类的虚表的指针（虚指针vptr）； 动态绑定： 构造函数为对象的虚指针赋值 抽象类：含有纯虚函数的类,不能直接定义对象 纯虚函数： 基类中声明的虚函数，在基类中没有定义具体的操作，要求在派生类中根据实际需求完 成自己的版本： virtual 函数类型 函数名**(**参数名**) =** 0**;** override 和 final :C++11override声明的函数，必须在基类中找到原型； final 不允许继承或者覆盖； 模板函数魔板：整数和浮点数求绝对值，需要多次重载函数，但是用函数模板，只需要设计通用功能；template\&lt;模板参数表> // 类型：class或者typename 常量： 函数定义 template&lt;typename T&gt; T abs(T x){ return x&lt;0?-x:x; } 类模板：template&lt;模板参数表&gt; class 类名{ 类成员声明; } //类成员定义 template &lt;模板参数表&gt; 类型名 类名&lt;模板参数标识符列表&gt; :: 函数名(参数表) { } 线性群体：按位置顺序有序排列直接访问： 数组类模板： 索引访问： 顺序访问： 链表类和结点类模板： 单链表：每个结点包括数据和指针，只有一个指向后续结点的称为单链表； 单链表结点类模板： template &lt;class T&gt; class Node{ private: Node&lt;T&gt; *next; public: T data; Node(const T&amp;item,Node&lt;T&gt;* next = 0); //构造函数 void insertAfter(Node&lt;T&gt; *p); //插入 Node&lt;T&gt; *deleteAfter(); //删除 Node&lt;T&gt; *nextNode() const; } template &lt;class T&gt; void Node&lt;T&gt;::insertAfter(Node&lt;T&gt; *p){ // *p是要插入的结点 // p节点的指针指向当前节点的后续结点 p-&gt;next = next; // next是原链表待插入位置的结点的指针 next = p; } template &lt;class T&gt; Node&lt;T&gt; *deleteAfter(){ Node&lt;T&gt; * tempPtr = next; if (next == NULL) //判断是否是删除最后的元素 return 0; next = tempPtr = next; return tempPtr; } 插入： 头插法：可以当队列 尾插法：栈 删除： 待查询： explicit关键字 构造函数 explicit可以抑制内置类型隐式转换 泛型设计基本概念： 编写不依赖具体数据类型的程序，通用的； STL简介：(Standard Template Library) C++ string类库入门： #include &lt;iostream&gt; #include &lt;string&gt; using namespace std; int main() { // 构造函数： string str1 = &quot;Yesterday&quot;; string str2(&quot;Today&quot;); string str3(&quot;Hello&quot;,2); //取c风格字符串 长度为 2 作为初值，即&quot;He&quot; string str4(str1, 6); // 始于位置6开始的字符串，即&quot;day&quot; string str5(str1,6,1); // 始于6，长度1，即&quot;d&quot; string str6(1,&#39;a&#39;); //6个&#39;a&#39; // 赋值，交换 str1.assign(&quot;hahahaha&quot;); //重新赋值 swap(str1,str2); //交换两个字符串内容 str1=&quot;Today&quot; str2=&quot;hahahaha&quot; // 追加 str1 += &quot; we&quot;; // += 可追加 string对象，字符串，字符 str1.append(&quot; ar&quot;); // append 可追加 string对象，字符串 str1.push_back(&#39;e&#39;); //push_back 只能追加字符 str1 = &quot;Today we are&quot; // 插入 str1.insert(0,&quot; family&quot;); //str1 = &quot;Today we are family&quot; // 删除 str1.erase(2,1); //第2个位置开始， len = 1 个字符 str1.clear(); //删除全部 // 访问字符串 string s = &quot;asdfgh&quot;; cout &lt;&lt; s[1]; // &#39;s&#39; cout &lt;&lt; s.at(2); // &#39;d&#39; // 查找 int position = s.find(&#39;f&#39;,0); // 从0开始查找第一次出现‘f’的坐标 // 替换 s.replace(s.find(&#39;f&#39;),3,&quot;ZZZ&quot;); //替换find的位置处 3个字符串为 “ZZZ” // 分割 getchar(); return 0; }]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[eclipse-and-github]]></title>
    <url>%2F2019%2F01%2F19%2Fgit%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[内容概要 本地 首次上传到 github 本地 更新到 github github 首次下载到 本地 github 更新到 本地 本地首次上传到 github 进入 github官网，选择 New repository 复制地址 http:XXXXXXXXXX.git 本地 右键自己的项目文件夹，选择 git bash here 克隆 github 仓库到本地(执行如下命令), 会在本地产生一个 github 上仓库同名的文件夹 XXX，将工程所有内容移入文件夹内 git clone http:XXXXXXXXXX.git cd XXX, 进入该目录，执行以下操作： git add . // git status git commit -m &quot;此次提交的备注信息&quot; git push -u origin master 本地更新到 github方法与上节中的5一致。 首次下载到本地git clone http:XXXXXXXXXX.git 更新到本地git pull]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[欢迎来我的小屋！]]></title>
    <url>%2F2019%2F01%2F18%2F%E6%AC%A2%E8%BF%8E%E6%9D%A5%E6%88%91%E7%9A%84%E5%B0%8F%E5%B1%8B%2F</url>
    <content type="text"><![CDATA[欢迎来到我的小屋做客]]></content>
      <categories>
        <category>闲聊</category>
      </categories>
      <tags>
        <tag>闲聊</tag>
      </tags>
  </entry>
</search>
